%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template:
% Exam Notes LaTeX Template
% Version 1.0 (30/08/2018)
%
% Document:
% Signal Processing exam notes
%
% Original author:
% Rambod Rahmani <rambodrahmani@autistici.org>
%
% License: 
% Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
% License.
% 
% Compilation:
% $ pdflatex main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-------------------------------------------------------------------------------
% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%-------------------------------------------------------------------------------

\documentclass[12pt,oneside,openany]{memoir}

\usepackage[utf8x]{inputenc}

\usepackage[T1]{fontenc}

\usepackage[italian]{babel}

\usepackage{url}

\usepackage{tabto}

\usepackage{wrapfig}

\usepackage[pdftex,dvipsnames]{xcolor}

\usepackage{gensymb}

\usepackage{amssymb,amsmath,amsthm}

\usepackage{mathtools}

\usepackage{mathpazo}

\usepackage{graphicx}

\usepackage{caption}

\usepackage{float}

\usepackage{csquotes}

\usepackage{afterpage}

\usepackage{hyperref}

\usepackage{textcomp}

\definecolor{mygreen}{RGB}{28,172,0}
\definecolor{mylilas}{RGB}{170,55,241}

\usepackage{listings}

\lstset{
	language=Matlab,
	basicstyle=\tiny,
	breaklines=true,
	morekeywords={matlab2tikz},
	keywordstyle=\color{blue},
	morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
	identifierstyle=\color{black},
	stringstyle=\color{mylilas},
	commentstyle=\color{mygreen},
	% without this there will be a symbol in the places where there is a
	% space
	showstringspaces=false,
	numbers=left,
	numberstyle={\tiny \color{black}},% size of the numbers
	numbersep=9pt, % this defines how far the numbers are from the text
	%some words to emphasise
	emph=[1]{for,end,break},emphstyle=[1]\color{red},
	%emph=[2]{word1,word2}, emphstyle=[2]{style},    
}

% Use more than one optional parameter in a new commands
\usepackage{xargs}

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\usepackage{expl3}
\ExplSyntaxOn
\cs_new_eq:NN \Repeat \prg_replicate:nn
\ExplSyntaxOff

\numberwithin{equation}{subsection}

% dedication environment
\newenvironment{dedication}
{
	% blank page before dedication
	\afterpage{\blankpage}
	% we want a new page
	\clearpage
	% no header and footer
	\thispagestyle{empty}
	% some space at the top 
	\vspace*{\stretch{1}}
	% the text is in italics
	\itshape
	% flush to the right margin
	\raggedleft
	% blank page after dedication
	\afterpage{\blankpage}
}
{
	% end the paragraph
	\par
	% space at bottom is one times that at the top
	\vspace{\stretch{1}}
	% finish off the page
	\clearpage
}

\newtheorem{theorem}{Teorema}[section]
\newtheorem{lemma}[theorem]{Lemma}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

\mathtoolsset{showmanualtags,mathic,centercolon}

\let\conjugate\overline

% blank page command
\newcommand{\blankpage}
{
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage
}

\setcounter{chapter}{-1}

\epigraphfontsize{\small\itshape}
\setlength\epigraphwidth{8cm}
\setlength\epigraphrule{0pt}

% show subsection in the table of contents
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\newcommand{\quads}[1]{\Repeat{#1}{\quad}}
\newcommand{\dt}{\ dt}
\newcommand{\df}{\ df}

\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}

\epigraphfontsize{\small\itshape}

%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\title{%
	Esame Comunicazioni Numeriche\\
	\vspace{1cm}
	\large \textit{
		"What I hear, I forget;
		What I see, I remember;
		What I do, I understand."
	}}
\author{Rambod Rahmani}

\begin{document}
\renewcommand{\rmdefault}{cmr}

\maketitle
\pagenumbering{gobble}

\begin{dedication}
To my twin brother Ramtin.
\end{dedication}

%-------------------------------------------------------------------------------
% TABLE OF CONTENTS
%-------------------------------------------------------------------------------

\newpage
\pagenumbering{roman}
\tableofcontents
\afterpage{\blankpage}

\newpage
\pagenumbering{arabic}

%-------------------------------------------------------------------------------
% Chapter 0: Disclaimer
%-------------------------------------------------------------------------------

\chapter{Disclaimer}
\epigraph{"Information is power. But like all power, there are those who want
to keep it for themselves. The world's entire scientific and cultural heritage,
published over centuries in books and journals, is increasingly being digitized
and locked up by a handful of private corporations. There is no justice in
following unjust laws. It's time to come into the light and, in the grand
tradition of civil disobedience, declare our opposition to this private theft
of public culture."}{--- \textup{Aaron Swartz}}

The material and information in this document are provided "AS IS" in good faith
and without warranties of any kind, either expressed or implied.
\bigbreak\noindent
This work is licensed under:
\bigbreak\noindent
\href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{
	\includegraphics[height=\baselineskip]{images/cc_88x31.png} 
	Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International 
	License}.

%-------------------------------------------------------------------------------
% Chapter 1: Introduction
%-------------------------------------------------------------------------------

\chapter{Introduction}
\epigraph{
	"The only person who is educated is the one who has learned how to
	learn and change."
}{--- \textup{Carl Rogers}}

This chapter contains introductory concepts needed as theoretical base to be
able to understand the signal processing topics presented in this text.

%-------------------------------------------------------------------------------
% Section 1: Complex Numbers
%-------------------------------------------------------------------------------

\section{Complex Numbers}
A complex number is a number that can be expressed in the form
\[z = a + ib,\]
where \(a\) is known to be the real part
\[a = \Re(z),\]
and \(b\) is known to be the imaginary part
\[b = \Im(z).\]
The complex number system can be defined as the algebraic extension of the
ordinary real numbers by an imaginary number \(i\). \(i\) is a solution of
the equation \(x^2 = −1\). Because no real number satisfies this equation, since
the square of a real number cannot be negative, \(i\) is called an imaginary
number.
\bigbreak
A real number \(a\) can be regarded as a complex number \(a + 0i\) whose
imaginary part is \(0\). A purely imaginary number \(bi\) is a complex number
\(0 + bi\) whose real part is zero. It is common to write \(a\) for \(a + 0i\)
and \(bi\) for \(0 + bi\).
\bigbreak
The complex numbers give rise to the fundamental theorem of algebra: every
non-constant polynomial equation with complex coefficients has a complex
solution. This property is true of the complex numbers, but not the reals. The
16th century Italian mathematician Gerolamo Cardano is credited with introducing
complex numbers in his attempts to find solutions to cubic equations.
\bigbreak
Geometrically, complex numbers extend the concept of the one-dimensional number
line to the two-dimensional complex plane by using the horizontal axis for the
real part and the vertical axis for the imaginary part.
\bigbreak
\textbf{Engineers reserve the letter \(i\) for electric current - the derivative
of electric charge with respect to time. We will therefore be using the letter
\(j\) instead of the letter \(i\).}

%-------------------------------------------------------------------------------
% Subsection 1: Cartesian form
%-------------------------------------------------------------------------------

\subsection{Cartesian form}
\begin{wrapfigure}{L}{0.5\textwidth}
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.5\textwidth]{images/complex_plane.png}
    \caption{A Complex Number in Cartesian Form.}
\end{wrapfigure}
A complex number can thus be identified with an ordered pair
\((\Re(z), \Im(z))\) in the Cartesian plane, an identification sometimes known
as the Cartesian form of \(z\). A complex number can be viewed as a point or
position vector in a two-dimensional Cartesian coordinate system called the
complex plane or Argand diagram, named after Jean-Robert Argand. The numbers are
conventionally plotted using the real part as the horizontal component, and
imaginary part as vertical (see Figure 1.1). In fact, a complex number can be
defined as an ordered pair \((a, b)\), but then rules for addition and
multiplication must also be included as part of the definition. William Rowan
Hamilton introduced this approach to define the complex number system.

%-------------------------------------------------------------------------------
% Subsection 2: Polar form
%-------------------------------------------------------------------------------

\subsection{Polar form}
\begin{wrapfigure}{L}{0.4\textwidth}
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.4\textwidth]{images/complex_number_polar_form.png}
    \caption{A Complex Number in Polar Form}
\end{wrapfigure}
An alternative way of defining a point P in the complex plane, other than using
the \(a\)- and \(b\)-coordinates, is to use the distance of the point from \(O\)
, the point whose coordinates are \((0, 0)\) (the origin), together with the
angle subtended between the positive real axis and the line segment OP in a
counterclockwise direction. This idea leads to the polar form of complex numbers
. (see Figure 1.2 where \(x\) and \(y\) were used instead of \(a\) and \(b\)).
\bigbreak
The absolute value (or modulus or magnitude) of a complex number \(z = a + bj\)
is
\[c = \abs{z} = \sqrt{a^2 + b^2}\]
If \(z\) is a real number (that is, if \(b = 0\)), then \(c = \abs{a}\). That is
, the absolute value of a real number equals its absolute value as a complex
number. By Pythagoras' theorem, the absolute value of complex number is the
distance to the origin of the point representing the complex number in the
complex plane. 
\bigbreak
The argument of \(z\) (in many applications referred to as the "phase") is the
angle of the radius \(OP\) with the positive real axis, and is written as
\(arg(z)\). As with the modulus, the argument can be found from the rectangular
form \(a + bj\):
\[
	\varphi = arg(z) =
	\begin{cases}
		arctan\left(\frac{b}{a}\right) \quads{4} \ if \quad a > 0\\
		arctan\left(\frac{b}{a}\right) + \pi \quads{2} \ if \quad a < 0 
		\quad and \quad b \geq 0\\
		arctan\left(\frac{b}{a}\right) - \pi \quads{2} \ if \quad a < 0 
		\quad and \quad b < 0\\
		\frac{\pi}{2} \quads{8} if \quad a = 0 \quad and \quad b > 0\\
		-\frac{\pi}{2} \quads{7} if \quad a = 0 \quad and \quad b < 0\\
		indeterminate \quads{2} \ \ if \quad a > 0
	 \end{cases}
\]
The value of \(\varphi\) is expressed in radians in this article. It can
increase by any integer multiple of \(2\pi\) and still give the same angle.
Hence, the \(arg()\) function is sometimes considered as multivalued. The polar
angle for the complex number 0 is indeterminate, but arbitrary choice of the
angle 0 is common. Together, \(c\) and \(\varphi\) give another way of
representing complex numbers, the polar form, as the combination of modulus and
argument fully specify the position of a point on the plane.
\bigbreak
If we are given the polar form to start with, the following formulas can be used
to retrieve the cartesian coordinates:
\[
	\begin{cases}
		a = c \cdot \cos(\varphi)\\
		b = c \cdot \sin(\varphi)
	\end{cases}
\]

%-------------------------------------------------------------------------------
% Subsection 3: Trigonometric form
%-------------------------------------------------------------------------------

\subsection{Trigonometric form}
The trigonometric form of a complex number \(z = a + bj\) can be easily obtain
by substitution in our previous equations:
\[
	z = a + bj = c \cdot \cos(\varphi) + c \cdot sin(\varphi) \cdot j =
	c \cdot (\cos(\varphi) + j \cdot sin(\varphi)).
\]

%-------------------------------------------------------------------------------
% Subsection 4: Euler's formula
%-------------------------------------------------------------------------------

\subsection{Euler's formula}
Euler's formula, named after Leonhard Euler, is a mathematical formula in
complex analysis that establishes the fundamental relationship between the
trigonometric functions and the complex exponential function. Euler's formula
states that for any real number \(\varphi\):
\[
	e^{j \varphi} = \cos(\varphi) + j \cdot sin(\varphi)
\]
where \(e\) is the base of the natural logarithm, \(j\) is the imaginary unit,
and \(cos\) and \(sin\) are the trigonometric functions cosine and sine
respectively, with the argument \(\varphi\) given in radians.
\bigbreak
When \(\varphi = \pi\), Euler's formula evaluates to \(e^{j \pi} + 1 = 0\),
which is known as Euler's identity.
\bigbreak
\begin{wrapfigure}{L}{0.4\textwidth}
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.4\textwidth]{images/euler_s_formula.png}
	\caption{Euler's formula Interpretation}
\end{wrapfigure}
This formula can be interpreted as saying that the function \(e^{j \varphi}\) is
a unit complex number, i.e., it traces out the unit circle in the complex plane
as \(\varphi\) ranges through the real numbers. Here \(\varphi\) is the angle
that a line connecting the origin with a point on the unit circle makes with the
positive real axis, measured counterclockwise and in radians.
\bigbreak
The original proof is based on the Taylor series expansions of the exponential
function \(e^z\) (where \(z\) is a complex number) and of \(sin(x)\) and
\(\cos(x)\) for real numbers \(x\). In fact, the same proof shows that Euler's
formula is even valid for all complex numbers \(z\). 

%-------------------------------------------------------------------------------
% Subsection 5: Euler's formula and Complex Numbers
%-------------------------------------------------------------------------------

\subsection{Euler's formula and Complex Numbers}
Using Euler's formula we can now write:
\[
	\boldsymbol{z = a + jb = c \cdot \cos(\varphi) + c \cdot sin(\varphi)
	\cdot j = c \cdot (\cos(\varphi) + j \cdot sin(\varphi)) =
	e^{j \varphi}}.
\]

%-------------------------------------------------------------------------------
% Subsubsection: Elementary Operations
%-------------------------------------------------------------------------------

\subsection{Elementary operations}
\subsubsection{Conjugate}
\begin{wrapfigure}{L}{0.4\textwidth}
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.4\textwidth]{images/complex_conjugate_picture.png}
	\caption{Geometric representation of z and its conjugate \(\conjugate{z}\)}
\end{wrapfigure}
The complex conjugate of the complex number \(z = a + jb\) is given by
\(a − jb\). It is denoted by either \(\conjugate{z}\) or \(z^*\). This unary
operation on complex numbers cannot be expressed by applying only their basic
operations addition, subtraction, multiplication and division.
\bigbreak
Geometrically, \(\conjugate{z}\) is the "reflection" of z about the real axis.
Conjugating twice gives the original complex number
\[
	\conjugate{\conjugate{z}},
\]
which makes this operation an involution. The reflection leaves both the real
part and the magnitude of \(z\) unchanged, that is
\[
	\Re(\conjugate{z}) = \Re(z) \quad and \quad \abs{\conjugate{z}} =
	\abs{z}.
\]
The imaginary part and the argument of a complex number \(z\) change their sign
under conjugation 
\[
	\Im(\conjugate{z}) = -\Im(z) \quad and \quad arg(\conjugate{z}) =
	-arg(z).
\]
\vspace{1.0cm}
\bigbreak
The real and imaginary parts of a complex number \(z = a + jb\) can be extracted
using the conjugation:
\[
	\boldsymbol{\Re(z) = a = \frac{z + \conjugate{z}}{2}} =
	\frac{a + jb + a - jb}{2} = \frac{2a}{2} = a.
\]
\[
	\boldsymbol{\Im(z) = b = \frac{z - \conjugate(z)}{2j}} =
	\frac{a + jb - a + jb}{2j} = \frac{2jb}{2j} = b.
\]
Moreover, a complex number is real if and only if it equals its own conjugate. 
\[
	z = \conjugate{z} \quad \rightarrow \quad z \in \mathbb{R}
\]

%-------------------------------------------------------------------------------
% Subsubsection: Additions and subtraction
%-------------------------------------------------------------------------------

\subsubsection{Addition and subtraction}
Two complex numbers \(z_1\) and \(z_2\) are most easily added by separately
adding their real and imaginary parts of the summands. That is to say:
\[
	z_1 + z_2 = a_1 = jb_1 + a_2 + jb_2 = (a_1 + a_2) + j(b_1 + b_2).
\]
Similarly, subtraction can be performed as
\[
	z_1 - z_2 = a_1 + jb_1 - (a_2 + jb_2) = (a_1 - a_2) +j(b_1 - b_2).
\]
\textbf{Please keep in mind that this is completely wrong:}
\[
	z_1 = c_1 \cdot e^{j \varphi_1} \quad , \quad z_2 = c_2
	\cdot e^{j \varphi_2}
\]
\[
	z_1 + z_2 = c_1 \cdot e^{j \varphi_1} + c_2 \cdot e^{j \varphi_2} =
	(c_1 + c_2) \cdot e^{j(\varphi_1 + \varphi_2)}
\]
\textbf{As a matter of fact:}
\[
	c_1 \cdot e^{j \varphi_1} = c_1 \cdot \cos(\varphi_1) + j \cdot
	c_1 \cdot sin(\varphi_1),
\]
\[
	c_2 \cdot e^{j \varphi_2} = c_2 \cdot \cos(\varphi_2) + j \cdot
	c_2 \cdot sin(\varphi_2),
\]
\[
	c_1 \cdot e^{j \varphi_1} + c_2 \cdot e^{j \varphi_2} = c_1 \cdot
	\cos(\varphi_1) + c_2 \cdot \cos(\varphi_2) + j \cdot
	(c_1 \cdot sin(\varphi_1) + c_2 \cdot sin(\varphi_2)),
\]
\textbf{while:}
\[
	(c_1 + c_2) \cdot e^{j (\varphi_1 + \varphi_2)} = (c_1 + c_2) \cdot
	(\cos(\varphi_1 + \varphi_2) + j \cdot sin(\varphi_1 + \varphi_2)) =
\]
\[
	c_1 \cdot \cos(\varphi_1 + \varphi_2) + c_2 \cdot
	\cos(\varphi_1 + \varphi_2) + j \cdot (c_1 \cdot
	sin(\varphi_1 + \varphi_2) + c_2 \cdot sin(\varphi_1 + \varphi_2)),
\]
\textbf{and:}
\[
	\cos(\varphi_1 + \varphi_2) \neq \cos(\varphi_1) + \cos(\varphi_2).
\]

%-------------------------------------------------------------------------------
% Subsubsection: Multiplication
%-------------------------------------------------------------------------------

\subsubsection{Multiplication}
Since the real part, the imaginary part, and the indeterminate \(i\) in a
complex number are all considered as numbers in themselves, two complex numbers,
given as \(z_1 = a_1 + j b_1\) and \(z_2 = a_2 + j b_2\) are multiplied under
the rules of the distributive property, the commutative properties and the
defining property \(i^{2} = -1\) in the following way
\[
	z_1 \cdot z_2 = (a_1 + j b_1) \cdot (a_2 + j b_2) =
\]
\[
	a_1 a_2 + j a_1 b_2 + j b_1 a_2 + (jj) b_1 b_2 = (a_1 a_2 + b_1 b_2) +
	j \cdot (a_1 b_2 + b_1 a_2).
\]
\textbf{The cartesian form is convenient for additions and subtractions but not
for multiplications.}

%-------------------------------------------------------------------------------
% Subsubsection: Multiplying a complex number by j
%-------------------------------------------------------------------------------

\subsubsection{Multiplying a complex number by j}
In our goal toward finding a geometric interpretation of complex multiplication,
let's consider next multiplying an arbitrary complex number \(z = a + jb\) by
\(j\):
\[
	z \cdot j = (a + jb) \cdot j = -b + ja
\]
\bigbreak
\begin{wrapfigure}{L}{0.4\textwidth}
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.4\textwidth]{images/multiply_by_i.png}
	\caption{Multiplying by i}
\end{wrapfigure}
Let's interpret this statement geometrically. The point \(z\) in \(\mathbb{C}\)
is located $a$ units to the right of the imaginary axis and $b$ units above the
real axis. The point $z \cdot j$ is located $b$ units to the left, and $a$ units
above. What has happened is that multiplying by $j$ has rotated the point $z$
$90^{\degree}$ counterclockwise around the origin to the point $zj$. Stated more
briefly, multiplication by $j$ gives a $90^{\degree}$ counterclockwise rotation
about 0. You can analyze what multiplication by $-j$ does in the same way.
You'll find that multiplication by $-j$ gives a $90^{\degree}$ clockwise
rotation about 0. When we don't specify counterclockwise or clockwise when
referring to rotations or angles, we'll follow the standard convention that
counterclockwise is intended. Then we can say that multiplication by $-j$ gives
a $-90^{\degree}$ rotation about 0, or if you prefer, a $270^{\degree}$ rotation
about 0.

%-------------------------------------------------------------------------------
% Section 2: The Field C of Complex Numbers
%-------------------------------------------------------------------------------

\newpage
\section{The Field C of Complex Numbers}
Before continuing with signal processing, I want to spend some time talking
about the field of complex numbers $\mathbb{C}$. The motivation for this is to
try and provide a much more formal definition of the field $\mathbb{C}$ than the
one roughly introduced in the previous section which represents the minimal
concepts required for the contents of the following pages. You can simply skip
over this section if you are not interested or have previous knowledge.

%-------------------------------------------------------------------------------
% Subsection: Naive introduction to the field C
%-------------------------------------------------------------------------------

\subsection{Naive introduction to the field C}
The usual way to quickly start with complex numbers is to say that we introduce
a new object, which I will call $j$ (mathematicians use $i$), such that the
following \textit{characteristic} property holds:
\[
	i^2 = -1.
\]
Now, having this object at my disposal, I can easily define the set of complex
numbers as the set with elements of the form $x + iy$ where $x$, $y$ are our
familiar real numbers:
\[
	\mathbb{C} \coloneqq \{x + iy : x,y \in \mathbb{R}, i^2 = -1\}.
\]
After this, using the usual rules of arithmetic operations as applied to the
real numbers, I can define the four fundamental operations for the complex
numbers as well. And I therefore have arithmetic in the set of complex numbers.
\bigbreak\noindent
The definition we just saw and the one used in the previous section are,
however, not really satisfactory, because they \textit{do not explain} what
$i$ is, and hence we get a sense of mystery here (and hence the name "imaginary"
numbers). But there is actually nothing imaginary about complex numbers, as was
realized by Gauss and others, if we identify them with the elements of our
familiar $\mathbb{R}^2$ and add a little more. This is the first formal
introduction for the set of complex numbers $\mathbb{C}$ I will provide in the
subsection \textbf{Definition of the field C} (as a side remark I note that
there are other ways to define complex numbers, but this one is arguably the
most natural). A second rigorous definition using matrices is provided in the
section \textbf{Constructing the complex numbers via the arithmetic of 2x2
matrices}.

%-------------------------------------------------------------------------------
% Subsection: Definition of the field C
%-------------------------------------------------------------------------------

\subsection{Definition of the field C}
\textit{The set of complex numbers, which is denoted by $\mathbb{C}$, is, by
definition, $\mathbb{R}^2$, this is, the set of all vectors with two real
coordinates, on which the operation of addition and multiplication are defined
as follows}
\[
	(x_1, y_1) + (x_2, y_2) = (x_1 + x_2, y_1 + y_2),
\]
\[
	(x_1, y_1) \cdot (x_2, y_2) = (x_1 x_2 - y_1 y_2, x_1 y_2 + y_1 x_2).
\]
Definitely, the multiplication rule should look a little strange at the
beginning, but of course the motivation comes from the previous subsection.
\bigbreak\noindent
\textit{The set $\mathbb{C}$, defined with such an addition and multiplication,
is a field. That is, it can be proved that for any
$z_1, z_2, z_3 \in \mathbb{C}$, we have}
\[
	z_1 + z_2 = z_2 + z_1,
\]
\[
	(z_1 + z_2) + z_3 = z_1 + (z_2 + z_3),
\]
\[
	z_1 z_2 = z_2 z_1,
\]
\[
	(z_1 z_2) z_3 = z_1 (z_2 z_3),
\]
\textit{there is a unique element $0 \in \mathbb{C}$ such that}
\[
	z_1 + 0 = z_1,
\]
\textit{for any $z_1 \in \mathbb{C}$ there is $-z_1 \in \mathbb{C}$ such that}
\[
	z_1 + (-z_1) = 0,
\]
for any nonzero $z \in \mathbb{C}$ there is $z^{-1} \in \mathbb{C}$ such that
\[
	z z^{-1} = 1.
\]
It should be clear that above $0 = (0, 0)$, where the $0$ on the left of the
equal sign is the complex zero, whereas the ones used in the round brackets are
in $\mathbb{R}$.
\bigbreak
Now, consider the following equation
\[
	z^2 + 1 = 0.
\]
Here $1$ and $0$ and the complex unit and the complex zero. We all know  that
this equation has  no real roots. But I claim that it has two complex roots
being $(0, 1)$ and $-(0, 1)$. This can easily be checked by substitution
according to the definition we provided for our addition and multiplication
(just keep in mind that .

%-------------------------------------------------------------------------------
% Subsection: Constructing the complex numbers via the arithmetic of 2x2 
% matrices
%-------------------------------------------------------------------------------

\subsection{Constructing the complex numbers via the arithmetic of 2x2 matrices}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Chapter 2: Concetti Preliminari
%-------------------------------------------------------------------------------

\chapter{Concetti Preliminari}
\epigraph{
	"Anyone who has never made a mistake has never tried anything new."
}{--- \textup{Albert Einstein}}

Di seguito alcune definizioni essenziali necessarie per poter comprendere i
contenuti presentati nei capitoli successivi.

%-------------------------------------------------------------------------------
% Section: Introduzione allo studio dei segnali
%-------------------------------------------------------------------------------

\section{Introduzione allo studio dei segnali}
La teoria dei segnali studia le propriet\`a matematiche e statistiche dei
segnali, definiti come funzioni matematiche del tempo. In generale, un segnale
\`e una variazione temporale dello stato fisico di un sistema o di una grandezza
fisica (potenziale o corrente elettrica per segnali elettrici, parametri di
campo elettromagnetico per segnali radio) che serve per rappresentare e
trasmettere messaggi ovvero informazione a distanza; il sistema in questione
pu\`o essere il pi\`u disparato. In elettronica un segnale viene dunque studiato
attraverso un modello matematico o funzione in cui il tempo (o il suo inverso,
la frequenza) \`e considerato variabile indipendente.
\bigbreak
Un segnale \`e \textit{una qualunque grandezza fisica variabile cui \`e
associata una informazione}.
\bigbreak
In generale esistono diversi tipi di segnali, ma tutti sono accomunati
dall'essere in natura segnali casuali e continui e quasi mai deterministici. La
teoria dei segnali studia la rappresentazione dei segnali in modo da poter poi
manipolarli e trattarli matematicamente. Questa rappresentazione richiede l'uso
di matematica astratta e, nel caso di segnali stocastici, della teoria della
probabilit\`a.
\bigbreak
La teoria si suddivide in due grandi branche a seconda del tipo di segnale in
esame: i "segnali determinati" o deterministici, di cui è possibile predire il
valore in un qualunque istante a piacere, e i "segnali stocastici" o aleatori,
il cui valore non \`e prevedibile, ma su cui \`e possibile ottenere soltanto
delle propriet\`a statistiche e che rientrano nella pi\`u vasta tematica dei
processi aleatori o stocastici.
\bigbreak
Nella trasmissione di informazione a distanza (telecomunicazione) i segnali
determinati vengono utilizzati per la modulazione tramite portante, mentre i
segnali contenenti l'informazione sono invece segnali aleatori, quindi processi
stocastici, dal momento che l'informazione viaggia sotto forma di "innovazione"
ovvero varia in maniera aleatoria nel tempo.
\bigbreak
I segnali periodici possono essere trattati mediante l'astrazione in uno spazio
vettoriale lineare quale lo spazio di Hilbert e quindi con l'utilizzo della
serie di Fourier. Per quanto riguarda i segnali non periodici, questi
necessitano della trasformata di Fourier.
\bigbreak
Altra suddivisione \`e quella in "segnali continui" e "segnali discreti". Ad
essi si associano rispettivamente le comunicazioni analogiche e le comunicazioni
digitali.
\bigbreak
Parte della teoria dei segnali \`e intimamente connessa con la teoria dei
sistemi giacch\'e molti segnali transitano come input in sistemi che elaborano
ovvero trasformano il segnale in ingresso restituendo in uscita un certo output.
Centrale \`e anche l'analisi di Fourier ovvero l'analisi spettrale. 

%-------------------------------------------------------------------------------
% Subsection: Prorieta' elementari dei segnali determinati
%-------------------------------------------------------------------------------

\subsection{Propriet\`a elementari dei segnali determinati}
Limitiamo per il momento la nostra attenzione ai segnali determinati e definiamo
alcune grandezze di fondamentale importanza per il prosieguo dello studio.
Supponiamo di disporre di un resistore di resistenza $R$ attraversato da una
corrente $i(t)$; l'espressione della potenza istantanea dissipata sul resistore
per effetto Joule \`e, come \`e noto, $Ri^2(t)$. Osserviamo quindi la
proporzionalit\`a tra la potenza istantanea e il \textit{quadrato} del segnale;
il coefficiente di proporzionalit\`a \`e legato al particolare esempio.
Estendendo in maniera astratta tale definizione, diremo che al segnale $x(t)$
\`e associata una potenza istantanea \textit{normalizzata} (aggettivo che
verr\`a poi sistematicamente omesso) pari a
\begin{equation}
	P_x(t) = P_x \triangleq \abs{x(t)}^2.
\end{equation}
Inoltre, tornando all'esempio del resistore, l'energia totale dissipata per
effetto del passaggio della corrente $i(t)$ \`e pari a
$\int\displaylimits_{-\infty}^{+\infty} Ri^2(t) \dt$. Conseguentemente,
definiremo l'\textit{energia} associata al segnale $x(t)$ come
\begin{equation}
	E_x(t) = E_x \triangleq \int\displaylimits_{-\infty}^{+\infty}
	\abs{x(t)}^2 \dt = \int\displaylimits_{-\infty}^{+\infty} P_x(t) \dt
\end{equation}
purch\`e l'integrale risulti convergente (cio\`e $E_x < \infty$). La definizione
di energia, bench\`e meno intuitiva, viene banalmente estesa anche ai segnali a
tempo discreto:
\begin{equation}
	E_x \triangleq \sum_{n = -\infty}^{+\infty} \abs{x[n]}^2 < \infty.
\end{equation}
Per tutti i segnali \textit{fisici} (cio\`e effettivamente osservati)
l'integrale (o la sommatoria) che definisce l'energia risulta convergente,
poich\`e ogni segnale proveniente da un sistema fisico \`e portatore di
\textit{energia finita}. Molto spesso per\`o conviene considerare
\textit{modelli ideali} di segnale, ovvero segnali idealizzati non esistenti in
natura, ma assai utili per approssimare casi reali.\\
\\
Consideriamo ora un generico segnale $x(t)$ a valori limitati ma energia
infinita, e costruiamo il segnale $x_T(t)$ con una operazione di
\textit{troncamento} come segue:
\begin{equation}
	x_T(t) = \begin{cases}
			x(t) \quads{2} \abs{t} \leq T/2\\
			0 \quads{4} altrove
		 \end{cases}
\end{equation}
Se indichiamo con $E_{x_T}$ l'energia di $x_T(t)$, \`e chiaro che in generale
$E_{x_T} < \infty$ poich\`e il segnale \`e diverso da zero e assume valori
finiti solo su di un intervallo limitato. Altrettanto chiaro \`e che se
ingrandiamo l'intervallo di osservazione per comprendere l'andamento di tutto il
segnale $x(t)$ (imponiamo cio\`e $T \rightarrow \infty$) otteniamo
$E_{x_T} \rightarrow \infty$. Introduciamo allora il concetto di
\textit{potenza} di un segnale: la potenza media del segnale $x_T(t)$ valutata
sull'intervallo di osservazione $\left[-T/2, T/2\right]$ \`e per definizione
pari all'energia di $x_T(t)$ rapportata alla durata dell'intervallo stesso:
\begin{equation}
	P_{x_T} \triangleq \frac{E_{x_T}}{T}.
\end{equation}
Siamo ora in grado di estendere a $x(t)$ questa definizione di \textit{potenza
media} attraverso un'operazione di passaggio al limite:
\begin{equation}\label{eq:signal_power}
	P_x \triangleq \lim_{T \rightarrow \infty} P_{x_T} =
	\lim_{T \rightarrow \infty} \frac{E_{x_T}}{T} =
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2}
	\abs{x(t)}^2 \dt.
\end{equation}
Analogamente a quanto visto per l'energia di un segnale, per i segnali a tempo
discreto abbiamo
\begin{equation}
	P_x \triangleq \lim_{N \rightarrow \infty} \frac{1}{2N + 1}
	\sum_{n = -N}^{N} \abs{x[n]}^2
\end{equation}
dove la notazione \`e autoesplicativa.
Talvolta torna utile usare il \textit{valore efficace} di un segnale a potenza
finita, definito sia per i segnali a tempo continuo, sia per quelli a tempo
discreto come
\begin{equation}
	x_{eff} \triangleq \sqrt{P_x}.
\end{equation}
Ricordiamo che il valore efficace di un dato segnale (chiamato nei paesi
anglosassoni RMS, \textit{Root Mean Square}) si pu\`o interpretare come quel
valore che dovrebbe assumere un segnale costante per avere la stessa potenza del
segnale dato.\\
Definiamo infine il \textit{valore medio temporale} di un segnale, che richiede
un procedimento al limite simile a quello appena visto relativamente alla
potenza:
\begin{equation}
	x_m \triangleq \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} x(t) dt,
\end{equation}
\begin{equation}
	x_m \triangleq \lim_{N \rightarrow \infty} \frac{1}{2N + 1}
	\sum_{n = -N}^{N} x[n].
\end{equation}
Nell'ingegneria elettrica, il valore medio rappresenta la "componente continuo"
(cio\`e costante) attorno alla quale si svolge l'evoluzione del segnale.

%-------------------------------------------------------------------------------
% Subsubsection: Esercizio Lezione 12 Marzo 2018
%-------------------------------------------------------------------------------

\subsubsection{Esercizio 12 Marzo 2018}
Calcolare $E_y$, $P_y$, $y_{eff}$ e $y_m$ per la funzione $y(t)$ definita  come
\begin{equation}
	y(t) = x(t) - x(-t),
\end{equation}
dove $x(t)$ \`e definita come
\[
	x(t) = A e^{-t} \cdot u(t)
	\footnote{Il segnale canonico "gradino unitario" \`e presente
	nell'Appendice A: Segnali Canonici.}.
\]
Infine quindi
\begin{equation}
	y(t) = A e^{-t} \cdot u(t) - A e^{t} \cdot u(-t).
\end{equation}
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{images/esercizio_12_marzo_2018.png}
    \caption{Grafico di $y(t)$ con $A = 1$.}
\end{figure}
Come possiamo vedere graficamente, il segnale gradino $u(t)$ fa si che la
funzione complessiva $y(t)$ si ottenga come contributo delle due funzioni
\[
	A e^{-t} \quads{2} t \rightarrow +\infty,
\]
e
\[
	-A e^{t} \quads{2} t \rightarrow -\infty.
\]
\textbf{Calcolo di $\boldsymbol{E_y}$}\\
\[
	E_y = \int\displaylimits_{-\infty}^{+\infty} \abs{y(t)}^2 \dt =
	\int\displaylimits_{-\infty}^{+\infty} \abs{x(t) - x(-t)}^2 \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} \abs{A e^{-t} \cdot u(t) -
	A e^{t} \cdot u(-t)}^2 \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{0} \abs{A e^{-t} \cdot u(t) - A e^{t}
	\cdot u(-t)}^2 \dt + \int\displaylimits_{0}^{+\infty} \abs{A e^{-t}
	\cdot u(t) - A e^{t} \cdot u(-t)}^2 \dt =
\]
\[
	\footnote{Notiamo che $u(t) = 0$ $[-\infty, 0]$ e $u(-t) = 0$
	$[0, +\infty]$.}= \int\displaylimits_{-\infty}^{0} \abs{- A e^{t} \cdot
	u(-t)}^2 \dt + \int\displaylimits_{0}^{+\infty} \abs{A e^{-t} \cdot
	u(t)}^2 \dt =
\]
\[
	\footnote{Notiamo che $\abs{u(t)}^2 = \abs{u(-t)}^2 = 1$.}=
	\int\displaylimits_{-\infty}^{0} \abs{- A e^{t}}^2 \dt + 
	\int\displaylimits_{0}^{+\infty} \abs{A e^{-t}}^2 \dt = 
	\int\displaylimits_{-\infty}^{0} A^2 e^{2t} \dt + 
	\int\displaylimits_{0}^{+\infty} A^2 e^{-2t} \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{0} A^2 e^{2t} \dt +
	\int\displaylimits_{0}^{+\infty} A^2 e^{-2t} \dt =
	\int\displaylimits_{-\infty}^{+\infty} A^2 e^{-2\abs{t}} \dt =
\]
\[
	= 2 \int\displaylimits_{0}^{+\infty} A^2 e^{-2t} \dt = 2 A^2
	\int\displaylimits_{0}^{+\infty} e^{-2t} \dt = 2 A^2 \cdot
	\left(-\frac{1}{2} \left[e^{-2t} \right]_0^\infty\right) =
\]
\[
	2 A^2 \cdot \left(-\frac{1}{2} e^{-2\cdot(\infty)} - \left(-\frac{1}{2}
	e^{-2\cdot(0)}\right) \right) = 2 A^2 \cdot \frac{1}{2} = A^2.
\]
\bigbreak\noindent
\textbf{Calcolo di $\boldsymbol{P_y}$, $\boldsymbol{y_{eff}}$ e
$\boldsymbol{y_y}$}\\
Dato che $E_y = A^2 = K < \infty$ allora
\[
	P_y = 0,
\]
\[
	y_{eff} = 0,
\]
\[
	y_m = 0.
\]
\begin{proof}
\begin{equation}
\begin{split}
	P_y = \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} \abs{y(t)}^2 \dt = 
	\quads{13}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} \abs{A e^{-t} u(t) - A e^t u(-t)}^2
	\dt =
	\quads{10}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		\int\displaylimits_{-T/2}^{0} \abs{A e^{-t} u(t) -
		A e^t u(-t)}^2 \dt + \int\displaylimits_{0}^{T/2}
		\abs{A e^{-t} u(t) - A e^t u(-t)}^2 \dt
	\right] =
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		\int\displaylimits_{-T/2}^{0} \abs{- A e^t u(-t)}^2 \dt +
		\int\displaylimits_{0}^{T/2} \abs{A e^{-t} u(t)}^2 \dt
	\right] =
	\quads{7}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		\int\displaylimits_{-T/2}^{0} A^2 e^{2t} \dt + 
		\int\displaylimits_{0}^{T/2} A^2 e^{-2t} \dt
	\right] =
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2}
	A^2 e^{-2\abs{t}} \dt =
	\quads{3}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} 2 \int\displaylimits_{0}^{T/2}
	A^2 e^{-2t} \dt = \lim_{T \rightarrow \infty} \frac{1}{T} 2 A^2
	\int\displaylimits_{0}^{T/2} e^{-2t} \dt =
	\quads{8}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} 2 A^2
	\int\displaylimits_{0}^{T/2} \frac{1}{2} \cdot 2 \cdot e^{-2t} \dt =
	\lim_{T \rightarrow \infty} \frac{1}{T} A^2 \int\displaylimits_{0}^{T/2}
	2 \cdot e^{-2t} \dt =
	\quads{7}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} A^2 \left[
		e^{-2t}\right]_0^{T/2} = \lim_{T \rightarrow \infty}
		\frac{A^2}{T} \left[e^{-T} - 1\right] = 0.
	\quads{8}
\end{split}
\end{equation}
\begin{equation}
	y_eff = \sqrt{P_y} = 0.
\end{equation}
\begin{equation}
\begin{split}
	y_m = \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} y(t) \dt = \lim_{T \rightarrow \infty}
	\frac{1}{T} \int\displaylimits_{-T/2}^{T/2} x(t) - x(-t) \dt =
	\quads{4}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} A e^{-t} u(t) - A e^{t} u(-t) \dt =
	\quads{7}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		\int\displaylimits_{-T/2}^{0} A e^{-t} u(t) - A e^{t} u(-t) \dt
		+ \int\displaylimits_{0}^{T/2} A e^{-t} u(t) - A e^{t} u(-t) \dt
	\right] =
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		\int\displaylimits_{-T/2}^{0} - A e^{t} u(-t) \dt +
		\int\displaylimits_{0}^{T/2} A e^{-t} u(t) \dt
	\right] =
	\quads{4}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		\int\displaylimits_{-T/2}^{0} - A e^{t} \dt +
		\int\displaylimits_{0}^{T/2} A e^{-t} \dt
	\right] =
	\quads{7}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		-A \left(e^0 - e^{-T/2}\right) + A \left(-e^{-T/2} + e^0\right)
	\right] =
	\quads{5}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} \left[
		-A + Ae^{-T/2} - Ae^{-T/2} + A
	\right] = 0.
	\quads{6}
\end{split}
\end{equation}
\end{proof}

%-------------------------------------------------------------------------------
% Section: Segnali periodici a tempo continuo
%-------------------------------------------------------------------------------

\newpage
\section{Segnali periodici a tempo continuo}
Un segnale $x(t)$ \`e periodico se soddisfa la seguente relazione
\begin{equation}
	x(t) = x(t + T_0)
\end{equation}
per ogni valore della variabile $t$. La grandezza $T_0$ rappresenta il
\textit{periodo} del segnale che \`e legato alla \textit{frequenza di
ripetizione $f_0$} del segnale stesso dalla relazione
\begin{equation}
	f_0 = \frac{1}{T_0}.
\end{equation}
\textit{L'energia $E_x$} del segnale periodico \`e infinita:
\begin{equation}
\begin{split}
	E_x = \int\displaylimits_{-\infty}^{+\infty} \abs{x(t)}^2 \dt =
	\sum_{k = -\infty}^{+\infty}
	\int\displaylimits_{-T_0/2 + k T_0}^{T_0/2 + k T_0} \abs{x(t)}^2 \dt =
	\\
	= \sum_{k = -\infty}^{+\infty} E_{T_0} =
	\lim_{k \rightarrow \infty} k \cdot E_{T_0} = \infty.
	\quads{4}
\end{split}
\end{equation}
Dove si \`e posto
\begin{equation}
	E_{T_0} = \int\displaylimits_{-T_0/2 + k T_0}^{T_0/2 + k T_0}
	\abs{x(t)}^2 \dt.
\end{equation}
\textbf{Per la precedente (2.13) \`e di fondamentale importanza la seguente
considerazione:}
\begin{lemma}
L'integrale di una funzione periodica lungo intervalli pari al periodo stesso
della funzione \`e uguale indipendentemente dall'intervallo considerato.
\\
Ovvero, sia $x(t) = x(t + T_0)$ $\forall t$ un funzione periodica di periodo
$T_0$, allora
\begin{equation}
	\int\displaylimits_{0}^{T_0} x(t) \dt =
	\int\displaylimits_{b}^{b + T_0} x(t) \dt.
\end{equation}
\end{lemma}
\begin{proof}
Poniamo
\begin{equation}
	H(b) = \int\displaylimits_{b}^{b + T_0} x(t) \dt.
\end{equation}
Calcoliamo la derivata di $H(b)$:
\[
	\frac{dH(b)}{db} = x(b + T_0) - x(b) =
	\footnote{Essendo $x(t)$ periodica infatti risulta $x(b + T_0) = x(b)$.}
	\ 0.
\]
Ne segue che $H(b)$ \`e costante, in particolare $H(b) = H(0)$.
\end{proof}
In generale invece $x(t)$ ha \textit{potenza} $P_x$ finita, per calcolare la 
quale, non \`e necessario il procedimento di passaggio al limite definito dalla
\eqref{eq:signal_power}, ma \`e sufficiente calcolare
\begin{equation}
	P_x = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \abs{x(t)}^2
	\dt.
\end{equation}
\begin{proof}
\begin{equation}
\begin{split}
	P_x = \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \abs{x(t)}^2 \dt = 
	\lim_{k \rightarrow \infty} \frac{1}{kT_0} \cdot 
	\int\displaylimits_{-kT_0/2}^{kT_0/2} \abs{x(t)}^2 \dt =
	\\
	= \lim_{k \rightarrow \infty} \frac{1}{kT_0} \cdot k \cdot 
	\int\displaylimits_{-T_0/2}^{T_0/2} \abs{x(t)}^2 \dt = 
	\lim_{k \rightarrow \infty} \frac{1}{T_0} \cdot 
	\int\displaylimits_{-T_0/2}^{T_0/2} \abs{x(t)}^2 \dt =
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \abs{x(t)}^2 \dt.
	\quads{8}
\end{split}
\end{equation}
\end{proof}
\textbf{Con segnali periodici \`e bene fare attenzione a non confondere il
periodo $T_0$ della funzione $x(t)$ con il troncamento $T$ della funzione
stessa.}
\bigbreak
Mentre il \textit{valore efficace} di un segnale periodico segue dalla
definizione della potenza $P_x$
\begin{equation}
	x_eff = \sqrt{P_x},
\end{equation}
analogamente a quanto visto per la potenza, l'espressione del \textit{valor
medio} si semplifica, per il segnale periodico $x(t)$, come segue:
\begin{equation}
	x_m = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \dt.
\end{equation}
\begin{proof}
\begin{equation}
\begin{split}
	x_m = \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} x(t) \dt = \lim_{k \rightarrow \infty}
	\frac{1}{kT_0} \int\displaylimits_{-kT_0/2}^{kT_0/2} x(t) \dt =
	\\
	= \lim_{k \rightarrow \infty} \frac{1}{kT_0} k 
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \dt = 
	\lim_{k \rightarrow \infty} \frac{1}{T_0} 
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \dt =
	\quad
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \dt.
	\quads{8}
\end{split}
\end{equation}
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Sviluppo in serie di Fourier in forma reale polare
%-------------------------------------------------------------------------------

\subsection{Sviluppo in serie di Fourier in forma reale polare}
Ci\`o premesso, ci poniamo una domanda: qual \`e il modo pi\`u appropriato di
procedere quando il segnale $x(t)$ \`e periodico con \textit{andamento
arbitrario}, e in particolare \textit{non sinusoidale}? La risposta a questo
quesito sta nella cosiddetta \textit{analisi di Fourier} che costituisce la
base della moderna teoria dei segnali. Infatti, sotto ipotesi piuttosto ampie,
che in seguito elencheremo, un segnale reale periodico qualunque pu\`o essere
espresso come \textit{somma di oscillazioni sinusoidali di ampiezza, frequenza e
fase opportune} cio\`e in una forma che richiami:
\begin{equation}\label{eq:periodic_signal_decomposition}
	x(t) = a_0 + a_1 \cos(2 \pi f_1 t + \vartheta_1) + a_2 \cos(2 \pi f_2 t
	+ \vartheta_2) + \dots
\end{equation}
In particolare, le frequenze di oscillazione includono in generale la "frequenza
zero" relative al termine costante, e sono \textit{multiple intere della
frequenza fondamentale $f_0$}, cosicch\'e la
\eqref{eq:periodic_signal_decomposition} diventa:\footnote{Il termine $A_0$ \`e
dato dal fatto che il coseno in $0$ valore $1$ mentre il seno vale $0$.}
\begin{equation}\label{eq:fourier_series_polar_form}
	x(t) = A_0 + 2 \sum_{n = 1}^{\infty} A_n \cos(2 \pi n f_0 t + \vartheta_n)
\end{equation}
dove per comodit\`a di notazione si \`e anche posto $A_0 = a_0$ e $2A_n = a_n$
per $n \geq 1$. Questa rappresentazione del segnale prende il nome di
\textit{sviluppo in seria di Fourier}; pi\`u precisamente la relazione
\eqref{eq:fourier_series_polar_form} costituisce l'\textit{espressione in forma
polare} dello sviluppo in serie di Fourier. Essa permette dunque di
rappresentare un segnale reale $x(t)$ come somma di una costante $A_0$ e di una
\textit{serie} il cui \textit{n}-esimo termine, detto \textit{n}-esima
\textit{oscillazione armonica} (o \textit{armonica tout-court}), ha ampiezza
$A_n > 0$, frequenza $nf_0$ (la \textit{n}-esima \textit{frequenza armonica}) e
fase iniziale $\vartheta_n$.
\bigbreak
Prima di procedere ulteriormente, da un punto di vista formale, \`e bene
dimostrare che la funzione $A_n \cos(2 \pi n f_0 t + \vartheta_n)$ sia
effettivamente periodica:
\[
	A_n \cos(2 \pi n f_0 t + \vartheta_n) = A_n \cos(2 \pi n f_0 (t - mT_0)
	+ \vartheta_n) \quad \forall m \in \mathbb{Z}.
\]
\begin{proof}
Ricordando che $f_0 \cdot T_0 = 1$ e che $T_{\cos(x)} = 2\pi$,
\begin{equation}
\begin{split}
	A_n \cos(2 \pi n f_0 (t - mT_0) + \vartheta_n) =
	\quads{5}
	\\
	= A_n \cos(2 \pi n f_0 t + \vartheta_n - 2 \pi n f_0 m T_0) =
	\quads{4}
	\\
	= A_n \cos(2 \pi n f_0 t + \vartheta_n - 2 \pi n m) = A_n
	\cos(2 \pi n f_0 t + \vartheta_n).
\end{split}
\end{equation}
\end{proof}
Evidentemente, ogni particolare segnale $x(t)$ sar\`a caratterizzato da
particolari insiemi di valori di $A_n$ e $\vartheta_n$. Dovremo quindi ricavare
formule utili per il calcolo delle ampiezze e delle fasi delle varie armoniche e
indicare condizioni matematiche che garantiscano la convergenza della serie
\eqref{eq:fourier_series_polar_form}. Il primo di questi problemi fu risolto dal
matematico L. Eulero attorno alla fine del 1700 in connessione con lo studio
delle corde vibranti. e fu ripreso alcuni anni pi\`u tardi da J.B. Fourier.
Quest'ultimo fu il primo a intuire l'importanza e la potenza della
rappresentazione \eqref{eq:fourier_series_polar_form}, che us\`o per risolvere
questioni di trasmissione del calore. La \textit{convergenza} della
\eqref{eq:fourier_series_polar_form} fu dimostrata in seguito in maniera
rigorosa da P.D. Dirichlet.

%-------------------------------------------------------------------------------
% Subsection: Sviluppo in serie di Fourier in forma complessa
%-------------------------------------------------------------------------------

\subsection{Sviluppo in serie di Fourier in forma complessa}
Per semplificare gli sviluppi analitici si preferisce usare una forma
alternativa della serie di Fourier. Richiamando le formule di Eulero delle
funzioni trigonometriche
\begin{equation}
	\cos(x) = \frac{e^{jx} + e^{-jx}}{2} \quads{2} , \quads{2} \sin(x) =
	\frac{e^{jx} - e^{-jx}}{2j},
\end{equation}
la \eqref{eq:fourier_series_polar_form} pu\`o essere riscritta come segue:
\begin{equation}\label{eq:fourier_series_complex_development}
\begin{split}
	x(t) = A_0 + 2 \sum_{n = 1}^{\infty} A_n
	\cos(2 \pi n f_0 t + \vartheta_n) =
	\quads{3}
	\\
	= A_0 + 2 \sum_{n = 1}^{\infty} A_n \ 
	\frac{e^{j(2 \pi n f_0 t + \vartheta_n)} + 
	e^{-j(2 \pi n f_0 t + \vartheta_n)}}{2} =
	\quads{2}
	\\
	= A_0 + \sum_{n = 1}^{\infty} A_n \left(
		e^{j(2 \pi n f_0 t + \vartheta_n)} + 
		e^{-j(2 \pi n f_0 t + \vartheta_n)}
	\right) =
	\quads{2}
	\\
	= A_0 + \sum_{n = 1}^{\infty} A_n \ e^{j(2 \pi n f_0 t + \vartheta_n)} +
	\sum_{n = 1}^{\infty} A_n \ e^{-j(2 \pi n f_0 t + \vartheta_n)} =
	\quad
	\\
	= A_0 + \sum_{n = 1}^{\infty} A_n \ e^{j\vartheta_n}e^{j 2 \pi n f_0 t}
	+ \sum_{n = 1}^{\infty} A_n \ e^{-j\vartheta_n}e^{-j 2 \pi n f_0 t} =
	\quad
	\\
	= A_0 + \sum_{n = 1}^{\infty} A_n \ e^{j\vartheta_n}e^{j 2 \pi n f_0 t}
	+ \sum_{n = -\infty}^{-1} A_{-n} \ 
	e^{-j\vartheta_{-n}}e^{j 2 \pi n f_0 t} =
\end{split}
\end{equation}
Definiamo ora le quantit\`a
\begin{equation}
\begin{split}
	X_0 \triangleq A_0
	\quads{6}
	\\
	X_n \ \triangleq A_n e^{j \vartheta_n} \quad , \quad n = 1, 2, 3, \dots
	\quad
	\\
	X_n \ \triangleq A_{-n} e^{-j \vartheta_{-n}} \quad , \quad n = \dots , 
	-2, -1.
\end{split}
\end{equation}
Se si effettuano le opportune sostituzioni nella
\eqref{eq:fourier_series_complex_development} si ricava
\begin{equation}\label{eq:fourier_series_complex_form}
	x(t) = X_0 + \sum_{n = 1}^{\infty} X_n \ e^{j 2 \pi n f_0 t} - 
	\sum_{n = -\infty}^{-1} X_n \ e^{j 2 \pi n f_0 t} = 
	\sum_{n = -\infty}^{\infty} X_n \ e^{j 2 \pi n f_0 t}
\end{equation}
che rappresenta l'\textit{espressione in forma complessa} della serie di
Fourier\footnote{Tale rappresentazione pu\`o essere estesa nella stessa forma
anche al caso di segnale $x(t)$ \textit{complesso}.}.
\bigbreak\noindent
Determiniamo ora una espressione per il calcolo del generico coefficiente di
Fourier $X_k$, dove $k$ deve intendersi \textit{fissato}. A tal fine
moltiplichiamo entrambi i membri della \eqref{eq:fourier_series_complex_form}
per il fattore $e^{-j 2 \pi k f_0 t}$
\begin{equation}
	x(t) \cdot e^{-j 2 \pi k f_0 t} = \sum_{n = -\infty}^{\infty} X_n
	e^{j 2 \pi n f_0 t} \cdot e^{-j 2 \pi k f_0 t}
\end{equation}
e integriamo il risultato in un intervallo pari al periodo $T_0$ del segnale
stesso:
\begin{equation}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot e^{-j 2 \pi k f_0 t} \dt
	= \int\displaylimits_{-T_0/2}^{T_0/2} \sum_{n = -\infty}^{\infty} X_n
	e^{j 2 \pi n f_0 t} \cdot e^{-j 2 \pi k f_0 t} \dt.
\end{equation}
Supponendo che la serie a secondo membro converga \textit{uniformemente} (cosa
che peraltro non \`e stata dimostrata fino a questo momento), possiamo
considerare il termine $\sum_{k = -\infty}^{\infty} X_n$ una costante:
\begin{equation}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot e^{-j 2 \pi k f_0 t} \dt
	= \sum_{n = -\infty}^{\infty} X_n \int\displaylimits_{-T_0/2}^{T_0/2}
	e^{j 2 \pi n f_0 t} \cdot e^{-j 2 \pi k f_0 t} \dt.
\end{equation}
Ovvero
\begin{equation}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot e^{-j 2 \pi k f_0 t} \dt
	= \sum_{n = -\infty}^{\infty} X_n \int\displaylimits_{-T_0/2}^{T_0/2}
	e^{j 2 \pi (n - k) f_0 t} \dt.
\end{equation}
Procediamo adesso con il calcolo dell'integrale a secondo membro. Ricordando che
$f_0 \cdot T_0 = 1$ si ha:
\[
	\int\displaylimits_{-T_0/2}^{T_0/2} e^{j 2 \pi (n - k) f_0 t} \dt =
	\footnote{$\int f(t) e^{f(t)} \dt = e^{f(t)}$ quindi si moltiplica e
	divide per $\frac{d}{dt} \left(j 2 \pi (n - k) f_0 t\right) = 
	j 2 \pi (n - k) f_0.$}
	\frac{1}{j 2 \pi (n - k) f_0} \int\displaylimits_{-T_0/2}^{T_0/2} 
	j 2 \pi (n - k) f_0 \ e^{j 2 \pi (n - k) f_0 t} \dt =
\]
\[
	= \frac{1}{j 2 \pi (n - k) f_0} \left[
		e^{j 2 \pi (n - k) f_0 t}
	\right]_{-T_0/2}^{T_0/2} = \frac{e^{j 2 \pi (n - k) f_0 (T_0/2)} - 
	e^{j 2 \pi (n - k) f_0 (-T_0/2)}}{j 2 \pi (n - k) f_0} =
\]
\[
	= \frac{e^{j \pi (n - k)} - e^{-j \pi (n - k)}}{j 2 \pi (n - k) f_0} =
	\footnote{$\frac{e^{j \pi (n - k)} - e^{-j \pi (n - k)}}{j 2} = 
	\frac{\sin[\pi(n - k)]}{2j}$.}
	\ \frac{\sin[\pi (n - k)]}{\pi (n - k) f_0}
\]
Il valore dell'integrale \`e pertanto nullo se $n \neq k$, essendo 
$\sin[\pi (n - k)] = 0$. Se $n = k$ allora si ottiene $\sin[0] = 0$ e il 
risultato perde di significato in quanto si ottiene una forma $\frac{0}{0}$. 
Tuttavia, ponendo $n = k$ direttamente nell'espressione di partenza si ricava 
che l'integrale cercato vale in questo caso $T_0$:
\[
	\int\displaylimits_{-T_0/2}^{T_0/2} e^{j 2 \pi (n - k) f_0 t} \dt =
		\begin{cases}
			T_0 \quad\quad n = k\\
			0 \quad\quad \ \ n \neq k
		\end{cases}
\]
Riprendendo quindi da dove avevamo lasciato la nostra ricerca per una
espressione per il generico coefficiente di Fourier,
\[
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot e^{-j 2 \pi k f_0 t} \dt
	= \sum_{n = -\infty}^{\infty} X_n \int\displaylimits_{-T_0/2}^{T_0/2}
	e^{j 2 \pi (n - k) f_0 t} \dt.
\]
\[
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot e^{-j 2 \pi k f_0 t} \dt
	= X_k T_0.
\]
Dalla quale si deduce infine, l'espressione del generico coefficiente di Fourier
$X_k$ data da:
\begin{equation}\label{eq:fourier_coefficient}
	X_k = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	e^{-j 2 \pi k f_0 t} \dt
\end{equation}
Questa relazione permette quindi di effettuare il calcolo dei
\textit{coefficienti della serie di Fourier} di un segnale $x(t)$ dato. In
particolare, per $k = 0$ si ha
\begin{equation}
	X_0 = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t) dt
\end{equation}
che coincide con l'espressione del valore medio $x_m$ del segnale.

%-------------------------------------------------------------------------------
% Subsection: Sviluppo in serie di Fourier in forma reale rettangolare
%-------------------------------------------------------------------------------

\subsection{Sviluppo in serie di Fourier in forma reale rettangolare}
Abbiamo dunque ricavato due possibili espressioni per la serie di Fourier, e
precisamente quella in forma polare \eqref{eq:fourier_series_polar_form} e
quella in forma complessa \eqref{eq:fourier_series_complex_form}; ne esiste
anche una terza, detta \textit{espressione in forma rettangolare}, che
ricaviamo di seguito. Sviluppando le funzioni cosinusoidali della
\eqref{eq:fourier_series_polar_form} si ha
\begin{equation}
\begin{split}
	x(t) = A_0 + 2 \sum_{n = 1}^{\infty} A_n \cos(2 \pi n f_0 t +
	\vartheta_n) =
	\quads{4}
	\\
	= A_0 + 2 \sum_{n = 1}^{\infty} A_n \left[\cos(2 \pi n f_0 t)
	\cos\vartheta_n - \sin(2 \pi n f_0 t) \sin\vartheta_n\right],
\end{split}
\end{equation}
dove \`e stata sfruttata l'identit\`a
$\cos(\alpha + \beta) = \cos\alpha \cos\beta - \sin\alpha \sin\beta$.\\
Se adesso si definiscono le quantit\`a
$a_0 \triangleq A_0$, $a_n \triangleq A_n \cos\vartheta_n$ e
$b_n \triangleq A_n \sin\vartheta_n$, con $n = 1, 2, \dots$,
si ricava la relazione cercata:
\begin{equation}
	x(t) = a_0 + 2 \sum_{n = 1}^{\infty} \left[a_n \cos(2 \pi n f_0 t) -
	b_n \sin(2 \pi n f_0 t)\right].
\end{equation}
I coefficienti dell'espressione in forma rettangolare $a_n$, $b_n$ sono legati a
quelli relativi all'espansione in forma complessa $X_n$ della relazioni
\begin{equation}
	a_n = \Re[X_n] = \frac{1}{T_0} \int\displaylimits_{[T_0]} x(t)
	\cos(2 \pi n f_0 t) \dt,
\end{equation}
\begin{equation}
	b_n = \Im[X_n] = -\frac{1}{T_0} \int\displaylimits_{[T_0]} x(t)
	\sin(2 \pi n f_0 t) \dt.
\end{equation}
Nelle equazioni precedenti, la notazione $\int\displaylimits_{[T_0]}$ sta a
indicare che l'integrale pu\`o essere esteso a un qualunque intervallo temporale
di ampiezza $T_0$. Per ragioni di simmetria, \`e buona norma scegliere
l'intervallo $[-T_0/2, \ T_0/2]$.

%-------------------------------------------------------------------------------
% Subsection: Il criterio di Dirichlet
%-------------------------------------------------------------------------------

\subsection{Il criterio di Dirichlet}
Ricordiamo che negli sviluppi analitici necessari per ottenere l'espressione del
generico coefficiente di Fourier \eqref{eq:fourier_coefficient}
\begin{equation}
	X_k = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	e^{-j 2 \pi k f_0 t} \dt
\end{equation}
\`e stata ipotizzata la convergenza uniforme della serie ottenuta in
\eqref{eq:fourier_series_complex_form}
\begin{equation}
	\sum_{n = -\infty}^{\infty} X_n e^{j 2 \pi n f_0 t}.
\end{equation}
Per i segnali che si incontrano comunemente nella applicazioni pratiche, questa
ipotesi \`e sempre verificata; spesso per\`o, per schematizzare fenomeni fisici,
si fa ricorso a funzioni che non rappresentano esattamente i segnali in esame,
ma che offrono il vantaggio non indifferente di una maggiore
\textit{semplicit\`a}. Per tali funzioni, tuttavia, non \`e pi\`u assicurata in
generale la possibilit\`a di uno sviluppo in serie di Fourier e diventa quindi
necessario disporre di criteri che garantiscano la correttezza di tale sviluppo.
\bigbreak
Un insieme di condizioni sufficienti che garantiscano la possibilit\`a di
sviluppare un segnale in serie di Fourier \`e il cosiddetto \textit{criterio di
Dirichlet} che pu\`o essere enunciato come segue:
\begin{itemize}
	\item se $x(t)$ \`e assolutamente integrabile\footnote{Una funzione
		assolutamente integrabile su un intervallo \`e una funzione per
		la quale esiste finito l'integrale del valore assoluto della
		funzione sull'intervallo di integrazione considerato.} sul
		periodo $T_0$, vale a dire se verifica la condizione
		\begin{equation}
			\int\displaylimits_{-T_0/2}^{T_0/2} \abs{x(t)}^2 \dt <
			\infty.
		\end{equation}
	\item se $x(t)$ \`e continuo o presenta in un periodo un numero finito
		di discontinuit\`a di prima specie;
	\item se $x(t)$ \`e derivabile rispetto al tempo nel periodo, escluso al
		pi\`u un numero finito di punti nei quali esistono finite la
		derivata destra e sinistra,\\
	\\
	\textit{allora la serie di Fourier converge al valore assunto dalla
		funzione $x(t)$ nei punti in cui questa \`e continua, e alla
		semisomma dei limiti destro e sinistro nei punti in cui $x(t)$
		presenta le eventuali discontinuit\`a di prima specie.}\\
	\\	
	La terza ipotesi del criterio pu\`o anche essere sostituita con la
		seguente, che risulta del tutto equivalente:
	\item se il segnale presenta un numero finito di massimi e minimi nel
		periodo.
\end{itemize}

%-------------------------------------------------------------------------------
% Subsection: Spettri di ampiezza e di fase
%-------------------------------------------------------------------------------

\subsection{Spettri di ampiezza e di fase}
Dunque, ogni segnale $x(t)$ che soddisfi il criterio di Dirichlet pu\`o essere
rappresentato con lo sviluppo in serie di Fourier
\begin{equation}
	x(t) = \sum_{n = -\infty}^{\infty} X_n e^{j 2 \pi n f_0 t},
\end{equation}
dove il particolare coefficiente $X_k$ della serie \`e dato da
\begin{equation}
	X_k = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	e^{-j 2 \pi k f_0 t} \dt.
\end{equation}
Naturalmente, la sequenza $X_n$ \`e in generale \textit{complessa}; per
rappresentarla \`e conveniente tracciare due grafici che prendono il nome di
\textit{spettro di ampiezza} e \textit{spettro di fase}\footnote{Il termine
"spettro" deve intendersi nel significato di "gamma di rappresentazione,
gamma di visione" e nasce in fisica nel campo della spettroscopia in cui si
\textit{analizza} la composizione dei materiali attraverso le "righe" di
emissione caratteristiche dei diversi elementi chimici.}. Il primo illustra
l'andamento dell'ampiezza (modulo) dei coefficienti $X_k$, il secondo ne
illustra l'andamento della fase, entrambi in funzione dell'ordine $k$ del
coefficiente o del valore della $k$-esima frequenza armonica $k f_0$. Esempi
stilizzati di queste rappresentazioni sono riportati nelle seguenti figure:
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/spettro_di_ampiezza_e_di_fase.jpg}
	\caption{Spettro di ampiezza e di fase.}
\end{figure}

%-------------------------------------------------------------------------------
% Subsubsection: Spettro di un coseno
%-------------------------------------------------------------------------------

\subsubsection{Spettro di un coseno}
Consideriamo il segnale
\begin{equation}
	x(t) = A \cos(2 \pi f_0 t).
\end{equation}
Esso rappresenta un'oscillazione cosinusoidale di frequenza $f_0$; il periodo
del segnale \`e $T_0 = 1/f_0$. Ricaviamo i coefficienti di Fourier:
\begin{equation}
	\begin{split}
		X_n = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
		e^{-j 2 \pi n f_0 t} = \frac{1}{T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} A \cos(2 \pi f_0 t)
		e^{-j 2 \pi n f_0 t} \dt =
		\\
		= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} A \ 
		\frac{e^{j 2 \pi f_0 t} + e^{-j 2 \pi f_0 t}}{2} \ 
		e^{-j 2 \pi n f_0 t} \dt =
		\quads{6}
		\\
		= \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		e^{j 2 \pi f_0 (1 - n) t} \dt + \frac{A}{2 T_0} 
		\int\displaylimits_{-T_0/2}^{T_0/2} e^{j 2 \pi f_0 (1 + n) t} 
		\dt =
		\quads{4}
		\\
		= \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \left( 
		\cos[2 \pi f_0 (1 - n) t] + j \sin[2 \pi f_0 (1 - n) t] \right)
		\dt +
		\quads{3}
		\\
		+ \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \left(
		\cos[2 \pi f_0 (1 + n) t] - j \sin[2 \pi f_0 (1 + n) t] \right)
		\dt =
		\quads{3}
		\\
		= \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		\cos[2 \pi f_0 (1 - n) t] \dt + \frac{A}{2 T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 - n) t]
		\dt +
		\\
		+ \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		\cos[2 \pi f_0 (1 + n) t] \dt - \frac{A}{2 T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 + n) t]
		\dt =
	\end{split}
\end{equation}
Osserviamo a questo punto che
\begin{equation}
	\begin{split}
		\frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \cos[2 \pi f_0 (1 - n) t] \dt =
			\begin{cases}
				\frac{A}{2 T_0} \cdot T_0 = \frac{A}{2} \quad\quad n = 1\\
				\frac{A}{2 T_0} \cdot 0 = 0 \quad\quad \ \ \ n \neq 1
			\end{cases},
		\\
		\frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 - n) t] \dt =
			\begin{cases}
				\frac{A}{2 T_0} \cdot 0 = 0 \quad\quad n = 1\\
				\frac{A}{2 T_0} \cdot 0 = 0 \quad\quad n \neq 1
			\end{cases}
			\quad
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		\frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \cos[2 \pi f_0 (1 + n) t] \dt =
			\begin{cases}
				\frac{A}{2 T_0} \cdot T_0 = \frac{A}{2} \quad\quad n = -1\\
				\frac{A}{2 T_0} \cdot 0 = 0 \quad\quad \ \ \ n \neq -1
			\end{cases},
		\\
		-\frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 + n) t] \dt =
			\begin{cases}
				-\frac{A}{2 T_0} \cdot 0 = 0 \quad\quad n = -1\\
				-\frac{A}{2 T_0} \cdot 0 = 0 \quad\quad n \neq -1
			\end{cases}
	\end{split}
\end{equation}
Ne segue quindi che
\begin{equation}
	\begin{split}
		= \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} 
		\cos[2 \pi f_0 (1 - n) t] \dt + \frac{A}{2 T_0} 
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 - n) t] 
		\dt +
		\\
		+ \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		\cos[2 \pi f_0 (1 + n) t] \dt - \frac{A}{2 T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 + n) t]
		\dt =
		\\
		=
		\begin{cases}
			\frac{A}{2} \quad\quad n = \pm 1\\
			0 \quad\quad \ n \neq \pm 1
		\end{cases}.
		\quads{10}
	\end{split}
\end{equation}
In conclusione,
\begin{equation}
	\begin{split}
		\abs{X_{\pm 1}} = \frac{A}{2} \ ,\\
		\angle X_{\pm 1} = 0.
	\end{split}
\end{equation}
Gli spettri di ampiezza e fase del segnale sono mostrati nella seguente figura:
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/cosine_power_phase_spectrum.jpg}
	\caption{Spettri di ampiezza e fase del segnali cosinusoidale.}
\end{figure}
Il calcolo pu\`o essere effettuato anche tramite un rapido ragionamento: se si
confronta l'espressione in forma polare della serie di Fourier di un segnale
generico
\begin{equation}
	x(t) = A_0 + 2 \sum_{k = 1}^{\infty} A_k
	\cos(2 \pi k f_0 t + \vartheta_k),
\end{equation}
comparandola a
\begin{equation}
	x(t) = A \cos(2 \pi f_0 t).
\end{equation}
\`e possibile vedere immediatamente che
\[
	A_0 = 0
\]
\[
	2 \sum_{k = 1}^{\infty} A_k \cos(2 \pi k f_0 t + \vartheta_k) = A
	\cos(2 \pi f_0 t)
	\footnote{Notare che $\cos(2 \pi f_0 t)$ ha fase iniziale nulla.}
	\Longrightarrow 
		\begin{cases}
			A_1 = \frac{A}{2}, \vartheta_1 = 0\\
			A_k, \vartheta_K = 0 \ \forall k \neq 1
		\end{cases}
\]
ovvero
\[
	X_1 = \frac{A}{2}, X_{-1} = \frac{A}{2};
	\quad X_k = 0 \ \forall k \neq \pm 1.
\]

%-------------------------------------------------------------------------------
% Subsubsection: Spettro di un seno
%-------------------------------------------------------------------------------

\subsubsection{Spettro di un seno}
Consideriamo il segnale
\begin{equation}
	x(t) = A \sin(2 \pi f_0 t).
\end{equation}
Esso rappresenta un'oscillazione sinusoidale di frequenza $f_0$; il periodo del
segnale \`e $T_0 = 1/f_0$. Ricaviamo i coefficienti di Fourier:
\[
	X_n = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	e^{-j 2 \pi n f_0 t} = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
	A \sin(2 \pi f_0 t) e^{-j 2 \pi n f_0 t} \dt =
\]
\[
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} A \ 
	\frac{e^{j 2 \pi f_0 t} - e^{-j 2 \pi f_0 t}}{2j} \ e^{-j 2 \pi n f_0 t}
	\dt =
\]
\[
	= \frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
	e^{j 2 \pi f_0 (1 - n) t} \dt - \frac{A}{j 2 T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} e^{j 2 \pi f_0 (1 + n) t} \dt =
\]
\[
	= \frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \left(
	\cos[2 \pi f_0 (1 - n) t] + j \sin[2 \pi f_0 (1 - n) t] \right) \dt +
\]
\[
	- \frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \left(
	\cos[2 \pi f_0 (1 + n) t] - j \sin[2 \pi f_0 (1 + n) t] \right) \dt =
\]
\begin{equation}
	\begin{split}
		= \frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		\cos[2 \pi f_0 (1 - n) t] \dt + \frac{A}{j 2 T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 - n) t]
		\dt +
		\\
		- \frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		\cos[2 \pi f_0 (1 + n) t] \dt + \frac{A}{j 2 T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 + n) t]
		\dt =
	\end{split}
\end{equation}
Osserviamo a questo punto che
\begin{equation}
	\begin{split}
		\frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \cos[2 \pi f_0 (1 - n) t] \dt =
			\begin{cases}
				\frac{A}{j 2 T_0} \cdot T_0 = \frac{A}{j 2} \quad\quad n = 1\\
				\frac{A}{j 2 T_0} \cdot 0 = 0 \quad\quad \ \ \ n \neq 1
			\end{cases},
		\\
		\frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 - n) t] \dt =
			\begin{cases}
				\frac{A}{j 2 T_0} \cdot 0 = 0 \quad\quad n = 1\\
				\frac{A}{j 2 T_0} \cdot 0 = 0 \quad\quad n \neq 1
			\end{cases}
			\quad
	\end{split}
\end{equation}
\begin{equation}
	\begin{split}
		-\frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \cos[2 \pi f_0 (1 + n) t] \dt =
			\begin{cases}
				-\frac{A}{j 2 T_0} \cdot T_0 = -\frac{A}{j 2} \quad\quad n = -1\\
				-\frac{A}{j 2 T_0} \cdot 0 = 0 \quad\quad \ \ \ \ \ \ n \neq -1
			\end{cases},
		\\
		\frac{A}{j 2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 + n) t] \dt =
			\begin{cases}
				\frac{A}{j 2 T_0} \cdot 0 = 0 \quad\quad n = -1\\
				\frac{A}{j 2 T_0} \cdot 0 = 0 \quad\quad n \neq -1
			\end{cases}
			\quads{2}
	\end{split}
\end{equation}
Ne segue quindi che
\begin{equation}
	\begin{split}
		= \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		\cos[2 \pi f_0 (1 - n) t] \dt + \frac{A}{2 T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 - n) t]
		\dt +
		\\
		+ \frac{A}{2 T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
		\cos[2 \pi f_0 (1 + n) t] \dt - \frac{A}{2 T_0}
		\int\displaylimits_{-T_0/2}^{T_0/2} j \sin[2 \pi f_0 (1 + n) t]
		\dt =
		\\
		=
		\begin{cases}
			\frac{A}{j 2} \quad\quad \ \ \ \ n = 1\\
			0 \quad\quad \ \ \ \ \ n \neq \pm 1\\
			-\frac{A}{j 2} \quad\quad n = -1
		\end{cases}.
		\quads{10}
	\end{split}
\end{equation}
In conclusione, ricordando che
\[
\frac{A}{j 2} \cdot \frac{j}{j} = -j \cdot \frac{A}{2},
\]
\[
-\frac{A}{j 2} \cdot \frac{j}{j} = j \cdot \frac{A}{2},
\]
possiamo scrivere che
\begin{equation}
	\abs{X_{\pm 1}} = \abs{\pm j \cdot \frac{A}{2}} = \frac{A}{2} \ ,
\end{equation}
\begin{equation}
	\angle X_{\pm 1} =
		\begin{cases}
			\angle -j \cdot \frac{A}{2} = \angle \frac{A}{2} e^{-j}
			= -\frac{\pi}{2}\\
			\angle \ j \cdot \frac{A}{2} = \angle \frac{A}{2} e^{j}
			= \frac{\pi}{2}\\
		\end{cases}.
\end{equation}
Gli spettri di ampiezza e fase del segnale sono mostrati nella seguente
figura:\\
\textbf{Lo spettro di ampiezza \`e ovviamente uguale a quello del coseno visto
precedentemente, mentre lo spettro di fase \`e differente.}
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/sine_phase_spectrum.jpg}
	\caption{Spettro di fase del segnale sinusoidale.}
\end{figure}
Il calcolo pu\`o essere effettuato anche tramite un rapido ragionamento:
riscriviamo il segnale come
\begin{equation}
	x(t) = A \sin(2 \pi f_0 t) = A \cos(2 \pi f_0 t - \frac{\pi}{2}),
\end{equation}
ora, se si confronta l'espressione in forma polare della serie di Fourier di un
segnale generico
\begin{equation}
	x(t) = A_0 + 2 \sum_{k = 1}^{\infty} A_k
	\cos(2 \pi k f_0 t + \vartheta_k),
\end{equation}
comparandola a
\begin{equation}
	x(t) = A \cos(2 \pi f_0 t - \frac{\pi}{2}).
\end{equation}
\`e possibile vedere immediatamente che
\[
	A_0 = 0
\]
\[
	2 \sum_{k = 1}^{\infty} A_k \cos(2 \pi k f_0 t + \vartheta_k) = A
	\cos(2 \pi f_0 t - \frac{\pi}{2})
	\footnote{Notare che $\cos(2 \pi f_0 t - \frac{\pi}{2})$ ha fase
	iniziale $-\frac{\pi}{2}$.}
	\Longrightarrow 
		\begin{cases}
			A_1 = \frac{A}{2}, \vartheta_1 = -\frac{\pi}{2}\\
			A_k, \vartheta_K = 0 \ \forall k \neq 1
		\end{cases}
\]
ovvero
\[
	X_1 = \frac{A}{2} e^{-j\frac{\pi}{2}}, X_{-1} = \frac{A}{2}
	e^{j\frac{\pi}{2}}; \quad X_k = 0 \ \forall k \neq \pm 1.
\]

%-------------------------------------------------------------------------------
% Subsection: Matlab 2.1
%-------------------------------------------------------------------------------

\newpage
\subsection{MATLAB 2.1}
Verifichiamo le properit\`a della serie di Fourier (simmetria e linearit\`a)
considerando un esempoi di segnale periodico tratto dalla realt\`a. Prendiamo,
in particolare, il tracciato di un \textit{elettrocardiogramma}, che \`e stato
acquisito registrando l'attivit\`a elettrica del cuore di un paziente al variare
del tempo. Come \`e noto, la differenza di potenziale tra gli elettrodi
applicati sul corpo del paziente (cio\`e l'elettrocardiogramma) ha un andamento
tipo (pressoch\'e) periodico, in virtu\`u della regolarit\`a temporale con la
quale vengono prodotti gli impulsi da parte del miocardio.\\
Per verificare sperimentalmente quanto abbiamo appena introdotto, facciamo
ricorso al pacchetto software per l'analisi e la simulazione di segnali Matlab.
Riguardo all'uso di Matlab occorre fare alcune precisazioni: innanzitutto
osserviamo che fino ad ora abbiamo sempre ragionato in termini di segnali
\textit{analogici}, e in particolare di segnali a \textit{tempo continuo}.
Matlab \`e invece un simulatore \textit{a tempo discreto}, che fa uso di segnali
numerici o \textit{digitali}. Questi segnali a tempo discreto vengono introdotti
per \textit{emulare} o \textit{simulare} segnali a tempo continuo (che non
possono essere trattati dai computer digitali). Dunque in luogo di una forma
d'onda continua nel tempo $x(t)$, Matlab elabora un vettore temporale
$\boldsymbol{x}$ ottenuto, come nelle classiche procedure di calcolo numerico,
raccogliendo una successione di \textit{N} valori consecutivi ed equispaziati di
$x(t)$. Tali campioni vengono estratti da $x(t)$ su di un dato intervallo
(dominio) temporale con un certo quanto temporale $\Delta t$; in breve, si ha
che
$\boldsymbol{x} = [x(0), x(\Delta t), x(2 \Delta t), \dots, x((N - 1)\Delta t)]$
. Un vettore $N$-dimensionale come quello appena definito rappresenta la tipica
entit\`a trattata da Matlab per emulare un segnale analogico.
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/matlab_ecg_signal.jpg}
	\caption{Elettrocardiogramma.}
\end{figure}
La figura precedente riporta l'andamento nel tempo della differenza di
potenziale prodotta dall'attivit\`a cardiaca, registrata su di un paziente
che nel passato ha subito un infarto miocardico (segnali di questo tipo sono
facilmente estraibili da database disponibili online
\footnote{\href{https://www.physionet.org/physiobank/database/mitdb/}{
	https://www.physionet.org/physiobank/database/mitdb/}}).
Nel caso specifico, i dati sono immagazzinati nel file binario 'egc.dat', che
utilizza 16 bit per rappresentare ciascun campione temporale, con una spaziatura
temporale pari a $\Delta t = 20 ms$. Si consideri il seguente script Matlab
\lstinputlisting[frame=single, language=Matlab]{resources/ecg.m}
Le righe di codice
\begin{lstlisting}[frame=single, language=Matlab]
fOut = fopen('ecg.dat');
numeroCampioni = 5000;
x = fread(fOut, numeroCampioni, 'int16');
\end{lstlisting}
servono a leggere $500$ campioni dall'elettrocardiogramma e memorizzarli nel
vettore $\boldsymbol{x}$. A questo punto, \`e possibile visualizzare l'andamento
temporale del segnale con
\begin{lstlisting}[frame=single, language=Matlab]
deltaT = 0.02;
tempo = (1:numeroCampioni)*deltaT;
figure
plot(tempo, x);
\end{lstlisting}
Utilizzando la funzione \texttt{fft} in dotazione a Matlab, \`e possibile
valutare sperimentalmente lo spettro del segnale in esame:
\begin{lstlisting}[frame=single, language=Matlab]
lunghezzaFft = 2^nextpow2(numeroCampioni);
X=fft(x, lunghezzaFft);
X=[X(lunghezzaFft/2+1:lunghezzaFft) X(1:lunghezzaFft/2)];
frequenza = linspace(-0.5, 0.5, length(X))/deltaT;
figure
plot(frequenza, abs(X));
\end{lstlisting}
Il risultato delle precedenti operazioni \`e lo spettro di ampiezza illustrato
di seguito. Come possiamo facilmente vedere, sono presenti righe abbastanza
marcate (e simmetriche rispetto all'origine), che ci consentono di affermare che
lo spettro calcolato \`e quello di un segnale periodico. Possiamo identificare
l'armonica fondamentale $f_0$ a un frequenza praticamente uguale a $1 Hz$; ci\`o
suggerisce che la pulsazione cardiaca del paziente al momento
dell'elettrocardiogramma \`e pari a $60$ battiti al minuto, come del resto ci
potevamo aspettare valutando la distanza temporale $T_0$ tra due picchi
consecutivi del segnale. Tuttavia, lo spettro ottenuto non \`e esattamente a
righe come ci aspetteremo da un segnale periodico: le righe non sono isolate,
in quanto si manifestano componenti frequenziali anche tra un'armonica e
l'altra, e non sono neanche infinitamente strette. Questi fenomeni sono dovuti
al fatto che il battito cardiaco \`e soltanto \textit{approssimativamente}
periodico. In realt\`a anche su piccola scala temporale, l'attivit\`a miocardica
ha leggere irregolarit\`a temporali e quindi lo spettro risultante non \`e
perfettamente a righe.
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/matlab_ecg_spectrum.jpg}
	\caption{Elettrocardiogramma.}
\end{figure}

%-------------------------------------------------------------------------------
% Subsection: Matlab 2.2
%-------------------------------------------------------------------------------

\newpage
\subsection{MATLAB 2.2}
L'equazione di sintesi 
\begin{equation}\label{eq:fourier_series_complex_form}
	x(t) = \sum_{n = -\infty}^{\infty} X_n \ e^{j 2 \pi n f_0 t}
\end{equation}
richiede un numero illimitato di armoniche per ricostruire il segnale periodico
$x(t)$. Le considerazioni fatte per\`o suggeriscono che una approssimazione
soddisfacente del segnale pu\`o essere conseguita anche con un numero
\textit{finito} di armoniche. Possiamo verificare "sperimentalmente" l'effetto
che il troncamento a un numero finito $K$ di armoniche produce sulla sintesi del
segnale \textit{treno di impulsi}
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/treno_di_impulsi.png}
	\caption{Rappresentazione grafica del segnale treno di impulsi.}
\end{figure}
sfruttando nuovamente il pacchetto software Matlab. Per riprodurre l'equazione
di sintesi dobbiamo generare una rappresentazione vettoriale delle forme d'onda
$cos(2 \pi k f_0 t)$ al variare di $k$. In altre parole, occorre calcolare i
punti di queste curve utilizzando come supporto dei vettori temporali
sufficientemente "fitti" (cio\`e con $\Delta t$ sufficientemente piccolo), che
siano in grado di approssimare in maniera accurata l'andamento dei segnali a
tempo continuo.\\
Si consideri il seguente script Matlab
\lstinputlisting[frame=single, language=Matlab]{resources/fourier_approximation.m}
Di seguito i grafici del seguente script per $K = 3$, $K = 7$, $K = 15$, $K = 30$.
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/matlab_approssimazione_1.jpg}
	\caption{Approssimazione del treno di impulsi per $K = 3$.}
\end{figure}
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/matlab_approssimazione_2.jpg}
	\caption{Approssimazione del treno di impulsi per $K = 7$.}
\end{figure}
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/matlab_approssimazione_3.jpg}
	\caption{Approssimazione del treno di impulsi per $K = 15$.}
\end{figure}
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/matlab_approssimazione_4.jpg}
	\caption{Approssimazione del treno di impulsi per $K = 30$.}
\end{figure}
\`E interessante fornire la rappresentazione grafica dell'approssimazione che si
ottiene per un valore crescete di $K$ sino ad ottenere una quasi perfetta
rappresentazione del segnale treno di impulsi.
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/matlab_approssimazione_5.jpg}
	\caption{Approssimazione del treno di impulsi per $K = 300$.}
\end{figure}
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/matlab_approssimazione_6.jpg}
	\caption{Approssimazione del treno di impulsi per $K = 3000$.}
\end{figure}
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/matlab_approssimazione_7.jpg}
	\caption{Approssimazione del treno di impulsi per $K = 30000$.}
\end{figure}
In tutto questo volume gli esperimenti Matlab vengono usati come semplici
strumenti per chiarire ulteriormente i concetti esposti nel testo. Non vi
\`e nessuna pretesa di \textit{far apprendere} Matlab con le sue regole e/o i
suoi trucchi. Non possiamo pre\`o sottrarci, di quando in quando, a qualche
osservazione di carattere generale sulla programmazione. In particolare, il
codice presentato qui sopra merita alcune precisazioni riguardo allo stile di
scrittura. Come \`e possibile notare soprattutto nelle ultime righe, l'approccio
adottato fa un uso apprezzabile di \textit{matrici} e \textit{vettori}, a
scapito talvolta della leggibilit\`a del codice. Per esempio, invece di
utilizzare un ciclo \texttt{for} in funzione dei coefficienti \texttt{k} per
l'equazione di sintesi come in
\begin{lstlisting}[frame=single, language=Matlab]
% X_0 = 1/2
componenteContinua = 0.5;

% inizializzazione del vettore del segnale approssimante
ones(1, length(tempoNormalizzato));

% ciclo sui vari indici k
for k = 1:2:K
	% X_k
	coefficienteK = 2/(k*pi)*(-1)^((k-1)/2);
	
	% aggiornamento del vettore segnale approssimante mediante l'equazione di sintesi
	xApprossimante = xApprossimante + coefficienteK * cos(2*pi*k*tempoNormalizzato);
end
\end{lstlisting}
(e come si farebbe utilizzando un linguaggio di programmazione general-purpose,
per esempio C o C++), abbiamo usato il prodotto matriciale tra vettore
\texttt{coseni} e \texttt{coefficienti}. Il motivo di tale scelta risiede nel
fatto che Matlab, a differenza di altri ambienti di simulazione numerica e altri
linguaggi di programmazione, \`e ottimizzato per il calcolo matriciale. \`E
buona regola, quindi, sfruttare questa caratteristica, adottando uno stile di
programmazione conseguente, per ottenere un codice efficiente che venga eseguito
velocemente sulla piattaforma di calcolo utilizzata.

%-------------------------------------------------------------------------------
% Section: Segnali aperiodici a tempo continuo
%-------------------------------------------------------------------------------

\newpage
\section{Segnali aperiodici a tempo continuo}
Consideriamo il segnale detto \textit{treno di impulsi rettangolari di durata
$T$ e periodo $T_0$} ($T < T_0$) rappresentato nella seguente figura
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/treno_di_impulsi_rettangolari.jpg}
	\caption{Treno di impulsi rettangolari.}
\end{figure}
Per questo segnale si definisce il parametro \textit{duty-factor} (o 
\textit{duty-cycle}) $\delta = T/T_0$ che esprime il rapporto tra la durata $T$
di ciascun impulso e il periodo di ripetizione del segnale $T_0$.
\bigbreak
Per rappresentare pi\`u comodamente il treno di impulsi rettangolari \`e utile
definire la funzione
\begin{equation}
	rect(\alpha) \triangleq
		\begin{cases}
			1 \quad\quad \abs{\alpha} < \frac{1}{2}\\
			\frac{1}{2} \quad\quad \abs{\alpha} = \frac{1}{2}\\
			0 \quad\quad altrove
		\end{cases}
\end{equation}
il cui andamento (impulso rettangolare) \`e rappresentato nella seguente figura
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/rect_alpha.jpg}
	\caption{Grafico della funzione $rect(\alpha)$.}
\end{figure}
Ancora una volta, questa semplice funzione rappresenta un'astrazione matematica
utile per schematizzare impulsi che hanno \textit{tempo di salita} molto breve
rispetto alla propria \textit{durata}. Utilizzando questa funzione \`e possibile
scrivere la seguente espressione per il treno di impulsi rettangolari
\begin{equation}
	x(t) = \sum_{n = -\infty}^{+\infty} a \cdot rect\left(\frac{t - nT_0}{T}
	\right).
\end{equation}
Il segnale periodico \textit{treno di impulsi rettangolari} \`e infatti
rappresentato come la \textit{sovrapposizione} di infiniti impulsi di durata $T$
ottenuti ciascuno \textit{ritardando} l'impulso "base" non periodico $rect(t/T)$
di $nT_0$ secondi con $n = 0, \pm1,\dots$. In questo caso diremo che
l'impulso-base $rect(t/T)$ \`e stato \textit{periodicizzato} con periodo di
ripetizione $T_0$ per ottenere il segnale $x(t)$.
\bigbreak
Calcoliamo ora i coefficienti dello sviluppo in serie del segnale in esame:
\[
	X_n = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	e^{-j 2 \pi n f_0 t} \dt =\footnote{Calcolare il coefficiente per
	$rect(t/T)$ \`e equivalente a calcolarlo per
	$rect\left(\frac{t - nT_0}{T}\right)$ dato che la funzione \`e
	periodica.} \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} a 
	\cdot rect(t/T) e^{-j 2 \pi n f_0 t} \dt =
\]
\begin{equation}
	\begin{split}
		= \underbrace{\frac{1}{T_0} \int\displaylimits_{-T/2}^{T/2} a
		\cdot e^{-j 2 \pi n f_0 t} \dt}_{rect(t/T) \ = \ 1 \ \forall \ 
		t \ \in \ [-T/2, \ T/2]} = \frac{1}{T_0}
		\int\displaylimits_{-T/2}^{T/2} a \cdot
		\frac{-j 2 \pi n f_0}{-j 2 \pi n f_0} \cdot e^{-j 2 \pi n f_0 t}
		\dt =
		\\
		= \frac{a}{T_0} \cdot \frac{1}{-j 2 \pi n f_0}
		\int\displaylimits_{-T/2}^{T/2} (-j 2 \pi n f_0) \cdot
		e^{-j 2 \pi n f_0 t} \dt =
		\quads{4}
		\\
		= \frac{a}{T_0} \cdot \left[
			\frac{e^{-j 2 \pi n f_0 t}}{(-j 2 \pi n f_0)}
		\right]_{-T/2}^{T/2} = \frac{a}{T_0} \cdot
		\frac{e^{-j 2 \pi n f_0 t} - e^{j 2 \pi n f_0 t}}{(-j 2 \pi n f_0)} =
		\quads{3}
		\\
		= \frac{a}{T_0} \frac{e^{-j 2 \pi n f_0 t} - e^{j 2 \pi n f_0 t}}{(-j 2 \pi n f_0)}
		= \frac{a}{T_0} \frac{sin(\pi n f_0 T}{\pi n / T_0} =
		\quads{4}
		\\
		= \frac{a T}{T_0} \cdot \frac{sin(\pi n T/T_0)}{\pi n T/T_0}
		\quads{9}
		\\
	\end{split}
\end{equation}
Per esprimere $X_n$ in una forma pi\`u concisa, definiamo una ulteriore funzione
notevole
\begin{equation}
	sinc(\alpha) \triangleq \frac{sin(\pi \alpha)}{\pi \alpha},
\end{equation}
per cui
\begin{equation}
	X_n = \frac{aT}{T_0} sinc(\frac{kT}{T_0}) = a \ \delta \ sinc(k \delta),
\end{equation}
dove si \`e posto $\delta = T/T_0$ e $\alpha = kT/T_0$.

%-------------------------------------------------------------------------------
% Subsection: Dalla serie all'integrale di Fourier
%-------------------------------------------------------------------------------

\subsection{Dalla serie all'integrale di Fourier}
Il significato e l'importanza della rappresentazione in serie di Fourier di un
segnale periodico a tempo continuo sono stati ampiamente discussi nella sezione
precedente. Molti segnali che si osservano nei fenomeni naturali non sono per\`o
periodici. Sorge allora immediata la questione della possibilit\`a di ottenere
una scomposizione simile alla serie di Fourier \textit{anche per i segnali
aperiodici}. \textbf{\`E possibile cio\`e rappresentare anche un segnale non
periodico come una opportuna \textit{sovrapposizione} di segnali elementari, in
particolare sinusoidali?}
\bigbreak
Per rispondere a questa domanda, consideriamo come caso di studio il segnale
aperiodico appena introdotto impulso rettangolare:
\[
	x(t) = rect\left(\frac{t}{T}\right).
\]
Mettiamo ora in relazione questo segnale con il treno di impulsi rettangolari
\textit{periodico}
\begin{equation}
	x_p(t) = \sum_{n = -\infty}^{+\infty} x(t - nT_0) =
	\sum_{n = -\infty}^{+\infty} rect\left(\frac{t - nT_0}{T}\right)
\end{equation}
di cui gi\`a conosciamo la rappresentazione in serie di Fourier. Come \`e chiaro
, $x_p(t)$ \`e ottenuto \textit{periodicizzando} $x(t)$ con periodo di
ripetizione $T_0$, come suggerito nella seguente figura
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/treno_di_impulsi_rettangolari_2.jpg}
	\caption{Treno di impulsi rettangolari.}
\end{figure}
Il segnale originario $x(t)$ pu\`o essere considerato come una sorta di
caso-limite di un segnale periodico: partendo da $x_p(t)$, si riottiene
l'impulso "base" $x(t)$ centrato in $t = 0$ se si pensa di \textit{fare una
periodicizzazione di periodo $T_0 \rightarrow \infty$}. Al di l\`a del
particolare esempio, se si costruisce un segnale periodico $x_p(t)$ per
periodicizzazione del segnale aperiodico $x(t)$ \`e vero in generale che
\begin{equation}
	x(t) = \lim_{T_0 \rightarrow \infty} x_p(t).
\end{equation}
Naturalmente, il segnale $x_p(t)$, essendo periodico di periodo $T_0$, pu\`o
essere rappresentato mediante serie di Fourier tramite la ben nota relazione
\[
	x_p(t) = \sum_{n = -\infty}^{+\infty} X_n e^{j 2 \pi n f_0 t}
\]
con $f_0 = 1/T_0$ e con i coefficienti di Fourier $X_n$ dati da
\[
	X_n = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x_p(t)
	e^{-j 2 \pi n f_0 t} \dt.
\]
\textbf{Questo ci fa gi\`a intuire che, dato che il segnale $x(t)$ rappresenta
un caso limite del segnale $x(t)$, sia possibile ottenere una espressione
tramite i coefficienti di Fourier anche per il segnale aperiodico $x(t)$ come
caso limite dei coefficienti di Fourier del segnale periodico $x_p(t)$.}
\bigbreak
Nase adesso l'esigenza di stabilire il comportamento della serie di Fourier e
dei relativi coefficienti $X_n$ quando $T_0 \rightarrow \infty$.\\
Osserviamo innanzitutto che aumentando il periodo di ripetizione $T_0$ si riduce
la frequenza fondamentale $f_0 = 1/T_0$, e quindi si riduce la differenza tra due
generiche frequenze armoniche consecutive. Ci\`o determina un infittimento dello
spettro del segnale se la scala di rappresentazione delle frequenze resta la
stessa. Inoltre, dalla relazione 
\[
	X_n = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x_p(t)
	e^{-j 2 \pi n f_0 t} \dt
\]
\`e facile vedere che l'\textit{ampiezza} dei coefficienti tende a ridursi man
mano che $T_0$ cresce; al limite, per $T_0 \rightarrow \infty$, lo spettro di
$x_p(t)$ tende a divenire sempre pi\`u fitto e ad assumere valori sempre pi\`u
piccoli per tutte le frequenze armoniche.\\
Si pu\`o facilmente ovviare al problema della riduzione delle ampiezze delle
righe spettrali definendo, per ciascuna delle frequenze armoniche $nf_0$, una
sorta di "coefficiente di Fourier modificato"
\begin{equation}
	X(nf_0) \triangleq T_0 \cdot X_n = \int\displaylimits_{-T_0/2}^{T_0/2}
	x_p(t) e^{-j 2 \pi n f_0 t} \dt
\end{equation}
che evidentemente non \`e una quantit\`a che tende a zero per
$T_0 \rightarrow \infty$.\\
Riscriviamo dunque l'espansione in serie di Fourier di $x_p(t)$ usando il
coefficiente modificato\footnote{Dato che si \`e posto
$X(nf_0) \triangleq T_0 \cdot X_n$, ne segue che
$X_n = X(n f_0) \frac{1}{T_0} = X(n f_0) \cdot f_0$.}
\begin{equation}
	x_p(t) = \sum_{n = -\infty}^{+\infty} X(n f_0) e^{j 2 \pi n f_0 t}
	\cdot f_0.
\end{equation}
Possiamo adesso effettuare il passaggio cruciale al limite per
$T_0 \rightarrow \infty$ (ovvero per $f_0 \rightarrow 0$. Il segnale periodico
$x_p(t)$ al primo membro si trasforma nel segnale aperiodico $x(t)$. La somma a
secondo membro invece, per definizione, si trasforma in un \textit{integrale} e
si ottiene uno \textit{sviluppo} del segnale aperiodico $x(t)$ tramite
\begin{equation}\label{eq:fourier_integral}
	x(t) = \int\displaylimits_{-\infty}^{+\infty} X(f) e^{j 2 \pi f t} \df.
\end{equation}
Il segnale aperiodico \`e dunque rappresentabile attraverso il cosiddetto
\textbf{integrale di Fourier}. Resta da determinare l'espressione della funzione
$X(f)$ che compare nell'integrando della \eqref{eq:fourier_integral}.
Innanzitutto, \`e chiaro che tale quantit\`a risulta una funzione complessa
della variabile continua $f$, che mantiene il significato di \textit{frequenza}.
L'espressione di $X(f)$ si ottiene passando al limite per
$T_0 \rightarrow \infty$ nella definizione di $X(nf_0)$ del coefficiente di
Fourier modificato:
\begin{equation}\label{eq:tcf_coefficient}
	X(f) = \lim_{\begin{split}T_0 \rightarrow \infty \\ f_0 \rightarrow 0\end{split}}
		\int\displaylimits_{-T_0/2}^{T_0/2} x_p(t) e^{-j 2 \pi n f_0 t}
		\dt = \int\displaylimits_{-\infty}^{\infty} x(t)
		e^{-j 2 \pi n f t} \dt 
\end{equation}
che rappresenta la \textbf{trasformata continua di Fourier del segnale
$x(t)$}.\\
In maniera euristica, possiamo dire che la variabile continua $f$ \`e, in un
certo senso, il limite della variabile discreta $n f_0$ di partenza, quando
$f_0 \rightarrow 0$.
\bigbreak
\textbf{Commentiamo il risultato ottenuto.} Nella serie di Fourier per un
segnale periodico, quest'ultimo viene rappresentato mediante componenti
sinusoidali a frequenze in \textit{relazione armonica}, cio\`e tutti multiple di
un'unica fondamentale, nonch\`e di ampiezza \textit{finita}. Nel caso del
segnale aperiodico, la \eqref{eq:fourier_integral}, detta anche
\textit{antitrasformata di Fourier} (o \textit{trasformata inversa di Fourier}),
permette ancora di rappresentare il segnale aperiodico $x(t)$ come la
sovrapposizione di componenti sinusoidali, ma questa volta di ampiezza
\textit{infinitesima} $\abs{X(f)} \df$ e di frequenza $f$ variabile \textit{con
continuit\`a} su tutto l'asse reale. In altre parole, il segnale aperiodico \`e
visto come un segnale periodico "di periodo illimitato" e quindi con frequenza
fondamentale "infinitamente piccola".
\bigbreak
Riportiamo di nuovo le due equazioni relative alla rappresentazione del segnale
aperiodico, osservando che in generale indicheremo con la lettera maiuscola $X$,
$Y$, $Z$, la trasformata di Fourier rispettivamente di un segnale $x$, $y$, $z$:
\begin{equation}
	x(t) = \int\displaylimits_{-\infty}^{\infty} X(f) e^{j 2 \pi f t} \df
	\quad\quad , \quad\quad X(f) =
	\int\displaylimits_{-\infty}^{\infty} x(t) e^{-j 2 \pi f t} dt.
\end{equation}
La prima delle due rappresenta evidentemente un'equazione di \textit{sintesi}
che permette di rappresentale il segnale come sovrapposizione di segnali
elementari, ed \`e chiaramente analoga alla
\eqref{eq:fourier_series_complex_form} per i segnali periodici; la seconda \`e
un'equazione di \textit{analisi}, analoga alla \eqref{eq:fourier_coefficient},
che permette di determinare il \textit{peso} che le varie componenti
frequenziali (a tutte le possibili frequenze variabili con continuit\`a da
$-\infty$ a $+\infty$). hanno nella composizione di $x(t)$. Tali relazioni
mettono in corrispondenza un segnale del tempo con la propria trasformata di
Fourier, funzione a valori complessi della frequenza. Come d'uso anche con i
coefficienti di Fourier, questa relazione viene riassunta con la notazione
\begin{equation}
	x(t) \iff X(f).
\end{equation}
Un modo alternativo di indicare sinteticamente le operazioni di trasformata e
antitrasformata \`e quello mutuato alla notazione degli operatori caratteristica
dell'analisi funzione:
\begin{equation}
	X(f) = \mathcal{F}[x(t)] \quads{2} , \quads{2} x(t) =
	\mathcal{F}^{-1}[X(f)]
\end{equation}

%-------------------------------------------------------------------------------
% Subsubsection: Esempio
%-------------------------------------------------------------------------------

\subsubsection{Esempio}
Conseideriamo il segnale impulso rettangolare
\[
	x(t) = rect\left(\frac{t}{T}\right)
\]
e calcoliamone la trasformata di Fourier $X(f)$. questa \`e data da
\[
	X(f) = \int\displaylimits_{-\infty}^{+\infty} x(t) e^{-j 2 \pi f t} \dt
	= \int\displaylimits_{-\infty}^{+\infty} rect\left(\frac{t}{T}\right)
	e^{-j 2 \pi f t} \dt =
\]
\[
	= \int\displaylimits_{-T/2}^{T/2} rect\left(\frac{t}{T}\right)
	e^{-j 2 \pi f t} \dt = \int\displaylimits_{-T/2}^{T/2} 1 \cdot
	e^{-j 2 \pi f t} \dt =
\]
\[
	= \left[\frac{e^{-j 2 \pi f t}}{-j 2 \pi f}\right]_{-T/2}^{T/2} =
	\frac{\sin(\pi f T)}{\pi f}.
\]
Gli spettri di ampiezza e di fase del segnale $x(t)$ sono rappresentati nella
seguente figura
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/spettro_rect_alpha.jpg}
	\caption{Spetro \textbf{a)} di ampiezza e \textbf{b)} di fase
		dell'impulso rettangolare di ampiezza unitaria.}
\end{figure}

%-------------------------------------------------------------------------------
% Subsubsection: Esempio
%-------------------------------------------------------------------------------

\subsubsection{Esempio}
Consideriamo il segnale
\[
	x(t) = 2 \cdot rect\left(\frac{t}{2T}\right) +
	1 \cdot rect\left(\frac{t}{T}\right)
\]
graficamente rappresentabile come
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/esempio_14_03_2018.png}
	\caption{Segnale $x(t)$.}
\end{figure}
\noindent
Possiamo scomporre il nostro segnale come constributo di due segnali
\[
	x_1(t) = 2 \cdot rect\left(\frac{t}{2T}\right)
\]
\[
	x_2(t) = 1 \cdot rect\left(\frac{t}{T}\right)
\]
Calcoliamone la trasformata di Fourier.
\bigbreak\noindent
Sappiamo gi\`a che 
\[
	X_2(f) = T \cdot sinc(f T).
\]
Calcoliamo quindi la trasformata di $x_1(t)$
\[
	x_1(t) = 2 \cdot rect\left(\frac{t}{2T}\right) =
	2 \cdot rect\left(\frac{t}{T'}\right)
\]
da cui segue immediatamente che
\[
	X_1(f) = 2 \cdot T' \cdot sinc(f T') =
	2 \cdot 2T \cdot sinc(f 2T) =
	4T \cdot sinc(2 f T).
\]
Infine
\[
	X(f) = X_1(f) + X_2(f) =
	4T \cdot sinc(2 f T) + T \cdot sinc(f T).
\]

%-------------------------------------------------------------------------------
% Subsubsection: Esempio
%-------------------------------------------------------------------------------

\subsubsection{Esempio}
Consideriamo il segnale
\[
	x(t) = A \cdot rect\left(\frac{t - 3T}{T}\right)
\]
Graficamente rappresentabile come
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/esempio_14_03_2018_2.png}
	\caption{Segnale $x(t)$.}
\end{figure}
Per calcolarne la trasformata continua possiamo utilizzare il \textbf{teorema
del ritardo}.
\[
	X(f) = A \cdot T sinc(f T) \cdot e^{-j 2 \pi f 3T}.
\]

%------------------------------------------------
% Subsubsection: Esempio
%------------------------------------------------

\newpage
\subsection{Criteri di esistenza della Trasformata Continua di Fourier ($TCF$)}
Indichiamo adesso delle condizioni sufficienti per la rappresentazione del
segnale $x(t)$ attraverso la propria trasformata di Fourier $X(f)$, nel senso
gi\`a discusso riguardo la serie di Fourier. Se tali condizioni sono soddisfatte
\`e possibile affermare che la conoscenza dell'andamento nel tempo del segnale
$x(t)$ \`e equivalente alla conoscenza dell'andamento frequenziale della
relativa trasformata di Fourier.\\
Una prima condizione sufficiente afferma che se il segnale $x(t)$ ha energia
finita
\begin{equation}
	E_x = \int\displaylimits_{-\infty}^{+\infty} \abs{x(t)}^2 \dt < \ +\infty
\end{equation}
allora la trasformata $X(f)$ esiste, nel senso che l'integrale
\eqref{eq:tcf_coefficient} \`e convergente e la rappresentazione del segnale
come integrale di Fourier (antitrasformata) coincide quasi ovunque con il
segnale originario $x(t)$.\\
Un secondo criterio sufficiente meno restrittivo (criterio di Dirichlet) pu\`o
essere enunciato come segue:
\begin{itemize}
	\item \textit{se} il segnale $x(t)$ \`e assolutamente sommabile, ovvero
		$\int_{-\infty}^{\infty} \abs{x(t)} \dt < +\infty;$
	\item \textit{se} in qualunque intervallo finito $t_1 \leq t \leq t_2$
		il segnale $x(t)$ ha un numero finito di discotinuit\`a di
		prima specie;
	\item \textit{se} in qualunque intervallo finito $t_1 \leq t \leq t_2$
		il senglae $x(t)$ ha un numero finito di massimi e minimi;
\end{itemize}
\textit{allora} il segnale \`e rappresentabile come integrale di Fourier -
cio\`e l'antitrasformata della sua propria trasformata di Fourier $X(f)$.

%-------------------------------------------------------------------------------
% Subsection: Banda
%-------------------------------------------------------------------------------

\newpage
\subsection{Banda}
In informatica e in telecomunicazioni, il termine \textit{banda} indica la
quantit\`a di dati informativi che possono essere trasferiti, attraverso una
connessione, in un dato periodo di tempo, e la cui ampiezza \`e in analogia con
l'ampiezza di banda in campo fisico.
\[
	BANDA \ = \ \frac{DIMENSIONE \ \ DELLE \ \ INFORMAZIONI}
	{TEMPO \ \ DI \ \ TRASFERIMENTO}
\]
Pi\`u precisamente, nell'ambito della trasmissione, per banda si intende
l'intervallo di frequenze in cui il canale trasmette misurato dall'ampiezza di
banda $B$ e la velocit\`a di trasmissione espressta in $bit/s$, \`e propozionale
a tale banda $B$ a mezzo del parametro noto come efficienza spettrale.\\
Normalmente, la banda dipende dal tipo di mezzo fisico utilizzato e dalle sue
condizioni fisiche (interferenze, saturazione, ecc...), studiati nel campo delle
telecomunicazioni, rappresentando di fatto una risorsa limitata e in molti casi
anche condivisa tra pi\`u utenti.

%-------------------------------------------------------------------------------
% Subsubsection: Larghezza di banda
%-------------------------------------------------------------------------------

\subsubsection{Larghezza di banda}
In telecomunicazioni ed elettronica la larghezza di banda \`e la misura
dell'ampiezza di banda dello spettro di un segnale informativo trasmesso dalla
banda passante disponibile o utilizzata in un canale di comunicazione oppure,
la banda di lavoro di un certo sistema fisico in relazione alla sua risposta in
frequenza. La sua importanza in telecomunicazioni \`e legata al fatto che essa
\`e a sua volta strettamente legata alla velocit\`a di trasmissione dei dati:
la quantit\`a di informazioni trasmissibile sul canale \`e infatti strettamente
collegata all'intervallo di frequenze utilizzato nella trasmissione in base al
\textbf{teorema di campionamento di Nyquist-Shannon}.
\bigbreak
Nel caso delle comunicazioni analogiche, la banda si misura in modo indiretto,
ed \`e data dall'intervallo di frequenze occupato dal segnale (per esempio, una
comunicazioni telefonica analogica occupa le frequenze che vanno da $300 Hz$ a
$3400 Hz$, quindi ha una larhjezza di banda di $3100 Hz$ ovvero la differenza
tra $3400 Hz$ e $300 Hz$.

%-------------------------------------------------------------------------------
% Subsection: Convoluzione
%-------------------------------------------------------------------------------

\newpage
\subsection{Convoluzione}
In matematica, in particolare nell'analisi funzionale, la convoluzione \`e
un'operazione tra due funzioni di una variabile che consiste nell'integrare il
prodotto fra la prima e la seconda traslata di un certo valore.
\bigbreak
Formalmente si considerino due funzioni $f(t)$ e $g(t)$ definite da $\mathbb{R}$
in s\`e, con $f$ e $g$ integrabili secondo Lebesgue su $\mathbb{R}$. Si
definisce convoluzione di $f$ e $g$ la funzione definita nel seguente modo:
\[
    (f \cdot g)(t) \coloneqq \int\displaylimits_{-\infty}^{+\infty} f(\tau)
    \cdot g(t - \tau) \ d \tau = \int\displaylimits_{-\infty}^{+\infty}
    f(t - \tau) \cdot g(\tau) \ d \tau
\]
dove $\int\displaylimits_{-\infty}^{+\infty}$ denota l'integrale definito
nell'insieme dei numeri reali.\\
Le limitazioni poste alle funzioni $f$ e $g$ assicurano che l'integrale sia un
numero reale.
\bigbreak
La convoluzione viene utilizzata in vari campi della fisica, della statistica,
dell'elettronica, dell'analisi d'immagini e della grafica computerizzata.
Quando si studiano sistemi dinamici lineari stazionari, l'uscita è data dalla
convoluzione tra il segnale in ingresso e la risposta all'impulso del sistema,
la cui trasformata di Laplace (o la trasformata di Fourier) \`e la funzione di
trasferimento del sistema.
\bigbreak
A parole, si tratta dell'integrale del prodotto delle funzioni dopo che una
delle funzioni di partenza \`e stata rovesciata e traslata, e si pu\`o
considerare una forma di trasformata integrale.\\
Pi\`u in generale, si possono considerare due funzioni $f(t)$ e $g(t)$ definite
su $\mathbb{R}^d$ a valori in $\mathbb{C}$, la cui convoluzione\`e data da
\[
    (f \cdot g)(z) = \int\displaylimits_{\mathbb{R}^d}^{} f(y) g(x - y) \ dy =
    \int\displaylimits_{\mathbb{R}^d}^{} f(x - y) g(x) \ dy.
\]
\bigbreak
La convoluzione tra due segnali $x(t)$ e $y(t)$, reali o complessi, indicata
simbolicamente come
\[
    C_{xy}(\tau) = x(t) \ast y(t)
\]
\`e data, indifferentemente, dalle due espressioni
\[
    C_{xy}(\tau) = \int\displaylimits_{-\infty}^{+\infty} x(t) y(\tau - t) \dt.
\]
e
\[
    C_{xy}(\tau) = \int\displaylimits_{-\infty}^{+\infty} x(\tau - t) y(t) \dt.
\]
La convoluzione \`e un operatore lineare: questa propriet\`a \`e molto utile per
semplificare il calcolo di convoluzioni di segnalidecomponibili nella somma di
segnali pi\`u semplici.

%-------------------------------------------------------------------------------
% Subsection: Sommario
%-------------------------------------------------------------------------------

\newpage
\subsection{Sommario}
Questo capitolo ha dimostrato che l'analisi di Fourier \`e applicabile anche ai
segnali \textbf{aperiodici}. Immaginando infatti di ottenere un segnale
aperiodico come il \textbf{limite} di un segnale periodico quando il
\textbf{periodo di ripetizione $T_0$ tende a infinito}, si riesce a estendere
l'espansione in serie di Fourier, valida per il segnale periodico, anche ai
segnali non periodici, ottenendo cos\`i l'\textbf{integrale} di Fourier. In 
questa equazione di \textbf{sintesi}, il ruolo che nella serie giocavano i
coefficienti $X_k$ viene riservato alla \textbf{trasformata continua}
di Fourier $X(f)$ del segnale aperiodico $x(t)$. Il segnale \`e ancora scomposto
come una \textbf{sovrapposizione} di infinite componenti sinusoidali di
frequenza variabile \textbf{con continuit\`a} da $-\infty$ a $+\infty$, con fase
$\angle X(f)$ e con ampiezza \textbf{infinitesima} $\abs{X(f)} \df$. Molte delle
propriet\`a di simmetria degli spettri di ampiezza $A(f) = \abs{X(f)}$ e di fase
$\vartheta(f) = \angle X(f)$ del segnale aperiodico sono analoghe a quelle gi\`a
discusse per i segnali periodici.
\bigbreak
Sono state ricavate molte propriet\`a notevoli della trasformata continua di
Fourier, che permettono di calcolare lo spettro di un segnale che subisce
particolari operazioni di trasformazione: \textbf{ritardo},
\textbf{modulazione}, \textbf{combinazione lineare} tra pi\`u segnali, ed \`e
stata messa in evidenza la \textbf{dualit\`a} tra i domini del tempo e della
frequenza. Tra le propriet\`a pi\`u importanti, menzioniamo i teoremi di
\textbf{integrazione} e \textbf{derivazione}, secondo i quali operazioni di
carattere \textbf{differenziale} sul segnale temporale equivalgono a pi\`u
semplici operazioni \textbf{algebriche} sulle trasformate. Analoga
considerazione pu\`o farsi a proposito del teorema della \textbf{convoluzione},
per cui l'operazione di integrale di convoluzione nel tempo corrisponde a un
semplice \textbf{prodotto} in ambito frequenziale.
\bigbreak
La necessit\`a di estendere l'operazione di derivata anche in casi in cui
segnale temporale \`e \textbf{discontinuo} ha portato all'introduzione della
funzione generalizzata $\delta(t)$ di Dirac, formalmente definita come la
derivata della funzione gradino unitario. Da questa definizione, precisata poi
in senso limite, sono state ricavate numerose altre propriet\`a, come la
propriet\`a \textbf{campionatrice} e la \textbf{neutralit\`a} nei confronti
della convoluzione. Attraverso la $\delta(t)$ di Dirac, \`e possibile ricavare
le trasformate di Fourier \textbf{generalizzate} di segnali non trasformabili
in senso ordinario: il segnale costante, il gradino unitario, le funzioni seno e
coseno, e i segnali periodici. A quest'ultimo proposito, si \`e poi ricavata la
relazione di \textbf{campionamento in frequenza} che sussiste tra la trasformata
continua di un segnale aperiodico e i coefficienti di Fourier del segnale
periodico ottenuto \textbf{periodicizzando} il segnale aperiodico dato.
\bigbreak
Il capitolo si \`e chiuso infine con l'esame della relazione tra la trasofrmata
di Fourier $X(f)$ e la \textbf{trasformata di Laplace} $X(s)$ di un segnale
causale. Si \`e messo in luce, in particolare, che la trasformata di Fourier si
pu\`o direttamente ricavare da quella di Laplace solo quando la \textbf{zona di
convergenza} di quest'ultima comprende l'asse immaginario $s = j 2 \pi f$.

%-------------------------------------------------------------------------------
% Section: Sistemi monodimensionali a tempo continuo
%-------------------------------------------------------------------------------

\newpage
\section{Sistemi monodimensionali a tempo continuo}
Sistemi monodimensionali a tempo continuo.
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Segnali a tempo discreto
%-------------------------------------------------------------------------------

\section{Segnali a tempo discreto}
Segnali a tempo discreto.
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Sistemi monodimensionali a tempo discreto
%-------------------------------------------------------------------------------

\section{Sistemi monodimensionali a tempo discreto}
Sistemi monodimensionali a tempo discreto.
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Progetto di filtri digitali
%-------------------------------------------------------------------------------

\section{Progetto di filtri digitali}
Progetto di filtri digitali.
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Richiami di teoria della probabilita'
%-------------------------------------------------------------------------------

\newpage
\section{Richiami di teoria della probabilit\`a}
\textbf{Premessa}
\bigbreak
\noindent
Dobbiamo momentaneamente abbandonare lo studio dei segnali in senso stretto per
richiamare alcuni concetti matematici fondamentali alla compresione della natura
e delle particolari propriet\`a dei segnali aleatori. Questi ultimi infatti
vengono studiati attraverso gli strumenti della \textit{teoria della
probabilit\`a e delle variabili aleatorie}, che si presuppone gi\`a nota al
lettore. Lo scopo di questo capitolo \`e soltanto quello di \textit{richiamare}
i risultati principali della teoria della probabilit\`a, e di riformularli con
le notazioni che saranno poi riprese nel prossimo capitolo. Il lettore che si
ritiene ferrato su questi argomenti pu\`o usare questo capitolo soltanto come
\textit{rifetimento}, e pu\`o proseguire lo studio dei segnali procedendo
direttamente alla lettura del capitolo successivo.

%-------------------------------------------------------------------------------
% Subsection: Esperimenti deterministici e aleatori
%-------------------------------------------------------------------------------

\subsection{Esperimenti deterministici e aleatori}
Ogni volta che si devono compiere misurazioni per controllare il verificarsi di
certi avvenimenti, \`e necessario effettuare un \textit{esperimento}. La
definizione di esperimento che possiamo dare, cos\`i come potrebbe trovarsi su
di un vocabolario, \`e appunto quella di una \textit{prova pratica intesa alla
verifica di una certa ipotesi di lavoro}. Supponiamo che si desideri studiare la
"caduta di un grave", cio\`e di un corpo materiale, lasciato libero a una certa
altezza $h$ dal suolo, e in particolare che si desideri verificare la formula
galileiana del tempo di caduta $t_0 = \sqrt{2 \cdot h/g}$ (dove $g$ \`e
l'accelerazione di gravit\`a). Dobbiamo costruire un \textit{esperimento} che
consiste nell'effettuare una \textit{prova} di caduta, misurando con la massima
accuratezza possibile il tempo impiegato per arrivare al suolo. I dati raccolti
in molte prove permetteranno quindi di \textit{verificare l'ipotesi} che sta
alla vase dell'\textit{experimento} stesso. \bigbreak Dobbiamo per\`o notare una
importante diversit\`a di fondo tra certi tipi di esperimento e altri, che porta
alla introduzione di due diverse \textit{categorie} di esperimenti. Tornando
all'esempio precedente di caduta di un grave, e supponendo di effettuare molte
prove, notiamo che i \textit{risultati} che otteniamo in ogni prova sono molto
simili fra loro. Nel limite in cui l'effetto della resistenza dell'aria risulta
trascurabile o pu\`o essere a sua volta calcolato (ad esempio, conducendo
l'esperimento in ambiente controllato al chiuso e usando corpi sferici costruiti
con materiale ad alta densit\`a) otteniamo risultati praticamente
\textit{identici} di volta in volta. Possiamo allora dire che l'esperimento
condotto \`e di carattere \textit{deterministico}, nel senso che \`e possibile
\textit{prevedere} il risultato \textit{a priori}, cio\`e prima di effettuare
una prova dell'esperimento stesso. Quest'ultimo mostra dunque una completa
predicibilit\`a: esiste una \textit{legge} di carattere matematico che rende di
fatto inutile la materiale effettuazione di una prova perch\`e ne predice
accuramente il risultato. \bigbreak Sorvoliamo ovviamente sulle implicazioni che
questo concetto di determinismo ha sulla visione del mondo e della vita umana, e
procediamo presentando un diverso tipo di esperimento. Immaginiamo dunque di
sederci alla cassa di un supermercato e di contare il numero di clienti che si
presentano nell'intervallo di tempo di un'ora dalle $11:00$ alla $12:00$ di ogni
giorno. Questo pu\`o ritenersi a buon diritto un \textit{esperimento} in virt\`u
del quale \`e possibile verificare il numero di casse che sono necessarie per
evitare lunghe code. Sfortunatamente, questo esperimento d\`a risultati anche
molto diversi da prova a prova, e non \`e possibile \textit{prevedere} a priori
il risultato di nessuna delle prove effettuate. Il numero di persone osservato
di volta in volta cambia in maniera \textit{aleatoria}\footnote{L'aggettivo
\textit{aleatorio} deriva dal latino \textit{alea}, cio\`e dado, che \`e
ritenuto l'oggetto casuale per antonomasia.}, cio\`e casuale. Bisogna dunque
rinunciare a descrivere questo esperimento con una legge? La risposta
fortunatamente \`e \textit{no}. Ovviamente, sar\`a comunque impossibile trovare
una leggere che potr\`a \textit{predire} in ogni prova il numero di clienti
osservato: questo dato \`e ricavabile solo \textit{a posteriori}, cio\`e
\textit{dopo} che la prova stessa \`e stata effettuata. Quel che \`e possibile
fare in questo caso per\`o \`e predire il comportamento \textit{globale} dei
dati che si ottengono \textit{effettuando molte prove
dell'esperimento}\footnote{La legge dei grandi numeri oppure teorema di
Bernoulli (in quanto la sua prima formulazione \`e dovuta a Jakob Bernoulli),
descrive il comportamento della media di una sequenza di $n$ prove di una
variabile casuale, indipendenti e caratterizzate dalla stessa distribuzione di
probabilit\`a ($n$ misure della stessa grandezza, $n$ lanci della stessa moneta,
ecc\dots), al tendere ad infinito della numerosit\`a della sequenza stessa
($n$). In altre parole, grazie alla legge dei grandi numeri, possiamo fidarci
che la media sperimentale, che calcoliamo a partire da un numero sufficiente di
campioni, sia sufficientemente vicina alla media vera, ovvero quella calcolabile
teoricamente. Che cosa significhi "ragionevolmente sicuri" dipende da quanto
vogliamo essere precisi nel nostro test: con dieci prove, avremmo una stima
grossolana, con cento, ne otterremmo una molto pi\`u precisa, con mille, ancora
di pi\`u, e così via: il valore di $n$ che siamo disposti ad accettare come
sufficiente dipende dal grado di casualit\`a che riteniamo necessario per il
dado in questione.}. In quest'ultimo caso, infatti, i dati raccolti mostrano
quella che viene chiamata \textit{regolarit\`a statistica}. Nell'esperimento
aleatorio per antonomasia, il lancio di un dado, nessuno \`e in grado di predire
il risultato di una prova, e cio\`e la faccia che si presenta lanciando il dado
a un certo istante. L'esperienza per\`o suggerisce che se abbiamo la pazienza di
effettuare molti lanci del dado, diciamo $6000$ (possibilmente con un apposito
meccanismo!), osserviamo all'incirca $1000$ volte la faccia $1$, all'incirca
$1000$ volte la faccia 2, \dots, all'incirca $1000$ volte la faccia $6$. Questa
regolarit\`a permette di ricavare \textit{anche per l'esperimento aleatorio}
alcune leggi cui l'esperimento ottempera, per\`o nel senso statistico appena
accennato. \bigbreak Come l'analisi matematica tradizionale \`e lo strumento
matematico per eccellenza che descrive gli esperimenti deterministici (si pensi
alla relazione tra l'analisi infinitesimale e la dinamica dei corpi), cos\`i la
\textit{teoria della probabilit\`a} \`e lo strumento matematico sviluppato
appositamente per descrivere gli esperimenti aleatori. Quello che un tempo
veniva chiamato "calcolo delle probabilit\`a" nasce infatti tra il
diciassettesimo e il diciottesimo secolo a opera principalmente del matematico
svizzero J. Bernoulli e dei matematici francesi B. Pascal e P.S. de Laplace per
quantificare le vincite di giocatori e gestori dei giochi d'azzardo (dadi,
carte, estrazioni di palline ecc\dots). E a giudicare dai gudagni dei casin\`o
in tutto il mondo, \`e palese che questa teoria funziona alquanto bene!

%-------------------------------------------------------------------------------
% Subsection: Elementi di teoria della probabilit\`a
%-------------------------------------------------------------------------------

\subsection{Elementi di teoria della probabilit\`a}

%-------------------------------------------------------------------------------
% Subsubsection: Esperimento aleatorio, spazio di probabilita' e proprieta'
%                elementari
%-------------------------------------------------------------------------------

\subsubsection{Esperimento aleatorio, spazio di probabilit\`a e propriet\`a
elementari}
Immaginiamo dunque di effettuare un esperimento aleatorio. Vediamo come la
teoria della probabilit\`a permetta di \textit{modellare} questo esperimento, e
come si possano poi ricavare delle \textit{leggi} applicabili all'esperimento
stesso. Per caratterizzare tale esperimento dobbiamo individuare innanzitutto
l'insieme di tutti i suoi possibili risultati (ad esempio, le possibili facce
del dado, o il numero di clienti che si possono presentare in un'ora alla
cassa): tale insieme \`e detto \textit{spazio campione} e si indica,
convenzionalmente, con la lettera $\Omega$. Se l'esperimento prevede un numero
finito (dado) o infinito numerabile (clienti alla cassa) di risultati, questi
verranno indicati con il simbolo $\omega_i$, $i = 1, \dots$. Quindi, con la
notazione tipica della teoria degli insiemi, possiamo scrivere \[     \Omega =
{\omega_1, \ \omega_2, \ \dots}. \] Oltre che i singoli risultati
dell'esperimento, \`e spesso importante considerare anche dei \textit{gruppi} di
risultati. Ad esempio, nell'esperimento della cassa del supermercato, \`e
importnate considerare tutti i casi in cui il numero di clienti in un'ora \`e
maggiore (ad esempio) di $20$, perch\`e essi potrebbero rappresentare un numero
eccessivo per quella cassa. L'interesse nell'esperimento potrebbe essere allora
concentrato su questo \textit{evento}: "numero di clienti in un'ora superiore a
$20$", cio\`e su tutti quei risultati dello spazio campione contenuti nel
\textit{sottoinsieme} $\{21, 22, \dots\}$. I gruppi di risultati dello spazio
campione sono chiamati \textit{eventi}. Formalmente, gli \textit{eventi} sono
tutti i sottoinsiemi dello spazio campione che soddisfano le seguenti
condizioni:
\begin{itemize}
    \item se $A$ \`e un evento, anche il complemento $\overline{A}$, rispetto 
        all'insieme $\Omega$, \`e un evento;
    \item se $A$ e $B$ sono eventi, anche la loro unione $A \cup B$ \`e un
        evento.
\end{itemize}
Usando queste propriet\`a si pu\`o anche dimostrare che
\begin{itemize}
    \item l'intersezione $A \cap B$ di due eventi arbitrari $A$ e $B$ \`e un
        evento;
    \item dato un evento $A$, gli insiemi $A \cup \overline{A}$ e $A \cap 
        \overline{A}$ sono eventi, il primo, coincidente con $\Omega$ (ovvero 
        con tutto lo spazio campione), \`e detto \textit{evento certo}, mentre 
        il secondo, indicato con il simbolo $\varnothing$ e non centenente alcun
        risultato dell'esperimento, \`e detto \textit{evento impossibile}.
\end{itemize}
Le definizioni e propriet\`a sopra elencate dicono che gli evneti di uno spazio
campione costituiscono una \textit{classe} $S$, ovvero un insieme
\textit{chiuso} rispetto alle operazioni di unione e di intersezione. \bigbreak
A questo punto possiamo introdurre la \textit{caratterizzazione completa di un
esperimento aleatorio} che richiede sostanzialmente tre elementi: \textit{a}) la
descrizione del suo \textit{spazio campione} $\Omega$; \textit{b})
l'individuazione della sua \textit{classe degli eventi} $S$, e infine
\textit{c}) la descrizione della sua \textit{legge di probabilit\`a} $Pr(\cdot)$
che associa ad ogni evento una misura della sua probabilit\`a di presentazione.
La terna $(\Omega, S, Pr(\cdot))$ che rappresenta la descrizione
dell'esperimento \`e chiama \textit{spazio di probabilit\`a}. Qualche volte, con
libert\`a di linguaggio, identificheremo con \textit{esperimento aleatorio}
ci\`o che in realt\`a \`e lo \textit{spazio di probabilit\`a}, identificando
cio\`e l'esperimento con la propria descrizione matematica astratta.
\bigbreak
Non \`e il caso di esaminare qui le verie definizioni e interpretazioni della
probabilit\`a di un evento, che sono oggetto di discussione nei testi
specificamente dedicati alla teoria dell probabilit\`a e alla statistica (si
veda comunque a questo proposito l'esempio del \textbf{lancio di un dado non
truccato} presentato nelle pagine successive). Secondo lo scopo di questo
capitolo, ci limiteremo qui a richiamare le propriet\`a base della probabilit\`a
la cui conoscenza sar\`a indispensabile allo studio elementare dei processi
aleatori.
\bigbreak
La probabilit\`a di una classe di eventi pu\`o essere definita secondo la
\textit{teoria assiomatica} la cui forma moderna si pu\`o sostanzialmente far
risalire al matematico russo A.N. Kolmogorov. Secondo questo approcio, assegnato
un esperimento aleatorio con uno spazio campione $\Omega$ e la relativa classe
degli eventi $S$, una leggere di probabilit\`a $Pr(\cdot)$ \`e semplicemente una
\textit{corrispondenza} che associa a ogni elemento di $S$ cio\`e a ogni
\textit{evento} di interesse in una prova dell'esperimento, un numero reale che
soddisfa i seguenti \textit{assiomi}:
\begin{itemize}
	\item \textbf{A.1} - la probabilit\`a di un evento arbitrario $A$ \`e non
        negativa:
	    \[
		    Pr(A) \geq 0
	    \]
	\item \textbf{A.2} - la probabilit\`a dell'evento certo \`e unitaria (
        \textit{assioma di normalizzazione}):
	    \[
		    Pr(\Omega) = 1
	    \]
	\item \textbf{A.3} - dati due eventi $A$ e $B$ mutuamente esclusivi (o 
        incompatibili, o disgiunti, cio\`e che non possono verificarsi 
        contemporaneamente), la probabilit\`a dell'evento unione \`e data dalla 
        somma delle probabilit\`a di $A$ e $B$:
	    \[
		    A \cap B = \varnothing \Rightarrow Pr(A \cup B) = Pr(A) + Pr(B)
	    \]
\end{itemize}
Da questi \textit{assiomi} si possono poi ricavare alcune \textit{propriet\`a}
(cio\`e dimostrare alcuni \textit{teoremi} o corollari) che sembrano ovvie, ma
che devono comunque essere ricondotte ai soli principi primi (cio\`e agli
assiomi stessi):
\begin{itemize}
    \item Dato un evento $A$, la probabilit\`a dell'evento complementare
        $\overline{A}$ \`e data dal complemento a uno di $Pr(A)$:
        \[
            Pr(\overline{A}) = 1 - Pr(A);
        \]
    \item L'insieme impossibile ha probabilit\`a nulla di verificarsi;
    \item La probabilit\`a di un evento $A$ non pu\`o assumere un valore
        maggiore di uno:
        \[
            0 \leq Pr(A) \leq 1;
        \]
    \item Dati due eventi $A$ e $B$, la probabilit\`a dell'evento unione
        $A \cup B$ \`e espressa dall'uguaglianza
        \[
            Pr(A \cup B) = Pr(A) + Pr(B) - Pr(B \cap A).
        \]
\end{itemize}
L'intersezione fra due eventi $A$ e $B$ pu\`o anche essere rappresentata con la
scrittura $A \ B$, cos\`i come talvolta l'unione tra eventi viene indicata con
la scrittura $A + B$. La probabilit\`a $Pr(B \cap A) = Pr(A B)$ dell'evento
intersezione fra $A$ e $B$ \`e chiamata \textit{probabilit\`a congiunta} degli
eventi $A$ e $B$. Le probabilit\`a $Pr(A)$ e $Pr(B)$ sono dette, invece,
\textit{probabilit\`a marginali}.
\bigbreak
Data una coppia di eventi $A$ e $B$, con $Pr(B) \neq 0$, la probabilit\`a
$Pr(A|B)$ dell'evento $A$ \textit{condizionata} al verificarsi dell'evento $B$
\`e definita dalla relazione
\[
    Pr(A|B) \triangleq \frac{Pr(AB)}{Pr(B)}.
\]

%-------------------------------------------------------------------------------
% Subsubsection: Probabilita' Condizionata
%-------------------------------------------------------------------------------

\subsubsection{Probabilit\`a Condizionata}
In teoria della probabilit\`a la probabilit\`a condizionata di un evento $A$
rispetto ad un evento $B$ \`e la probabilit\`a che si verifichi $A$, sapendo che
$B$ \`e verificato. Questa probabilit\`a, indicata con $Pr(A | B)$ o $P_{B}(A)$,
esprime una "correzione" delle aspettative per $A$, dettata dall'osservazione di
$B$.\\
Poich\`e, come si vedr\`a nella successiva definizione, $Pr(B)$ compare al
suo denominatore, $Pr(A | B)$ ha senso se e solo se $B$ ha una probabilit\`a non
nulla di verificarsi.
\bigbreak
Data una coppia di eventi $A$ e $B$, con $Pr(B) \neq 0$, la probabilit\`a
$Pr(A|B)$ dell'evento $A$ condizionata al verificarsi dell'evento $B$ \`e
definita dalla relazione
\begin{equation}
    \begin{split}
        Pr(A | B) \triangleq \frac{Pr(AB)}{Pr(B)} = \frac{Pr(A \cap B)}{Pr(B)} =
        \\
        = \frac{Pr(A) + Pr(B) - Pr(A \cup B)}{Pr(B)}.
        \quads{2}
    \end{split}
\end{equation}
\bigbreak
Nella teoria della probabilit\`a $Pr(A)$ \`e detta comunemente probabilit\`a a
priori dell'evento $A$ e $Pr(A | B)$ probabilit\`a a posteriori di $A$ dato $B$.
La probabilit\`a condizionata (in alcuni testi chiamata anche condizionale) ha
un significato importante, che ruota attorno all'evento condizionante $B$.
Infatti, $Pr(A | B)$ \`e la probabilit\`a che l'evento $A$ assume una volta che
l'evento $B$ si \`e gi\`a verificato. La definizione suggerisce proprio questo:
la probabilit\`a a priori di $A$ viene scalata del fattore $1/Pr(B)$ per tenere
conto che l'evento $B$, essendosi gi\`a verificato, deve considerarsi come una
sorta di "nuovo spazio campione" in quanto al di fuori di questo niente pu\`o
verificarsi. In questo senso bisogna rinormalizzare tutte le probabilit\`a
rispetto a quello di $B$.\\
Si noti inoltre che $Pr(B | B) = 1$.
\bigbreak
Due eventi $A$ e $B$ sono \textit{indipendenti} se la probabilit\`a marginale
$Pr(A)$ e la probabilit\`a condizionata $Pr(A | B$ sono identiche, cio\`e in
pratica se il verificarsi dell'evento $B$ non ha alcuna influenza sull'evento
$A$:
\[
    Pr(A) = Pr(A|B)
\]
o, tenendo conto della definizione di probabilit\`a condizionata:
\[
    Pr(AB) = Pr(A) \cdot Pr(B).
\]
Pertanto, se gli eventi $A$ e $B$ sono indipendenti, la probabilit\`a congiunta
$Pr(AB)$ \`e pari al prodotto delle probabilit\`a marginali. La relazione
precedente viene utilizzata spesso per definire l'indipendenza fra due eventi,
anche se il suo significato non \`e immediatamente comprensibile com la prima.

%-------------------------------------------------------------------------------
% Subsubsection: Esempi Probabilita' Condizionata
%-------------------------------------------------------------------------------

\subsubsection{Esempi Probabilit\`a Condizionata}
\textbf{Esempio 1}\\
La probabilit\`a di ottenere $4$ con il lancio di un dado a sei facce (evento
$A$) ha probabilit\`a $Pr(A) = 1/6$ di verificarsi. Sapendo per\`o che il
risultato del lancio \`e un numero tra $4$, $5$, $6$ (evento $B$ verificato) la
probabilit\`a di $A$ diventa
\[
    Pr(A | B) \triangleq \frac{Pr(A B)}{Pr(B)} = \frac{Pr(A) + Pr(B) -
    Pr(A \cup B)}{Pr(B)}\footnote{Dati due eventi $A$ e $B$, la loro unione: 
    $A \cup B$ \`e data dall'evento "Almeno uno degli eventi $A$ e $B$ si
    verifica".}=
\]
\[
    = \frac{\frac{1}{6} + \frac{3}{6} - \frac{3}{6}}{\frac{3}{6}} =
    \frac{\frac{1}{6}}{\frac{3}{6}}.
\]
\bigbreak\noindent
\textbf{Esempio 2}\\
Si consideri questo secondo esempio: la probabilit\`a di ottenere $1$ con il
lancio di un comune dado (evento $A$) ha probabilit\`a $Pr(A) = 1/6$ di
verificarsi. Sapendo per\`o che il risultato del lancio \`e un numero tra $4$,
$5$, $6$ (evento $B$ verificato), la probabilit\`a di $A$ diventa
\[
    \frac{Pr(A \cap B)}{Pr(B)} = \frac{Pr(A) + Pr(B) - Pr(A \cup B)}{Pr(B)} 
    \footnote{La probabilit\`a dell'unione di due eventi incompatibili \`e
    uguale alla somma delle probabilit\`a di ciascun evento.}=
    \frac{1/6 + 3/6 - 4/6}{} = 0.
\]
\bigbreak\noindent
\textbf{Note Aggiuntive:} Dati due eventi mutuamente esclusivi (o
\textit{incompatibili} o \textit{disgiunti}, cio\`e che non possono verificarsi
contemporaneamente), la probabilit\`a dell'evento unione \`e data dalla somma
delle probabilit\`a di $A$ e $B$:
\[
    A \cap B = \emptyset \Longrightarrow Pr(A \cup B) = Pr(A) + Pr(B).
\]
Dati due eventi $A$ e $B$ conpatibili (quando cio\`e il verificarsi dell'uno non
esclude il verificarsi dell'altro), la probabilit\`a dell'evento unione
$A \cup B$ \`e espressa dall'uguaglianza:
\[
    Pr(A \cup B) = Pr(A) + Pr(B) - Pr(A \cap B).
\]

%-------------------------------------------------------------------------------
% Subsubsection: Teorema della Probabilita' Composta
%-------------------------------------------------------------------------------

\subsubsection{Teorema della Probabilit\`a Composta}
Il teorema della probabilit\`a composta deriva dal concetto di probabilit\`a
condizionata
\begin{equation}
    Pr(A \cap B) = Pr(B) Pr(A|B) = Pr(A) Pr(B|A),
\end{equation}
per cui la probabilit\`a che si verifichino entrambi i due eventi \`e pari alla
probabilit\`a di uno dei due eventi moltiplicato con la probabilit\`a dell'altro
evento condizionato al verificarsi del primo.
\bigbreak\noindent
Nel caso di indipendenza stocastica\footnote{Nell'ambito del calcolo delle
probabilit\`a, l'indipendenza stocastica di due eventi $A$ e $B$ si ha quando il
verificarsi di uno non modifica la probabilit\`a di verificarsi dell'altro,
ovvero quando la probabilit\`a condizionata $Pr(A | B)$ oppure $Pr(B | A)$\`e
pari rispettivamente a $Pr(A)$ e $Pr(B)$.} si ottiene che la probabilit\`a
congiunta \`e pari al prodotto delle probabilit\`a:
\begin{equation}
    Pr(A|B) = Pr(A) Pr(B).
\end{equation}
\bigbreak\noindent
Il teorema della probabilit\`a composta può essere generalizzato al caso
dell'intersezione di un numero arbitrario di eventi:
\begin{equation}
    Pr(A_1 \cap A_2 \cap A_3 \cap \dots \cap A_n) =
\end{equation}
\[
    Pr(A_1) Pr(A_2 | A_1) Pr(A_3 | A_1 \cap A_2) \dots Pr(A_n | A_1 \cap \dots
    \cap A_{n-1}).
\]

%-------------------------------------------------------------------------------
% Subsubsection: Esempio lancio di un dado non truccato
%-------------------------------------------------------------------------------

\subsubsection{Esempio lancio di un dado non truccato}
Cerchiamo di definire un esperimento aleatorio (nel senso della deifnizione
appena vista) che modelli il \textit{lancio di un dado non truccato}.
Evidentemente lo spazio compione $\Omega$ \`e costituito da $6$ risultati
$\{\omega_1, \omega_2, \dots, \omega_6\}$, dove $\omega_i$ corrisponde al
presentarsi al termine del lancio della faccia $i$-esima (cio\`e quella con nu
numero di puntini pari ad $i$). Poich\`e lo spazio campione $\Omega$ \`e
\textit{finito}, la classe degli eventi \textit{S} \`e semplicemente costituita
dalla raccolta di tutti i sottoinsiemi di $\Omega$ stesso (che sono in numero,
come \`e noto, di $2^6$, inclusi $\varnothing$ ed $\Omega$). La legge di
probabilit\`a degli eventi resta a questo punto definita non appena assegnamo
una probabilit\`a a ciascuno dei \textit{risultati} dello spazio $\Omega$. Fatto
ci\`o, \`e possibile calcolare la probabilit\`a di un qualunque evento $A$.
Sfruttando la \textit{simmetria} del problema, cio\`e l'ipotesi di dado non
truccato, \`e ragionevole imporre che
\[
    Pr(\{\omega_1\}) = Pr(\{\omega_2\}) = \dots = Pr(\{\omega_6\}) =
    \frac{1}{6}.
\]
Allora, ad esempio, la probabilit\`a dell'evento $A$ = \{\textit{la faccia del
dado \`e dispari}\} \`e
\[
    Pr(A) = Pr(\{\omega_1\}) \cup Pr(\{\omega_3\}) \cup Pr(\{\omega_5\})
\]
quindi, per il terzo assioma:
\[
    Pr(A) = Pr(\{\omega_1\}) + Pr(\{\omega_3\}) + Pr(\{\omega_5\}) = \frac{1}{6}
    + \frac{1}{6} + \frac{1}{6} = \frac{3}{6} = \frac{1}{2}.
\]
Per giustificare il fatto che tutte le facce del dado hanno la stessa
probabilit\`a di presentarsi, osserviamo che in casi come questo, di esperimento
\textit{simmetrico} e spazio campione finito, $Pr(A)$ pu\`o essere calcolata
attraverso la cosiddetta \textit{definizione classica di probabilit\`a},
attribuite a Pascal. Questa prevede di individuare il numero $N_F(A)$ dei
cosiddetti \textit{casi favorevoli} ad $A$, e il numero $N_P$ dei cosiddetti
\textit{casi possibili}. Quest'ultimo \`e semplicemente il numero totale di 
risultati contenuti in $\Omega$, mentre il primo \`e il numero dei risultati 
elementari contenuti in $A$ stesso. La probabilit\`a cercata \`e allora data dal
rapporto
\[
    Pr(A) = \frac{N_F(A)}{N_P}
\]
Considerando di nuovo $A =$ \{\textit{la faccia del dado \`e dispari}\}, \`e
chiaro che $N_F(A) = 3$ ed $N_P = 6$, e quindi,
\[
    Pr(A) = \frac{N_F(A)}{N_P} = \frac{3}{6} = \frac{1}{2}.
\]
L'ipotesi cruciale alla base della definizione "classica" \`e la
\textit{perfetta simmetria} del dado o, in altri termini,
l'\textit{equiprobabilit\`a} di tutti i possibili risultati dell'esperimento.
Questa definizione ha il pregio di essere molto semplice, ma pu\`o essere
applicata solamente a una classe ristretta di esperimenti. In particolare, essa
\`e incapace di modella il caso in cui il dado risulti "truccato", cio\`e le
varie facce (intenzionalmente o per imperfezioni di manifattura) \textit{non}
siano equiprobabili.
\bigbreak
In questi casi \`e pi\`u conveniente dare un'altra definizione di probabilit\`a,
che trova una sua giustificazione in un \textit{approcio di tipo sperimentrale}.
Consideriamo di nuovo l'esperimento del lancio del dado ed effetuiamo $N$
\textit{prove} dell'esperimento stesso (cio\`e lanciamo il dado $N$ volte).
Indichiamo poi con $N_A$ il numero di volte in cui, nelle suddette ripetizioni,
si verifica l'evento $A$ (cio\`e la faccia uscita mostra un numero dispari).
All'aumentare di $N$, ovvero del numero di lanci, si ottiene una situazione
simile a quella riassunta nella tabella seguente:
\begin{center}
    \begin{tabular}{| c | c | c |}
        \hline
        $N$ & $N_A$ & $N_A/N$ \\
        \hline
        $100$ & $47$ & $0.47$ \\
        \hline
        $1000$ & $491$ & $0.491$ \\
        \hline
        $10000$ & $4984$ & $0.4984$ \\
        \hline
        $100000$ & $50012$ & $0.50012$ \\
        \hline
        \dots & \dots & \dots \\
        \hline
    \end{tabular}
\end{center}
Si nota una cerca \textit{regolari\t`a} nella relazione tra il numeor di volte
in cui si verifica l'evento $A$ rispetto al numero totale di prove effettuate:
il rapporto
\[
    \frac{N_A}{N},
\]
detto \textit{frequenza relativa dell'evento} $A$, approssima il numero $1/2$ al
crescere di $N$. Da questa osservazione discende la \textit{definizione di
probabilit\`a di Von Mises} (o \textit{frequentista}), secondo la quale
\[
    Pr(A) = \lim_{N \rightarrow \infty} \frac{N_A}{N}.
\]
Tale definizione, rispetto alla definizione classica, presenta il vantaggio di
\textit{prescindere dalla simmetria del problema} e di poter modellare anche il
caso di dado "truccato", ma contiene un'operazione di limite che non si \`e in
grado di eseguire (e che pone anche questioni di esistenza).
\bigbreak
\`E interessante osservare che la definizione "frequentista" non \`e in
contrasto con quella assiomatrica di Kolmogorov. Infatti la probabilit\`a
$Pr(A)$, espresso con il limite che abbiamo appena introdotto, \`e
\textit{i}) una quantit\`a \textit{non negativa} poich\`e prodotta dal limite di
un rapporto fra quantit\`a positive; se inoltre \textit{ii}) l'evento $A$
coincide con l'evento certo $\Omega$, allora banalmente $N_A = N$ e quindi
$Pr(A) = 1$; se infine \textit{iii}) $A$ e $B$ sono due eventi mutuamente
esclusivi, una prova dell'esperimento che fa verificare $A \cup B$ d\`a un
risultato che sta o in $A$ o in $B$, ma che non pu\`o stare in entrambi, per cui
$N_{A \cup B} = N_A + N_B$ e allora
\[
    Pr(A \cup B) = lim_{N \rightarrow \infty} \frac{N_{A \cup B}}{N} =
    \lim_{N \rightarrow \infty} \frac{N_A + N_B}{N} =
\]
\[
    = \lim_{N \rightarrow \infty} \frac{N_A}{N} + \lim_{N \rightarrow \infty}
    \frac{N_A}{N} = Pr(A) + Pr(B),
\]
cio\`e tutti gli assiomi di Kolmogorov sono automaticamente verificati.

%-------------------------------------------------------------------------------
% Subsubsection: Teorema della Probabilita' Assoluta
%-------------------------------------------------------------------------------

\subsubsection{Teorema della Probabilit\`a Assoluta}
In teoria della probabilit\`a il teorema della probabilit\`a assoluta (detto 
anche teorema delle partizioni) afferma che se $A_1, \dots, A_n$ formano una
partizione dello spazio campionario di tutti gli eventi possibili $\Omega$
(ossia $A_i \cap A_j = \emptyset \ \forall i \neq j$ e $\cup_{i = 1}^{n} A_i =
\Omega$) e $B$ \`e un qualsiasi evento (dipendente dagli eventi $A_i$), allora:
\begin{equation}
    Pr(B) = \sum_{i = 1}^{n} Pr(A_i \cap B) = \sum_{i = 1}^{n} Pr(A_i)
    Pr(B | A_i).
\end{equation}
\textit{Vedi il teorema della probabilit\`a totale nel capitolo "Esame Orale".}

%-------------------------------------------------------------------------------
% Subsubsection: Esperimento aleatorio composto
%-------------------------------------------------------------------------------

\subsubsection{Esperimento aleatorio composto}
Consideriamo ora due \textit{diversi} esperimenti aleatori caratterizzati dagli
spazi campione $\Omega_1$ e $\Omega_2$ (ad esempio il lancio di un dado e
l'estrazione di una carta da un mazzo di $52$). \`E possibile definire un
\textit{esperimento composto} i cui risultati sono costituiti da una
\textit{coppia ordinata} dei risultati degli esperimenti componenti (ad esempio,
($faccia_3$ e $due_di_picche$)). Lo spazio campione $\Omega$ dell'esperimento
composto \`e costituito dal \textit{prodotto cartesiano} degli spazi dei due
esperimenti componenti, cio\`e $\Omega = \Omega_1 x \Omega_2$. Consideriamo,
adesso, un evento $A_1$ definito nello spazio campione $\Omega_1$ e un evento
$A_2$ definito in $\Omega_2$; vogliamo calcolare la probabilit\`a dell'evento
$A = A_1 x A_2$ appartenente allo spazio campione $\Omega$. Se i due esperimenti
sono \textit{indipendenti}, la probabilit\`a dell'evento composto $A$ \`e
\[
    Pr(A) = Pr_1(A_1) \times Pr_2(A_2)
\]
dove $Pr_1(\cdot)$ e $Pr_2(\cdot)$ rappresentano le leggi di probabilit\`a
definite rispettivamente per il primo e il secondo esperimenti componente. \`E
importante osservare che
\begin{itemize}
    \item dalla conoscenza delle leggi di probabilit\`a dei singoli esperimenti
        non \`e possibile, in generale, ricavare la legge di probabilit\`a
        dell'esperimento composto;
    \item le considerazioni appena illustrate per una coppia di esperimenti
        aleatori indipendenti possono essere immediatamente estese al caso di
        $n$ esperimenti aleatori indipendenti.
\end{itemize}
In questo ambito \`e utile ricordare il problema delle \textit{prove ripetute
binarie e indipendenti} (o \textit{prove di Bernoulli}). In tal caso
l'esperimento composto \`e costituito da $n$ esperimenti identici, indipendenti,
e aventi ciascuno uno spazio campione costituito da due risultati soltanto. In
questo modello ricadono numerosi esperimenti elementari (testa/croce,
vero/falso, 0/1, alto/basso, ecc\dots). Indichiamo con $\omega_0$ e $\omega_1$
questi risultati e con $p = Pr(\omega_0)$ e $q = Pr(\omega_1) 1 - p$ le loro
rispettive probabilit\`a, e definiamo l'evento $A =$ \{\textit{$\omega_0$ si
presenta $k$ volte in $n$ prove ripetute}\}. La \textit{formula di Bernoulli} (o
\textit{binomiale}) dice che
\[
    Pr(A) = {n \choose k} p^k q^{n - k} \quad , \quad 0 \leq k \leq n
\]
dove compare il coefficiente binomiale
\[
    {n \choose k} = \frac{n!}{k! (n - k)!}.
\]
\bigbreak
\noindent
\textbf{Esempio Formula di Bernoulli}\\
Una scatola contiene due monete: la prima \`e una moneta "perfetta", la seconda
\`e una moneta "truccata" avente $Pr(\{Testa\}) = 0.8$. Viene scelta casualmente
una delle due monete, che viene poi lanciata per dieci volte in condizioni
indipendenti, osservando l'uscita di 5 facce Testa e 5 facce Croce. Sulla base
di quest'ultima osservazione, qual \`e la probabilit\`a che la moneta scelta sia
quella "perfetta"? Cosa pu\`o dirsi di questa probabilit\`a se si osservano
$5000$ facce Testa su $1000$ lanci?\\
In questo esempio si deve calcolare la \textit{probabilit\`a condizionata}
$Pr(A|B)$ dove $B$ \`e l'evento osservato \{\textit{$5$ facce Testa, $5$ facce
Croce}\}, mentre $A$ \`e l'evento \{\textit{la moneta scelta \`e quella
perfetta}\}. Usando il teorema di Bayes \`e possibile ricavare $Pr(A|B)$ dalla
conoscenza delle probabilit\`a $Pr(B|A)$, $Pr(A)$, $Pr(B)$. Poich\`e la scelta
fra le due monete \`e casuale, abbiamo facilmente
\[
    Pr(A) = Pr(\overline{A}) = \frac{1}{2}.
\]
Supponiamo ora di aver effettivmanete scelto la moneta perfetta, cio\`e che si
sia verificato $A$. La probabilit\`a condizionata $Pr(B|A)$ pu\`o essere
calcolata mediante la formula di Bernoulli con $n = 10$, $k = 5$, $w_0$
corrispondente al presentarsi della faccia Testa e $w_1$ al presentarsi della
faccia Croce (le facce della moneta perfetta sono equiprobabili
$Pr(\{w_0\}) = Pr(\{w_1\}) = 0.5$), risulta
\[
    Pr(B|A) = {10 \choose 5} \left(\frac{1}{2}\right)^5
    \left(\frac{1}{2}\right)^{10 - 5} = {10 \choose 5} \left(
        \frac{1}{2}
    \right)^5
    \left(\frac{1}{2}\right)^5 = 
\]
\[
    {10 \choose 5} \frac{1}{2^{10}} = \frac{252}{1024} \cong 0.246.
\]
$Pr(B)$ pu\`o essere calcolata mediante il teorema della probabilit\`a totale,
con la partizione
\[
    \Omega = A \cup \overline{A},
\]
\[
    Pr(B) = Pr(B|A) Pr(A) + Pr(B|\overline{A}) Pr(A)
\]
Resta da ricavare la probabilit\`a $Pr(B|\overline{A})$ (cio\`e la probabilit\`a
di avere $5$ volte Testa su $10$ prove con la moneta \textit{truccata}), che
pu\`o di nuovo essere calcolata mediante la formula di Bernoulli con $n = 10$,
$k = 5$, $p = Pr(\{\omega_0\}) = 0.8$, $q = Pr(\{\omega_1\}) = 0.2$ (le facce
della moneta truccata non sono equiprobabili):
\[
    Pr(B|\overline{A}) = {10 \choose 5} (0.8)^5 (0.2)^5 = 252 \cdot (0.16)^5
    \cong 0.0264.
\]
Usando i valori appena ricavati possiamo finalmente determinare $Pr(B)$:
\[
    Pr(B) = {10 \choose 5} \left(\frac{1}{2}\right)^10 \cdot \frac{1}{2} +
    {10 \choose 5} (0.8)^5 (0.2)^5 \cdot \frac{1}{2} =
\]
\[
    = \frac{252}{2} \cdot \left[\frac{1}{1024} + (0.16)^5\right] \cong 0.1363
\]
e infine
\[
    Pr(A|B) = \frac{Pr(B|A) Pr(A)}{Pr(B)} = \frac{{10 \choose 5} {1 \choose 2}
    ^{2 \cdot 5} \cdot \frac{1}{2}}{\frac{1}{2} {10 \choose 5} {1 \choose 2}
    ^{2 \cdot 5} + \frac{1}{2} {10 \choose 5} (0.8)^5 (0.2)^5} =
\]
\[
    = \frac{1}{1 + (2 \cdot 0.8 \cdot 2 \cdot 0.2)^5} \cong 0.903.
\]
Questa probabilit\`a \`e piuttosto alta. Lanciando infatti la moneta
\textit{truccata}, \`e abbastanza impossibile ottenere un serie di risultati
"equiripartiti" $5$-$5$ come quella richiesta.\\
Ripetendo i calcoli per il caso in cui $B$ sia l'evento \{\textit{$5000$ facce
Testa, $5000$ facce Croce}\}, si ottiene la seguente espressione per $Pr(A|B)$:
\[
    Pr(A|B) = \frac{1}{1 + (0.8 \cdot 0.2 \cdot 4)^{5000}} \cong 1
\]
e si raggiunge quasi la \textit{certezza} che la moneta scelta sia
effettivamente quella perfetta. Infatti, la probabilit\`a di ottenere un cos\`i
preciso bilanciamento dei risultati \{\textit{Testa}\} e \{\textit{Croce}\} con
la moneta (pesantemente) truccata  \`e praticamente \textit{nulla}.

%-------------------------------------------------------------------------------
% Section: Segnali aleatori a tempo continuo e a tempo discreto
%-------------------------------------------------------------------------------

\newpage
\section{Segnali aleatori a tempo continuo e a tempo discreto}
Segnali aleatori a tempo continuo e a tempo discreto.
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Chapter 3: Esame Scritto
%-------------------------------------------------------------------------------

\chapter{Esame Scritto}
\epigraph{
	"The reason I talk to myself is because I'm the only one whose answers I
	accept."
}{--- \textup{George Carlin}}

Di seguito una lista di temi di esame presi dalla pagina "Comunicazioni
Numeriche 2017-18" sul sito E-learning del Polo di Ingegneria. Le soluzioni
fornite sono state arricchite di appunti personali e approfondimenti teorici.

%-------------------------------------------------------------------------------
% Section: Compitino 2014}
%-------------------------------------------------------------------------------

\section{Compitino 2014}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Compitino 2015
%-------------------------------------------------------------------------------

\section{Compitino 2015}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Compitino 2016
%-------------------------------------------------------------------------------

\section{Compitino 2016}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Compitino 2017
%-------------------------------------------------------------------------------

\section{Compitino 2017}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Compitino 2018
%-------------------------------------------------------------------------------

\section{Compitino 2018}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Prova di Comunicazioni Numeriche - 09 Gennaio 2018
%-------------------------------------------------------------------------------

\section{Prova di Comunicazioni Numeriche - 09 Gennaio 2018}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Prova di Comunicazioni Numeriche - 29 Gennaio 2018
%-------------------------------------------------------------------------------

\section{Prova di Comunicazioni Numeriche - 29 Gennaio 2018}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Prova di Comunicazioni Numeriche - 20 Febbraio 2018
%-------------------------------------------------------------------------------

\section{Prova di Comunicazioni Numeriche - 20 Febbraio 2018}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Section: Prova di Comunicazioni Numeriche - 09 Aprile 2018
%-------------------------------------------------------------------------------

\section{Prova di Comunicazioni Numeriche - 09 Aprile 2018}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Chapter 4: Esame Orale
%-------------------------------------------------------------------------------

\chapter{Esame Orale}
\epigraph{
	"Read less, study less, but think more."
}{--- \textup{Leo Tolstoy}}

Di seguito una lista di domande, con una possibile risposta, riprese dagli orali
delle ultime sessioni di esame suddivise tra \textbf{segnali deterministici},
\textbf{segnali aleatori} e \textbf{sistemi monodimensionali}.

%-------------------------------------------------------------------------------
% Section: Segnali Deterministici
%-------------------------------------------------------------------------------

\section{Segnali Deterministici}
Segnali il quale valore sia univocamente determinabile non appena siano fissati
i valori delle variabili indipendenti (in particolare, il tempo per segnali
monodimensionali). Questo accade quando il segnale \`e noto attraverso un
grafico, o una registrazione magnetica, o pi\`u semplicemente attraverso una ben
definita espressione matematica, o ancora perch\`e \`e il prodotto di sistemi e
apparati di cui si ha stretto controllo (per esempio, un generatore di forme
d'onda di un laboratorio elettronico).\\
Per modellare e studiare i segnali determinati sono sufficienti i concetti
dell'analisi matematica tradizionale.

%-------------------------------------------------------------------------------
% Subsection: Relazioni tra Potenza Media ed Energia di un Segnale
%-------------------------------------------------------------------------------

\newpage
\subsection{Relazioni tra Potenza Media \textit{$P_x$} ed Energia \textit{$E_x$}
di un Segnale}
Un segnale ad energia finita (matematicamente, a quadrato sommabile) ha potenza
media nulla:
\begin{equation}
	E_x = K < \infty \Longrightarrow P_x = 0.
\end{equation}
Viceversa, un segnale che abbia un valore finito diverso da zero della potenza
media ha necessariamente energia infinita:
\begin{equation}
	P_x = K < \infty, K \neq 0 \Longrightarrow E_x = \infty.
\end{equation}

\begin{lemma}
Un segnale ad energia finita (matematicamente, a quadrato sommabile) ha potenza
	media nulla:
\begin{equation}
	E_x = K < \infty \Longrightarrow P_x = 0.
\end{equation}
\end{lemma}
\begin{proof}
Dato un segnale $x(t)$ tale che $E_x = K < \infty$, ad energia finita, risulta
\begin{equation}
	E_x = \int\displaylimits_{-\infty}^{+\infty} \abs{x(t)}^2 dt = K < 
	\infty,
\end{equation}
ricordando poi che
\begin{equation}
	E_x = \lim_{T \rightarrow \infty} E_{x_T},
\end{equation}
ne segue che
\begin{equation}
	\lim_{T \rightarrow \infty} E_{x_T} = K < \infty.
\end{equation}
Allora
\begin{equation}
	\begin{split}
		P_x = \lim_{T \rightarrow \infty} P_{x_T} =
		\lim_{T \rightarrow \infty} \frac{E_{x_T}}{T} =
		\lim_{T \rightarrow \infty} E_{x_T} \cdot
		\lim_{T \rightarrow \infty} \frac{1}{T} =
		\\
		= K \cdot \lim_{T \rightarrow \infty} \frac{1}{T} = 0.
		\quads{7}
	\end{split}
\end{equation}
che dimostra la tesi iniziale.
\end{proof}

\begin{lemma}
Un segnale che abbia un valore finito diverso da zero della potenza media ha
	necessariamente energia infinita:
\begin{equation}
	P_x = K < \infty, K \neq 0 \Longrightarrow E_x = \infty.
\end{equation}
\end{lemma}
\begin{proof}
Dato un segnale $x(t)$ tale che $P_x = K < \infty$, con $K \neq \infty$, a
	potenza finita non nulla, risulta
\begin{equation}
	P_x \triangleq \lim_{T \rightarrow \infty} P_{x_T} =
	\lim_{T \rightarrow \infty} \frac{E_{x_T}}{T} = 
	\lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \abs{x(t)}^2 dt = K.
\end{equation}
Da cui segue che
\begin{equation}
	E_x = \lim_{T \rightarrow \infty} E_{x_T} = \lim_{T \rightarrow \infty}
	T \cdot \frac{E_{x_T}}{T} = \lim_{T \rightarrow \infty} T \cdot P_{x_T}
	= \lim_{T \rightarrow \infty} T \cdot K = \infty,
\end{equation}
che dimostra la tesi iniziale.
\end{proof}

\noindent\textbf{I due precedenti lemmi sono validi solo ed esclusivamente
nell'ordine in cui sono stati posti.}

%-------------------------------------------------------------------------------
% Subsubsection: Funzione a quadrato sommabile
%-------------------------------------------------------------------------------

\subsubsection{Funzione a quadrato sommabile}
In analisi matematica, una funzione $f(x)$ di una variabile reale a valori reali
e complessi si dice a quadrato sommabile, o anche a quadrato integrabile, in un
determinato intervallo $I = \left[a, b\right]$ se l'integrale del suo modulo in
$I$ \`e finito:
\begin{equation}
	\int\displaylimits_{a}^{b} \abs{f(x)}^2 dx < \infty.
\end{equation}
La nozione si estende a funzioni definite su di uno spazio di misura a valori in
uno spazio vettoriale topologico.\\
L'insieme di tutte le funzioni misurabili su un dato dominio, che in esso sono a
quadrato sommabile, forma uno spazio di Hilbert, il cosiddetto spazio $L^2$.

%-------------------------------------------------------------------------------
% Subsubsection: Applicazioni nella Meccanica Quantistica
%-------------------------------------------------------------------------------

\subsubsection{Applicazioni nella Meccanica Quantistica}
La condizione di quadrato sommabile \`e necessaria particolarmente nella
meccanica quantistica, in quanto costituisce una richiesta basilare per le
funzioni d'onda che descrivono il comportamento delle particelle elementari e,
in particolare, la probabilit\`a di osservare il sistema in un certo stato
quantico. Ad esempio, lo stato di una particella (senza spin) associata a un
campo scalare \`e una funzione d'onda della forma $\psi(x, y, z)$, dove
l'integrale
\begin{equation}
	p(V) = \int\displaylimits_{V} \abs{\psi(x, y, z)}^2 dx dy dz,
\end{equation}
rappresenta la probabilit\`a di trovare la particella $p$ nel volume $V$.\\
Per questo motivo, dato che la probabilit\`a deve essere necessariamente finita
e normalizzabile, si richiede che esista e abbia valore finito un integrale
della forma:
\begin{equation}
	\int\displaylimits_{-\infty}^{+\infty} \ dx
	\int\displaylimits_{-\infty}^{+\infty} \ dy
	\int\displaylimits_{-\infty}^{+\infty} \ dz \abs{\psi(x, y, z)}^2.
\end{equation}

%-------------------------------------------------------------------------------
% Subsection: Relazione Potenza Media, Valore Efficace e Valore Medio
%-------------------------------------------------------------------------------

\newpage
\subsection{Relazione Potenza Media, Valore Efficace e Valore Medio:
$P_x = 0 \Longrightarrow x_eff = 0, x_m = 0$}
\begin{theorem}
Dato un segnale $x(t)$, tale che $P_x = 0$, a potenza nulla, risulta
\begin{equation}
	\begin{cases}
		x_eff = 0\\
		x_m = 0
	\end{cases}.
\end{equation}
Ovvero
\begin{equation}
	P_x = 0 \Longrightarrow
			\begin{cases}
				x_eff = 0\\
				x_m = 0
			\end{cases}.
\end{equation}
\end{theorem}
\begin{proof}
Il primo punto \`e di facile dimostrazione. Infatti
\begin{equation}
	x_eff = \sqrt{P_x},
\end{equation}
quindi
\begin{equation}
	P_x = 0 \Longrightarrow x_eff = 0.
\end{equation}
Resta da dimostrare che
\begin{equation}
	P_x = 0 \Longrightarrow x_m = 0.
\end{equation}
Per fare ci\`o consideriamo il segnale $x(t) = \tilde x(t)  + x_m$ , dove $x_m$
	\`e il valore medio del segnale $x(t)$ stesso. Risulta quindi
	$\tilde x(t) \triangleq x(t) - x_m$, e quindi per definizione
	$\tilde x(t)$ \`e un segnale con valore medio nullo. Infatti
\begin{equation}\label{eq:valor_medio_x_tilde}
\begin{split}
	\tilde x_m = \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} \tilde x(t) \dt = 
	\lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} (x(t) - x_m) \dt =
	\quads{2}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} x(t) \dt - \lim_{T \rightarrow \infty} 
	\frac{1}{T} \int\displaylimits_{-T/2}^{T/2} x_m \dt =
	\quads{3}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} x(t) \dt - x_m \cdot 
	\lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} 1 \dt = x_m - x_m = 0.
\end{split}
\end{equation}
Sviluppiamo ora i calcoli per ottenere un'espressione per la potenza $P_x$ del
	segnale $x(t) = \tilde x(t) + x_m$:
\[
	P_x = \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \abs{x(t)}^2 \dt =
	\footnote{Dato $z \in \mathbb{C}$ allora
	$z \cdot z^* = ce^{j\varphi} \cdot ce^{-j\varphi} =
	c^{2}e^{j(\varphi - \varphi)} = c^2 = \abs{z}^2.$}
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2}
	x(t) \cdot x(t)^* \dt =
	\quads{2}
\]
\begin{equation}
\begin{split}
	= \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} (\tilde x(t) + x_m) \cdot 
	(\tilde x(t)^* + x_m^*) \dt =
	\quads{4}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} \tilde x(t) \tilde x(t)^* + x_m^* 
	\tilde x(t) + x_m \tilde x(t)^* + x_m x_m^* \dt =
	\quads{2}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \abs{\tilde x(t)}^2 + 
	(x_m^* \tilde x(t) + x_m \tilde x(t)^*) + \abs{x_m}^2 \dt =
	\quads{2}
	\\
	= \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \abs{\tilde x(t)}^2 \dt + 
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2} 
	(x_m^* \tilde x(t) + x_m \tilde x(t)^*) \dt
	\quad
	\\
	+ \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \abs{x_m}^2 \dt.
	\quads{10}
\end{split}
\end{equation}
Vediamo di risolvere questi tre limiti di integrali separatamente. Il primo
integrale \`e ovvio
\begin{equation}
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2}
	\abs{\tilde x(t)}^2 \dt = P_{\tilde x}.
\end{equation}
Il terzo integrale \`e di facile risoluzione
\begin{equation}
\begin{split}
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2}
	\abs{x_m}^2 \dt = \abs{x_m}^2 \cdot \lim_{T \rightarrow \infty}
	\frac{1}{T} \int\displaylimits_{-T/2}^{T/2} 1 \dt =
	\\
	= \abs{x_m}^2 \cdot \left[\frac{T}{2} + \frac{T}{2}\right] =
	\abs{x_m}^2.
	\quads{5}
\end{split}
\end{equation}
Per quanto riguarda il secondo invece, possiamo procedere cos\`i
\[
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2}
	(x_m^* \tilde x(t) + x_m \tilde x(t)^*) \dt =
	\footnote{Dato $z = a + ib$ con $z \in \mathbb{C}$, allora 
	$\frac{z + \conjugate{z}}{2} = \frac{a + ib + a - ib}{2} = \frac{2a}{2} 
	= a = \Re(z)$}
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2} 
	(x_m^* \tilde x(t) + (x_m^* \tilde x(t))^*) \dt =
\]
\begin{equation}
\begin{split}
	= \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} 2 \cdot 
	\Re\left(x_m^* \tilde x(t)\right) \dt =
	\\
	= 2 \cdot \Re\left(\lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} x_m^* \tilde x(t) \dt\right) =
\end{split}
\end{equation}
\[
	= 2 \cdot \Re\left(x_m^* \cdot \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \tilde x(t) \dt\right) = 2 \cdot 
	\Re\left(x_m^* \ \tilde x_m\right) =
	\footnote{Nella \eqref{eq:valor_medio_x_tilde} abbiamo calcolato che 
	$\tilde x_m = 0.$} \ 2 \cdot \Re\left(x_m^* \cdot 0\right) = 0.
\]
A questo punto, riportando i risultati ottenuti nell'integrale di partenza:
\begin{equation}
\begin{split}
	= \lim_{T \rightarrow \infty} \frac{1}{T} 
	\int\displaylimits_{-T/2}^{T/2} \abs{\tilde x(t)}^2 \dt + 
	\lim_{T \rightarrow \infty} \frac{1}{T} \int\displaylimits_{-T/2}^{T/2} 
	(x_m^* \tilde x(t) + x_m \tilde x(t)^*) \dt
	\\
	+ \lim_{T \rightarrow \infty} \frac{1}{T}
	\int\displaylimits_{-T/2}^{T/2} \abs{x_m}^2 \dt = P_{\tilde x} + 0 + 
	\abs{x_m}^2 = P_{\tilde x} + \abs{x_m}^2.
\end{split}
\end{equation}
Infine risulta quindi
\begin{equation}
P_x = P_{\tilde x} + \abs{x_m}.
\end{equation}
Da cui, dato che si \`e posto $P_x = 0$ come supposizione iniziale, e dato che
$P_{\tilde x} \geq 0$ e $\abs{x_m}^2 \geq 0$ per definizione, risulta
\begin{equation}
	P_x = 0 \Longrightarrow P_{\tilde x} + \abs{x_m} = 0 \Longrightarrow
		\begin{cases}
			P_{\tilde x} = 0\\
			\abs{x_m} = 0
		\end{cases},
\end{equation}
che dimostra la tesi iniziale:
\begin{equation}
	P_x = 0 \Longrightarrow \begin{cases}
					x_eff = 0\\
					\abs{x_m} = 0
				\end{cases}.
\end{equation}
\end{proof}

\noindent\textbf{Come conseguenza dei due teoremi visti nelle sezioni
\textit{4.1.1} e \textit{4.1.2} possiamo scrivere che}
\begin{equation}
	\begin{cases}
		\boldsymbol{SISTEMI \quad AD}\\
		\boldsymbol{ENERGIA \quad FINITA}\\
		\boldsymbol{(E_x = K < \infty)}
	\end{cases}
	\Longrightarrow
	\begin{cases}
		\boldsymbol{P_x = 0}\\
		\boldsymbol{x_eff = 0}\\
		\boldsymbol{x_m = 0}
	\end{cases}.
\end{equation}

%-------------------------------------------------------------------------------
% Subsubsection: Media quadratica
%-------------------------------------------------------------------------------

\subsubsection{Media quadratica}
La media quadratica di due o pi\`u numeri \`e definita come la radice quadrata
del rapporto tra la somma dei quadrati dei valori numerici ed il numero di
valori, ed \`e un indicatore statistico definito a partire dai quadrati dei
valori considerati.
Formalmente, dai n valori numerici
\begin{equation}
	x_1, x_2, \dots , x_n
\end{equation}
la formula di calcolo della media quadratica \`e data da
\begin{equation}
	Media \ \ quadratica = \sqrt{\frac{x_1^2 + x_2^2 + \dots + x_n^2}{n}} =
	\sqrt{\frac{\sum_{i = 1}^{n} x_i^2}{n}}.
\end{equation}

%-------------------------------------------------------------------------------
% Subsubsection: Valore Efficace
%-------------------------------------------------------------------------------

\subsubsection{Valore Efficace}
Si definisce valore efficace di una funzione continua $x(t)$, la radice della
media quadratica (ovvero la "radice della media dei quadrati" detta anche
\textit{valore quadratico medio}, in inglese \textit{root mean square}, da cui
la sigla RMS), sul periodo della funzione stessa: 
\begin{equation}
	x_rms = \sqrt{\frac{1}{T} \cdot \int\displaylimits_{0}^{T}
	\left[x(t)\right]^2 \dt}.
\end{equation}
Se si applica il procedimento ad un segnale costante si può facilmente
constatare che il suo valore efficace coincide con il valore reale. 

%-------------------------------------------------------------------------------
% Subsection: Biunivocita' della trasformata serie di Fourier
%-------------------------------------------------------------------------------

\newpage
\subsection{Biunivocit\`a della trasformata serie di Fourier}
Ogni segnale $x(t)$ che soddisfi il criterio di Dirichlet pu\`o essere
rappresentato con lo sviluppo in serie di Fourier
\begin{equation}
	x(t) = \sum_{n = -\infty}^{\infty} X_n \ e^{j 2 \pi n f_0 t},
\end{equation}
dove il particolare coefficiente $X_k$ della serie \`e dato da
\begin{equation}
	X_k = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	\ e^{-j 2 \pi k f_0 t} \dt.
\end{equation}
La seconda delle due \`e una equazione di \textit{analisi} che permette di
stabilire qual \`e il contenuto in termini di oscillazioni armoniche del segnale
(in una parola, di \textit{analizzare} il segnale). La prima delle due,
viceversa, \`e una equazione di \textit{sintesi} che, note le ampiezze e fasi
delle varie armoniche (cio\`e noti i coefficienti di Fourier), permette di
ricostruire, cio\`e di \textit{sintetizzare}, il segnale dato a partire dalle
proprie componenti frequenziali (armoniche). Evidentemente, l'equazione di
sintesi prevede l'uso di \textit{infinite} armoniche per ricostruire il segnale.
D'altronde, condizione necessaria alla convergenza della serie \`e che
l'ampiezza $\abs{X_n}$ delle armoniche \textit{tenda a zero} quando
$n \rightarrow \infty$. Questo comporta che le armoniche pi\`u "importanti" ai
fini della sintesi del segnale sono in numero limitato, e che quindi la serie
pu\`o essere sostituita ai fini pratici da una sommatoria di un numero finito di
termini.\\
Le equazioni di analisi e di sintesi permettono di stabilire una corrispondenza
tra il segnale $x(t)$ e la \textit{sequenza} $X_n$ costituita dai coefficienti
della serie (coefficienti di Fourier o di Eulero). Indicheremo tale
corrispondenza con la seguente scrittura:
\begin{equation}
	x(t) \iff X_n
\end{equation}
Questo tipo di notazione suggerisce che la conoscenza dell'andamento del segnale
$x(t)$ in ambito temporale \`e di fatto \textit{equivalente} alla conoscenza
della \textit{successione} dei coefficienti di Fourier $X_n$ in ambito
frequenziale, nel senso che il passaggio da un dominio all'altro \`e immediato
attraverso le relazioni di analisi e sintesi.
\begin{proof}
\begin{equation}
\begin{split}
	X_k = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	\ e^{-j 2 \pi k f_0 t} \dt =
	\quads{2}
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
	\sum_{n = -\infty}^{\infty} X_n \ e^{j 2 \pi n f_0 t} e^{-j 2 \pi k f_0 t}
	\dt =
	\quads{1}
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
	\sum_{n = -\infty}^{\infty} X_n \ e^{j 2 \pi (n - k) f_0 t} \dt =
	\quads{2}
	\\
	= \frac{1}{T_0} \sum_{n = -\infty}^{\infty} X_n
	\int\displaylimits_{-T_0/2}^{T_0/2} e^{j 2 \pi (n - k) f_0 t} \dt =
	\quads{2}
\end{split}
\end{equation}
\[
	= \frac{1}{T_0} \sum_{n = -\infty}^{\infty} X_n \left[
		\int\displaylimits_{-T_0/2}^{T_0/2} \left( 
		\cos[2 \pi (n - k) f_0 t] \dt + j \sin[2 \pi (n - k) f_0 t]
		\right) \dt \right] =
\]
\[
	= \frac{1}{T_0} \sum_{n = -\infty}^{\infty} X_n \left[
		\int\displaylimits_{-T_0/2}^{T_0/2} \cos[2 \pi (n - k) f_0 t] 
		\dt + j \int\displaylimits_{-T_0/2}^{T_0/2} 
		\sin[2 \pi (n - k) f_0 t] \dt\right].
\]
I due integrali sono risolvibili senza effettuare eccessivi calcoli matematici,
infatti
\[
	\cos(2 \pi (n - k) f_0 t) = 
		\begin{cases}
			1 \quad se \quad n = k\\
			1 \quad se \quad n \neq k
		\end{cases}
\]
\[
	\sin(2 \pi (n - k) f_0 t) = 
		\begin{cases}
			0 \quad se \quad n = k\\
			0 \quad se \quad n \neq k
		\end{cases}
\]
da cui segue che
\[
	\int\displaylimits_{-T_0/2}^{T_0/2} \cos(2 \pi (n - k) f_0 t) \dt =
	\footnote{Poich\`e integriamo il $coseno$ lungo intervalli pari al
	periodo.}
		\begin{cases}
			T_0 \quad se \quad n = k\\
			0 \quad \ se \quad n \neq k
		\end{cases}
\]
\[
	\int\displaylimits_{-T_0/2}^{T_0/2} \sin(2 \pi (n - k) f_0 t) \dt =
		\begin{cases}
			0 \quad se \quad n = k\\
			0 \quad se \quad n \neq k
		\end{cases}
\]
Possiamo quindi riprendere
\[
	X_k = \frac{1}{T_0} \sum_{n = -\infty}^{\infty} X_n \left[
		\int\displaylimits_{-T_0/2}^{T_0/2} \cos(2 \pi (n - k) f_0 t)
		\dt + j \int\displaylimits_{-T_0/2}^{T_0/2}
		\sin(2 \pi (n - k) f_0 t) \dt\right] =
\]
\[
	=\footnote{Di tutti i termini della sommatoria
	$\sum_{n = -\infty}^{\infty} X_n$ rimane infatti solo il
	\textit{k}-esimo per cui il primo dei due integrali non \`e nullo.} \ 
	\frac{1}{T_0} \ X_k \ T_0 = X_k
\]
\[
	X_k = X_k,
\]
che dimostra la tesi iniziale.
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Linearita' della trasformata serie di Fourier
%-------------------------------------------------------------------------------

\newpage
\subsection{Linearit\`a della trasformata serie di Fourier}
Consideriamo ora due segnali $x(t)$ e $y(t)$, entrambi periodici dello stesso
periodo $T_0$ e aventi coefficienti di Fourier $X_n$ and $Y_n$. Il segnale
\begin{equation}
	z(t) = a \cdot x(t) + b \cdot y(t)
\end{equation}
data dalla combinazione lineare di $x(t)$ e $y(t)$, \`e periodico di periodo
$T_0$, e ha coefficienti di Fourier $Z_n$ dati da
\begin{equation}
	Z_n = a \cdot X_n + b \cdot Y_n.
\end{equation}
\begin{proof}
Tale propriet\`a di linearit\`a dei coefficienti di Fourier deriva direttamente
dalla medesima propriet\`a dell'integrale; si ha infatti che
\begin{equation}
\begin{split}
	Z_n = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} z(t)
	\ e^{-j 2 \pi n f_0 t} \dt =
	\quads{8}
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \left[a \cdot x(t) +
	b \cdot y(t)\right] e^{-j 2 \pi n f_0 t} \dt =
	\quads{6}
	\\
	= \frac{a}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	\ e^{-j 2 \pi n f_0 t} \dt + \frac{b}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} y(t) \ e^{-j 2 \pi n f_0 t} \dt =
	\quads{3}
	\\
	= a \cdot \left[\frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	\ e^{-j 2 \pi n f_0 t} \dt \right] + b \cdot \left[\frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} y(t) \ e^{-j 2 \pi n f_0 t}
	\dt \right] =
	\\
	= a \cdot X_n + b \cdot Y_n.
	\quads{11}
\end{split}
\end{equation}
Naturalmente, lo sviluppo in serie di $z(t)$ \`e costituito da una somma di
oscillazioni aventi \textit{le stesse frequenze} di quelle che compongono i
segnali $x(t)$ e $y(t)$; pertanto, in generale una combinazione lineare di
segnali aventi medesimo periodo $T_0$ non introduce nuove armoniche.\\
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Esercizio
%-------------------------------------------------------------------------------

\subsubsection{Esercizio}
Consideriamo il segnale
\begin{equation}
	x(t) = A \cos(2 \pi f_0 t + \varphi) + A \sin(2 \pi f_0 t + \varphi),
\end{equation}
ricaviamo i coefficienti di Fourier.
\bigbreak\noindent
Notiamo subito che
\begin{equation}
	x(t) = A x_1(t) + A x_2(t),
\end{equation}
dove
\begin{equation}
	x_1(t) = \cos(2 \pi f_0 t + \varphi),
\end{equation}
\begin{equation}
	x_2(t) = \sin(2 \pi f_0 t + \varphi),
\end{equation}
dalla propriet\`a di linearit\`a dei coefficienti di Fourier deriva che
\begin{equation}
	X_n = A X_{1n} + B X_{2n}.
\end{equation}
Procediamo con i calcoli.
\improvement[inline]{To be continued.}
\noindent
In conclusione possiamo quindi scrivere
\begin{equation}
	X_n =
		\begin{cases}
			A X_{1n} \quad\quad\quad n = \pm 1\\
			B X_{2n} \quad\quad\quad n = \pm 2
		\end{cases}
			=
			\begin{cases}
				\frac{A}{2} e^{j \varphi} \quad\quad\quad n = 1\\
				\frac{A}{2} e^{-j \varphi} \quad\quad n = -1\\
				\frac{B}{2} e^{-j \frac{\pi}{2}} \quad\quad n = 2\\
				\frac{B}{2} e^{j \frac{\pi}{2}} \quad\quad\quad n = -2\\
				0  \quad\quad\quad\quad\quad n \neq \pm 1, \ \pm 2.
			\end{cases}
\end{equation}
\improvement[inline]{To be continued.}

%-------------------------------------------------------------------------------
% Subsection: Simmetria Hermitiana della trasformata serie di Fourier
%-------------------------------------------------------------------------------

\newpage
\subsection{Simmetria Hermitiana della trasformata serie di Fourier}
Dato un segnale reale $x(t)$ ($x(t) \in \mathbb{R} \rightarrow x(t) = x^*(t)$),
i coefficienti $X_n$ dello sviluppo in serie di Fourier in forma complessa di
$x(t)$ godono della propriet\`a di simmetria Hermitiana (o coniugata). Ovvero
\begin{equation}
	x(t) = x^*(t) \iff X_{-n} = X^*_{n}
\end{equation}
\begin{equation}
	X_{-n} = X^*_{n} \iff
		\begin{cases}
			\abs{X_{n}} = \abs{X_{-n}} \quads{2}
                (simmetria \ pari \ per \ il \ modulo)\\
			\angle X_n = - \angle X_{-n} \quad
                (simmetria \ dispari \ per \ la \ fase)
		\end{cases}
\end{equation}
\begin{proof}
\begin{equation}
\begin{split}
	X_{-n} = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	\ e^{-j 2 \pi (-n) f_0 t} \dt =
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	\ e^{j 2 \pi n f_0 t} \dt =
	\quad
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x^*(t)
	\ e^{j 2 \pi n f_0 t} \dt =
	\quad
	\\
	= \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} \left( x(t)
	\ e^{-j 2 \pi n f_0 t} \right)^* \dt =
	\quad
\end{split}
\end{equation}
\[
	=\footnote{Sia $z = re^{j\varphi} \in \mathbb{C}$ un numero complesso
	espresso in forma esponenziale. Allora: $\conjugate{z} = re^{-j\varphi}$
	dove $\conjugate{z}$ denota il complesso coniugato di $z$.}
	\footnote{Siano $z_1 , z_2 \in \mathbb{C}$. Allora: $(z_1 \cdot z_2)^* =
	\left[c_1 e^{j \varphi_1} \cdot c_2 e^{j \varphi_2}\right]^* =
	\left[c_1 c_2 \cdot e^{j (\varphi_1 + \varphi_2)}\right]^* = c_1 c_2
	\cdot e^{-j (\varphi_1 + \varphi_2)} = c_1 e^{-j \varphi_1} \cdot c_2
	e^{-j \varphi_2} = z_1^* \cdot z_2^*.$} \left[\frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \ e^{-j 2 \pi n f_0 t}
	\dt\right]^* = X^*_{n}.
\]
\end{proof}

\newpage

\begin{figure}
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/spettro_di_ampiezza_e_di_fase.jpg}
	\caption{Spettro di ampiezza e di fase.}
\end{figure}

%-------------------------------------------------------------------------------
% Subsubsection: Simmetria pari per il modulo
%-------------------------------------------------------------------------------

\subsubsection{Simmetria pari per il modulo}
\begin{equation}
	\abs{X_n} = \abs{X_{-n}}
\end{equation}
Sia $f(x)$ una funzione a valori reali di variabile reale e sia $D \subset
\mathbb{R}$ il suo dominio. Allora $f$ \`e pari se per ogni $x \in D$ vale
l'equazione:
\[
	f(x) = f(-x).
\]
Geometricamente, il grafico di una funzione pari \`e simmetrico rispetto
all'asse $y$.\\
\\
Il nome \textbf{pari} deriva dal fatto che le serie di Taylor di una funzione
pari centrata nell'origine contengono solo potenze pari.\\
\\
Esempi di funzioni pari sono $x^2$, $x^4$, $cos(x)$, $cosh(x)$.\\
\\
Esempio pratico:
\[
	y = x^2 - 1 \Longrightarrow f(-x) = (-x)^2 - 1 = x^2 - 1 = f(x).
\]

%-------------------------------------------------------------------------------
% Subsubsection: Simmetria dispari per la fase
%-------------------------------------------------------------------------------

\subsubsection{Simmetria dispari per la fase}
\begin{equation}
	\angle X_n = - \angle X_{-n}
\end{equation}
Sia $f(x)$ una funzione a valori reali di variabile reale e sia $D \subset
\mathbb{R}$ il suo dominio. Allora $f$ \`e dispari se per ogni $x \in D$
sussiste l'equazione:
\[
	f(-x) = - f(x),
\]
vale a dire
\[
	f(x) = - f(-x).
\]
Geometricamente, il grafico di una funzione dispari \`e simmetrico rispetto
all'origine degli assi.\\
\\
Il nome \textbf{dispari} deriva dal fatto che le serie di Taylor di una funzione
dispari centrate nell'origine contengono solo potenze dispari.\\
\\
Esempi di funzioni dispari sono $x$, $x^3•$, $sin(x)$, $sinh(x)$.\\
\\
Esempio pratico:
\[
	y = x^3 - x \Longrightarrow f(-x) = (-x)^3 - (-x) = -x^3 + x =
	-(x^3 - x) = -f(x).
\]

%-------------------------------------------------------------------------------
% Subsection: Trasformata di un segnale (periodico) reale e pari
%-------------------------------------------------------------------------------

\newpage
\subsection{Trasformata di un segnale (periodico) reale e pari}
Si consideri un segnale periodico
\[
	x(t) = x(t + T_0) \quad \forall t,
\]
reale
\[
	x(t) = x^*(t),
\]
e pari
\[
	x(t) = x(-t).
\]
In tal caso, il generico coefficiente di Fourier di $x(t)$ \`e una funzione pari
di $n$:
\[
	X_n = X_{-n}.
\]
\begin{proof}
\[
	X_{-n} = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	\ e^{-j 2 \pi (-n) f_0 t} \dt = \frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t) \ e^{j 2 \pi n f_0 t} \dt =
\]
\[
	\footnote{Poniamo ora $t = -t'$.}= \frac{1}{T_0}
	\int\displaylimits_{T_0/2}^{-T_0/2} x(-t')
	\ e^{-j 2 \pi n f_0 t'} -\dt' = \frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(-t') \ e^{-j 2 \pi n f_0 t'}
	\dt' = 
\]
\[
	\footnote{Dato che $x(t) = x(-t)$.}= \frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t')
	\ e^{-j 2 \pi n f_0 t'} \dt' = X_n.
\]
\end{proof}
\noindent
Che dimostra la nostra tesi.
\bigbreak
Ora, dato che in generale sappiamo che $X_{-n} = X_n^*$, ne segue che $X_n$ \`e
sia pari che reale ed \`e quindi possibile riscrivere la Serie di Fourier per
$x(t)$ in forma semplificata. Infatti:
\[
	x(t) = \sum_{n = -\infty}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} = X_0 +
	\sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} + \sum_{n = -\infty}^{-1}
	X_n \ e^{j 2 \pi n f_0 t} =
\]
\[
	= X_0 + \sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} +
	\sum_{n = 1}^{+\infty} X_{-n} \ e^{j 2 \pi (-n) f_0 t} =
\]
\[
	= X_0 + \sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} +
	\sum_{n = 1}^{+\infty} X_{-n} \ e^{-j 2 \pi n f_0 t} =
\]
\[
	\footnote{$X_n = X_{-n}$. dato che $x(t)$ \`e un segnale pari.}= X_0 +
	\sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} + \sum_{n = 1}^{+\infty}
	X_n \ e^{-j 2 \pi n f_0 t} =
\]
\[
	= X_0 + \sum_{n = 1}^{+\infty} X_n \left(e^{j 2 \pi n f_0 t} +
	\ e^{-j 2 \pi n f_0 t}\right) =
\]
\[
	= X_0 + 2 \cdot \sum_{n = 1}^{+\infty} X_n \left(
	\frac{e^{j 2 \pi n f_0 t} + e^{-j 2 \pi n f_0 t}}{2} \right) =
\]
\[
	= X_0 + 2 \cdot \sum_{n = 1}^{+\infty} X_n \cos(2 \pi n f_0 t).
\]
Abbiamo quindi semplificato l'espressione in serie di Fourier eliminando i
termini sinusoidale per i segnali periodici pari.\\ Questo risultato \`e
logicamente giustificabile se si riflette sul fatto che il coseno \`e una
funzione pari mentre il seno \`e una funzione dispari e quindi poco si presta a
rappresentare l'andamento periodico di un segnale pari.

%-------------------------------------------------------------------------------
% Subsection: Trasformata di un segnale (periodico) reale e dispari
%-------------------------------------------------------------------------------

\newpage
\subsection{Trasformata di un segnale (periodico) reale e dispari}
Si consideri un segnale periodico
\[
	x(t) = x(t + T_0) \quad \forall t,
\]
reale
\[
	x(t) = x^*(t),
\]
e dispari
\[
	x(t) = -x(-t).
\]
In tal caso, il generico coefficiente di Fourier di $x(t)$ \`e una funzione
dispari di $n$:
\[
	X_n = -X_{-n}.
\]
\begin{proof}
\[
	-X_{-n} = -\frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
    \ e^{-j 2 \pi (-n) f_0 t} \dt = \frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} -x(t) \ e^{j 2 \pi n f_0 t} \dt =
\]
\[
	\footnote{Poniamo ora $t = -t'$.}= \frac{1}{T_0}
	\int\displaylimits_{T_0/2}^{-T_0/2} -x(-t') \ e^{-j 2 \pi n f_0 t'}
	-\dt' = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} -x(-t')
	\ e^{-j 2 \pi n f_0 t'} \dt' = 
\]
\[
	\footnote{Dato che $x(t) = -x(-t)$.}= \frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{T_0/2} x(t') \ e^{-j 2 \pi n f_0 t'}
	\dt' = X_n.
\]
\end{proof}
\noindent
Che dimostra la nostra tesi.
\bigbreak
Ora, dato che in generale sappiamo che $X_{-n} = X_n^*$, ne segue che $X_n$ \`e
sia dispari che reale ed \`e quindi possibile riscrivere la Serie di Fourier per
$x(t)$ in forma semplificata. Infatti:
\[
	x(t) = \sum_{n = -\infty}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} = X_0 +
	\sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} + \sum_{n = -\infty}^{-1}
	X_n \ e^{j 2 \pi n f_0 t} =
\]
\[
	= X_0 + \sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} +
	\sum_{n = 1}^{+\infty} X_{-n} \ e^{j 2 \pi (-n) f_0 t} =
\]
\[
	= X_0 + \sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} +
	\sum_{n = 1}^{+\infty} X_{-n} \ e^{-j 2 \pi n f_0 t} =
\]
\[
	\footnote{$X_n = -X_{-n}$. dato che $x(t)$ \`e un segnale dispari.}= X_0
	+ \sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} +
	\sum_{n = 1}^{+\infty} (-X_n) \ e^{-j 2 \pi n f_0 t} =
\]
\[
	= X_0 + \sum_{n = 1}^{+\infty} X_n \ e^{j 2 \pi n f_0 t} -
	\sum_{n = 1}^{+\infty} X_n \ e^{-j 2 \pi n f_0 t} =
\]
\[
	= X_0 + \sum_{n = 1}^{+\infty} X_n \left(e^{j 2 \pi n f_0 t} -
	e^{-j 2 \pi n f_0 t}\right) =
\]
\[
	= X_0 + 2 j \cdot \sum_{n = 1}^{+\infty} X_n \left(
	\frac{e^{j 2 \pi n f_0 t} - e^{-j 2 \pi n f_0 t}}{2 j} \right) =
\]
\[
	= X_0 + 2 j \cdot \sum_{n = 1}^{+\infty} X_n \sin(2 \pi n f_0 t).
\]
Abbiamo quindi semplificato l'espressione in serie di Fourier eliminando i
termini cosinusoidale per i segnali periodici dispari.\\ Questo risultato \`e
logicamente giustificabile se si riflette sul fatto che il seno \`e una funzione
dispari mentre il coseno \`e una funzione pari e quindi poco si presta a
rappresentare l'andamento periodico di un segnale dispari.

%-------------------------------------------------------------------------------
% Subsection: Trasformata di un segnale (periodico) reale alternativo
%-------------------------------------------------------------------------------

\newpage
\subsection{Trasformata di un segnale (periodico) reale alternativo}
Si consideri un segnale periodico
\[
	x(t) = x(t + T_0) \quad \forall t,
\]
reale
\[
	x(t) = x^*(t),
\]
e alternativo
\[
	x(t) = -x\left(t + \frac{T_0}{2}\right).
	\footnote{L'andamento del segnale in un qualunque semiperiodo
	$t_0 \leq t < t_0 + T_0/2$ \`e identico all'andamento nel semiperiodo
	precedente $t_0 - T_0/2 \leq t < t$, cambiato di segno.}
\]
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/segnale_alternativo.jpg}
	\caption{Rappresentazione grafica di un segnale alternativo.}
\end{figure}
In tal caso, il generico coefficiente $X_n$ di Fourier di $x(t)$ \`e
\textit{nullo} per tutti i valori pari dell'indice $n$:
\begin{equation}
	X_n =
	\begin{cases}
		\frac{2}{T_0} \int\displaylimits_{0}^{T_0/2} x(t)
		e^{-j 2 \pi n f_0 t} \dt \quad\quad\quad\quad n \ dispari\\
		0 \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
		\quad \ \ \ n \ pari
	\end{cases}.
\end{equation}
\begin{proof}
\[
	X_{n} = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t)
	e^{-j 2 \pi n f_0 t} \dt =
\]
\[
	\footnote{Avendo suddiviso l'intervallo di integrazione
	$[-T_0/2, T_0/2]$ in due smeiperiodi.}= \frac{1}{T_0}
	\int\displaylimits_{-T_0/2}^{0} x(t) e^{-j 2 \pi n f_0 t} \dt +
	\frac{1}{T_0} \int\displaylimits_{0}^{T_0/2} x(t) e^{-j 2 \pi n f_0 t}
	\dt =
\]
Il primo integrale pu\`o essere riscritto come
\[
	\footnote{Poniamo $t^{'} = t + \frac{T_0}{2}$ nel primo integrale.
	Quindi $t = t^{'} - \frac{T_0}{2}$, e se $t$ va da $-\frac{T_0}{2}$ a
	$0$, allora $t^{'} = t + \frac{T_0}{2}$ va da $0$ a $\frac{T_0}{2}$.}=
	\frac{1}{T_0} \int\displaylimits_{0}^{T_0/2} x\left(t^{'} -
	\frac{T_0}{2}\right) e^{-j 2 \pi n f_0 \left(t^{'} -
	\frac{T_0}{2}\right)} \dt^{'} = \frac{1}{T_0}
	\int\displaylimits_{0}^{T_0/2} -x(t^{'}) e^{-j 2 \pi n f_0 \left(t^{'} -
	\frac{T_0}{2}\right)} \dt^{'} =
\]
\[
	= \frac{1}{T_0} \int\displaylimits_{0}^{T_0/2} -x(t^{'})
	e^{-j 2 \pi n f_0 t^{'}} e^{j 2 \pi n f_0 \frac{T_0}{2}} \dt^{'} =
	\frac{1}{T_0} \int\displaylimits_{0}^{T_0/2} -x(t^{'})
	e^{-j 2 \pi n f_0 t^{'}} e^{j \pi n} \dt^{'} =
\]
\[
	\footnote{$e^{j \pi n} = \cos(\pi n) - j \sin(\pi n) = \pm 1$.}=
	\frac{1}{T_0} \int\displaylimits_{0}^{T_0/2} -x(t^{'})
	e^{-j 2 \pi n f_0 t^{'}} (-1)^{n} \dt^{'} = (-1)^{n} \cdot
	\frac{1}{T_0} \int\displaylimits_{0}^{T_0/2} -x(t^{'})
	e^{-j 2 \pi n f_0 t^{'}} \dt^{'} =
\]
\[
	= -\frac{(-1)^{n}}{T_0} \int\displaylimits_{0}^{T_0/2} x(t)
	e^{-j 2 \pi n f_0 t} \dt.
\]
Ora, sostituendo questo risultato nell'integrale iniziale
\[
	= -\frac{(-1)^{n}}{T_0} \int\displaylimits_{0}^{T_0/2} x(t)
	e^{-j 2 \pi n f_0 t} \dt + \frac{1}{T_0} \int\displaylimits_{0}^{T_0/2}
	x(t^{'}) e^{-j 2 \pi n f_0 t^{'}} \dt^{'} =
\]
\[
	= \left[1 - (-1)^{n}\right] \cdot \frac{1}{T_0}
	\int\displaylimits_{0}^{T_0/2} x(t^{'}) e^{-j 2 \pi n f_0 t^{'}}
	\dt^{'}.
\]
Ne segue quindi che
\[
	X_n = \frac{2}{T_0} \int\displaylimits_{0}^{T_0/2} x(t)
	e^{-j 2 \pi n f_0 t} \dt \quad\quad\quad n \ dispari
\]
\[
	X_n = 0 \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
	\quad n \ pari.
\]
\end{proof}
\noindent
Che dimostra la nostra tesi.

%-------------------------------------------------------------------------------
% Subsection: Simmetrie della Trasformata Continua di Fourier (TCF)
%-------------------------------------------------------------------------------

\newpage
\subsection{Simmetrie della Trasformata Continua di Fourier ($TCF$)}
La funzione complessa $X(f)$ pu\`o essere rappresentata in forma polare o in
forma rettangolare:
\begin{equation}
	X(f) = R(f) + j I(f)
\end{equation}
dove $R(f)$ e $I(f)$ ne rappresentano rispettivamente la parte reale e la parte
immaginaria.\\
Vogliamo ora stabilire in che modo le propriet\`a della funzione $x(t)$ si
riflettano sulla sua trasformata.

%-------------------------------------------------------------------------------
% Subsubsection: Simmetrie della TCF: Simmetria Hermitiana
%-------------------------------------------------------------------------------

\subsubsection{Simmetrie della $TCF$: Simmetria Hermitiana}
Supponiamo che $x(t)$ sia una funzione \textit{reale}:
\[
	x(t) = x^*(t)
\]
in tal caso $X(f)$ \`e \textit{Hermitiana}
\[
	X(-f) = X^*(f).
\]
In tal caso le funzione $R(f)$ e $I(f)$ si ricavano immediatamente dalle
seguenti relazioni
\[
	R(f) = \int\displaylimits_{-\infty}^{+\infty} x(t) \cos(2 \pi f t) \dt,
\]
\[
	I(f) = -\int\displaylimits_{-\infty}^{+\infty} x(t) \sin(2 \pi f t) \dt.
\]
Da queste espressioni si vede chiaramente che
\[
	R(f) = R(-f),
\]
\[
	I(f) = -I(f),
\]
ovvero la parte reale della trasformata di un segnale reale \`e una funzione
\textit{pari} della frequenza, mentre la parte immaginaria ne \`e una funzione
\textit{dispari}.\\
La medesima propriet\`a si riflette ovviamente anche nelle funzioni $A(f)$ e
$\vartheta(f)$:
\[
	A(f) = A(-f),
\]
\[
	\vartheta(f) = - \vartheta(-f),
\]
per cui lo spettro di ampiezza di un segnale reale \`e una funzione pari, mentre
il suo spettro di fase \`e dispari.
\begin{proof}
\[
	X(-f) = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	e^{-j 2 \pi (-f) t} \dt = \int\displaylimits_{-\infty}^{+\infty} x(t)
	\cdot e^{j 2 \pi f t} \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} x^*(t) \cdot e^{j 2 \pi f t}
	\dt = \int\displaylimits_{-\infty}^{+\infty} \left[x(t) \cdot
	e^{-j 2 \pi f t}\right]^* \dt =
\]
\[
	= \left[\int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	e^{-j 2 \pi f t} \dt\right]^* = X^*(f).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Simmetrie della TCF: Segnali reali e pari
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Simmetrie della $TCF$: Segnali reali e pari}
Supponiamo adesso che $x(t)$ sia un segnale \textit{reale e pari}
\[
	x(t) = x^*(t) \quad\quad , \quad\quad x(t) = x(-t).
\]
Le relazioni che esprimono la parte reale e quella immaginaria del suo
coefficiente di Fourier si semplificano rispettivamente in
\[
	R(f) = 2 \int\displaylimits_{0}^{\infty} x(t) \cos(2 \pi f t) \dt,
\]
\[
	I(f) = 0.
\]
Ovvero, la trasformata di un segnale reale e pari \`e una funzione
\textit{reale e pari} della frequenza
\[
	X(f) = X^*(f) \quad\quad , \quad\quad X(f) = X(-f).
\]
\begin{proof}
$X(f) = X^*(f)$
\[
	X(f) = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	e^{-j 2 \pi f t} \dt =\footnote{Poniamo $t = -t^{'}$, $dt = dt^{'}$, se
	$t$ va da $-\infty$ a $+\infty$, allora $t^{'}$ va da $+\infty$ a
	$-\infty$.} \int\displaylimits_{+\infty}^{-\infty} x(-t^{'}) \cdot
	e^{-j 2 \pi f (-t^{'})} \ -dt^{'} =
\]
\[
	= - \int\displaylimits_{+\infty}^{-\infty} x(-t^{'}) \cdot
	e^{j 2 \pi f t^{'}} \ dt^{'} = \int\displaylimits_{-\infty}^{+\infty}
	x(-t^{'}) \cdot e^{j 2 \pi f t^{'}} \ dt^{'} =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} x(t^{'}) \cdot
	e^{j 2 \pi f t^{'}} \ dt^{'} = \int\displaylimits_{-\infty}^{+\infty}
	x^*(t^{'}) \cdot e^{j 2 \pi f t^{'}} \ dt^{'} =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} \left[x(t^{'}) \cdot
	e^{-j 2 \pi f t^{'}}\right]^* \ dt^{'} = \left[
		\int\displaylimits_{-\infty}^{+\infty} x(t^{'}) \cdot
		e^{-j 2 \pi f t^{'}} \ dt^{'}\right]^* = X^*(f).
\]
\end{proof}
\begin{proof}
$X(f) = X(-f)$
\[
	X(f) = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot 
	e^{-j 2 \pi f t} \dt = \int\displaylimits_{+\infty}^{-\infty} x(-t^{'}) 
	\cdot e^{-j 2 \pi f (-t^{'})} \ -dt^{'} =
\]
\[
	= - \int\displaylimits_{+\infty}^{-\infty} x(-t^{'}) \cdot 
	e^{-j 2 \pi (-f) t^{'}} \ dt^{'} = 
	\int\displaylimits_{-\infty}^{+\infty} x(t^{'}) \cdot 
	e^{-j 2 \pi (-f) t^{'}} \ dt^{'} = X(-f).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Simmetrie della TCF: Segnali reali e dispari
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Simmetrie della $TCF$: Segnali reali e dispari}
Supponiamo adesso che $x(t)$ sia un segnale \textit{reale e dispari}
\[
	x(t) = x^*(t) \quad\quad , \quad\quad x(t) = -x(-t).
\]
Le relazioni che esprimono al parte reale e quella immaginaria del suo
coefficiente di Fourier si semplificano rispettivamente in
\[
	R(f) = 0,
\]
\[
	I(f) = -2 \int\displaylimits_{0}^{\infty} x(t) sin(2 \pi f t) \dt.
\]
Ovvero, la trasformata di un segnale reale e dispari \`e una funzione
\textit{immaginaria pura e dispari} della frequenza
\[
	X(f) = -X(-f) \quad\quad , \quad\quad X(f) \ immaginario \ puro.
\]
\begin{proof}
$X(f) = -X(-f)$
\[
	-X(-f) = -\int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	e^{-j 2 \pi (-f) t} \dt = \int\displaylimits_{-\infty}^{+\infty} -x(t)
	\cdot e^{j 2 \pi f t} \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} -x(-t^{'}) \cdot
	e^{j 2 \pi f (-t^{'})} \dt = \int\displaylimits_{-\infty}^{+\infty}
	x(t^{'}) \cdot e^{-j 2 \pi f t^{'}} \dt = X(f).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Teoremi sulla trasformata continua di Fourier
%-------------------------------------------------------------------------------

\newpage
\subsection{Teoremi sulla trasformata continua di Fourier}
Dalla definizione di trasformata, seguono facilmente alcune ulteriori
propriet\`a, che chiameremo \textit{teoremi}, estremamente utili nel calcolo
delle trasformate dei segnali e comunque nell'uso dell'analisi di Fourier di
carattere applicativo.

%-------------------------------------------------------------------------------
% Subsubsection: Linearita' della TCF
%-------------------------------------------------------------------------------

\subsubsection{Linearit\`a della $TCF$}
Pu\`o essere conveniente in molti casi esprimere un segnale $x(t)$ come
combinazione lineare di due segnali $x_1(t)$ e $x_2(t)$:
\[
	x(t) = a \cdot x_1(t) + b \cdot x_2(t),
\]
con $a$ e $b$ costanti. Indicando come di consueto con $X_1(f)$ e $X_2(f)$ le
trasformate rispettivamente dei segnali $x_1(t)$ e $x_2(t)$, la trasformata
$X(f)$ di $x(t)$ \`e allora
\[
	X(f) = a \cdot X_1(f) + b \cdot X_2(f).
\]
\begin{proof}
Infatti, applicando semplicemente la definizione trasformata, la tesi segue
sfruttando la propriet\`a di linearit\`a dell'integrale stesso
\[
	X(f) = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	e^{-j 2 \pi f t} \dt = \int\displaylimits_{-\infty}^{+\infty} [a 
	\cdot x_1(t) + b \cdot x_2(t)] \cdot e^{-j 2 \pi f t} \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} a \cdot x_1(t) \cdot 
	e^{-j 2 \pi f t} \dt + \int\displaylimits_{-\infty}^{+\infty} b 
	\cdot x_2(t) \cdot e^{-j 2 \pi f t} \dt =
\]
\[
	= a \cdot \int\displaylimits_{-\infty}^{+\infty} x_1(t) \cdot 
	e^{-j 2 \pi f t} \dt + b \cdot \int\displaylimits_{-\infty}^{+\infty} 
	x_2(t) \cdot e^{-j 2 \pi f t} \dt =
\]
\[
	= a \cdot X_1(f) + b \cdot X_2(f).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Dualita' della TCF
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Dualit\`a della $TCF$}
La similitudine tra le relazioni di trasformata e antitrasformata intese come
"operatori" sulle funzioni rispettivamente $x(t)$ e $X(f)$ permette di risolvere
una questione: se $X(f)$ indica la trasformata del segnale $x(t)$, qual \`e la
trasformata del segnale \textit{temporale} $X(t)$, avente cio\`e lo stesso
andamento temporale originariamente posseduto nell'ambito frequenziale dalla
trasformata di $x(t)$? La risposta \`e la seguente: se
\[
	x(t) \iff X(f)
\]
allora
\[
	X(t) \iff x(-f).
\]
\begin{proof}
Infatti, sappiamo che il segnale $x(t)$ \`e legato alla sua trasformata dalla
relazione
\[
	x(t) = \int\displaylimits_{-\infty}^{+\infty} X(f) \cdot
    e^{j 2 \pi f t} \ df.
\]
Scambiando formalmente le variabili $t$ ed $f$ nella precedente relazione, si
ricava
\[
	x(f) = \int\displaylimits_{-\infty}^{+\infty} X(t) \cdot
    e^{j 2 \pi t f} \ dt.
\]
Se poi in questa relazione si effettua un cambiamento di variabile sostituendo
alla variabile $f$ la variabile $-f$, si ottiene
\[
	x(-f) = \int\displaylimits_{-\infty}^{+\infty} X(t) \cdot
    e^{-j 2 \pi f t} \dt
\]
che dimostra la tesi iniziale.
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Teorema del ritardo
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Teorema del ritardo}
Come viene modificata la trasformata di un segnale se questo viene traslato
sull'asse dei tempi (cio\`e, anticipato o ritardato)? Sia dunque $X(f)$ la
trasformata del segnale $x(t)$; allora la trasformata del segnale traslato a
destra della quantit\`a $t_0$
\[
	y(t) = x(t - t_0)
\]
\`e
\[
	Y(f) = X(f) e^{-j 2 \pi f t_0}.
\]
Questa operazione corrisponde evidentemente a un ritardo se $t_0 > 0$ e ad un
anticipo se $t_0 < 0$.
\begin{proof}
Applicando la definizione di trasformata si ha
\[
	Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) e^{-j 2 \pi f t} \dt
	= \int\displaylimits_{-\infty}^{+\infty} x(t - t_0) e^{-j 2 \pi f t} \dt
	=
\]
\[
	\footnote{Poniamo $t' = t - t_0$.}=
	\int\displaylimits_{-\infty}^{+\infty} x(t') e^{-j 2 \pi f (t' + t_0)}
	\dt ' =	\int\displaylimits_{-\infty}^{+\infty} x(t') e^{-j 2 \pi f t'}
	e^{-j 2 \pi f t_0} \dt ' =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} x(t') e^{-j 2 \pi f t'} \dt '
	\cdot e^{-j 2 \pi f t_0}
	= X(f) e^{-j 2 \pi f t_0}.
\]
\end{proof}
Questa propriet\`a mostra che un ritardo temporale modifica lo spettro di fase
della trasformata del segnale ma \textit{non cambia il suo spettro di ampiezza}.
Infatti, il teorema del ritardo si traduce nelle relazioni
\[
	\abs{Y(f)} = \abs{X(f)},
\]
\[
	\angle Y(f) = \angle X(f) - 2 \pi f t_0.
\]

%-------------------------------------------------------------------------------
% Subsubsection: Teorema del cambiamento di scala
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Teorema del cambiamento di scala}
Si consideri la situazione generale in cui due segnali siano legati dalla
relazione
\[
	y(t) = x(\alpha t).
\]
Cio\`e si effettua un cambiamento della scala temporale. Moltiplicando la
variabile indipendente $t$ del segnale $x(t)$ per coefficiente $\alpha$ si
producono i seguenti effetti:
\begin{itemize}
	\item $\abs{\alpha} > 1$ $\rightarrow$ \textit{compressione} della scala
		dei tempi
	\item $\abs{\alpha} < 1$ $\rightarrow$ \textit{dilatazione} della scala
		dei tempi
	\item $\alpha < 0$ \ \ $\rightarrow$ \textit{inversione} della scala dei
		tempi
\end{itemize}
In altri termini, se $\abs{\alpha} < 1$ l'evoluzione del segnale viene
"rallentata", viceversa se $\abs{\alpha} > 1$ il segnale viene "accelerato".
Operazoini di questo tipo vengono effettuate correntemente nell'elaborazione dei
segnali registrando il segnale ad una certa velocit\`a e riproducendolo a
velocit\`a diversa.
\bigbreak\noindent
Allora
\[
	Y(f) = \frac{1}{\abs{\alpha}} \cdot X\left(\frac{f}{\alpha}\right)
\]
\begin{proof}
$\alpha > 0$
\[
	Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) e^{-j 2 \pi f t} \dt
	= \int\displaylimits_{-\infty}^{+\infty} x(\alpha t) e^{-j 2 \pi f t}
	\dt =
\]
\[
	\footnote{Poniamo $t' = \alpha t$, da cui $dt' = d(\alpha t)$ e $dt' =
	\alpha dt \rightarrow dt = dt'/\alpha$.}= \int\displaylimits_{-\infty}
	^{+\infty} x(t') e^{-j 2 \pi f \frac{t'}{\alpha}} \frac{\dt'}{\alpha} =
	\frac{1}{\alpha} \int\displaylimits_{-\infty}^{+\infty} x(t')
	e^{-j 2 \pi \frac{f}{\alpha}t'} \dt' =
\]
\[
	= \frac{1}{\alpha} X\left(\frac{f}{\alpha}\right) \quad , \quad
	\alpha > 0.
\]
\end{proof}
\begin{proof}
$\alpha < 0$
\[
	Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) e^{-j 2 \pi f t} \dt
	= \int\displaylimits_{-\infty}^{+\infty} x(\alpha t) e^{-j 2 \pi f t}
	\dt =
\]
\[
	\footnote{Poniamo $t' = \alpha t$, da cui $t = \frac{t'}{\alpha}$,
	$dt' = d(\alpha t) \rightarrow dt' = \alpha \cdot dt'$,
	$dt = \frac{dt'}{\alpha}$.}=
	\int\displaylimits_{-\infty}^{+\infty} x(t')
	e^{-j 2 \pi f \frac{t'}{\alpha}} -\frac{\dt}{\alpha} = -\frac{1}{\alpha}
	\int\displaylimits_{-\infty}^{+\infty} x(t')
	e^{-j 2 \pi \frac{f}{\alpha} t'} \dt' =
\]
\[
	= -\frac{1}{\alpha} X\left(\frac{f}{\alpha}\right) \quad , \quad
	\alpha < 0.
\]
\end{proof}
I risultati ottenuti per $\alpha > 0$ e $\alpha < 0$ possono allora essere
riassunti con
\[
	x(\alpha t) \iff \frac{1}{\abs{\alpha}} X\left(\frac{f}{\alpha}\right).
\]
Si nota quindi che una dilatazione dell'asse dei tempi comporta una compressione
dell'asse delle frequenze, e viceversa. Se infatti il segnale viene
"rallentato", vengono a predominare le componenti frequenziali a bassa
frequenza, che sono responsabili per cos\`i dire dell'evoluzione del segnale; lo
spettro allora di "addensa" nell'intorno della frequenza nulla.

%-------------------------------------------------------------------------------
% Subsection: Teorema della Modulazione
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema della Modulazione}
In telecomunicazioni ed elettronica con il termine modulazione si indica
l'insieme delle tecniche di trasmissione finalizzate ad imprimere un segnale
elettrico o elettromagnetico, detto modulante, generalmente contenente
informazione cio\`e variabile in maniera aleatoria nel tempo, su di un altro
segnale elettrico o elettromagnetico, detto portante, sviluppato ad alta
frequenza (frequenza portante $>>$ frequenza modulante). Il risultato della
modulazione \`e la conversione del segnale modulante dalla banda base alla
cosiddetta banda traslata (segnale modulato), secondo il teorema della
modulazione.
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/modulato_modulante_oscillazione.png}
	\caption{Segnale modulato.}
\end{figure}
L'operazione inversa di ripristino del segnale informativo originario in banda
base \`e detto demodulazione. Il dispositivo in trasmissione che attua
l'operazione di modulazione sul segnale informativo \`e detto modulatore, mentre
il dispositivo in ricezione che attua l'operazione di demodulazione \`e detto
demodulatore, compresi rispettivamente nel trasmettitore e nel ricevitore. In
un sistema di ricetrasmissione tali sistemi vengono riuniti entrambi sotto la
dizione \textit{Modem} (dalla composizione di Modulazione e Demodulazione).\\
I segnali modulati possono rappresentare le informazioni pi\`u diverse: audio,
video, dati, ecc... L'onda portante \`e un'onda elettromagnetica o un segnale
elettrico a frequenza ben determinata (molto maggiore della frequenza del
segnale modulante), che pu\`o essere trasmessa in aria o nel vuoto (ad esempio
nelle radiocomunicazioni), o tramite altro mezzo fisico (ad esempio un cavo).
In caso di comunicazioni in fribra ottica la portante \`e la radiazione laser la
cui frequenza \`e tipicamente espressa come lunghezza d'onda.
\bigbreak
In generale, il motivo per cui si utilizzano le tecniche di modulazione risiede
nel fatto che i segnali rappresentanti le informazioni da trasmettere sono in
prevalenza di natura passa-basso (il loro contenuto spettrale \`e concentrato
per lo pi\`u a basse frequenze), mentre i canali trasmissivi che pi\`u
comunamente si utilizzano, per poter trasmettere segnali modulati
contemporaneamente, (come canali hertziani e fibre ottiche) sono tipicamente di
natura passa-banda cio\`e trasmettono in una banda a frequenza diversa da quella
del segnale informativo originario. In sostanza occorre quindi convertire in
frequenza, mediante tale tecnica, lo spettro del segnale rappresentante
l'informazione.
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/modulatore.png}
	\caption{Modulatore.}
\end{figure}
Esistono diversi tipi di modulazione analogica, utilizzate nelle rispettive
trasmissioni analogiche:
\begin{itemize}
	\item AM - (Amplitude Modulation) modulazione di ampiezza;
	\item FM - (Frequency Modulation) modulazione di frequenza;
	\item PM - (Phase Modulation) modulazione di fase.
\end{itemize}
In sostanza, l'informazione da trasmettere pu\`o essere codificata all'interno
di variazioni di ampiezza, frequenza e fase, ed in ricezione dovr\`a essere
recuperata, ovvero demodulata dal segnale portante ricevuto.
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=1.0\textwidth]{images/segnale_modulato.png}
	\caption{Codifica dell'informazione da trasmettere.}
\end{figure}
Enunciamo ora formalmente il cosiddetto teorema della modulazione.

%-------------------------------------------------------------------------------
% Subsubsection: Modulazione con coseno
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Modulazione con coseno}
Se, come di consueto, $x(t) \iff X(f)$, allora definendo
\[
	y(t) = x(t) \cdot \cos(2 \pi f_0 t)
\]
segue che
\[
	Y(f) = \frac{1}{2} \cdot X(f - f_0) + \frac{1}{2} X(f + f_0) =
	\frac{X(f - f_0) + X(f + f_0)}{2}.
\]
\begin{proof}
\[
	Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) e^{-j 2 \pi f t} \dt
	= \int\displaylimits_{-\infty}^{+\infty} x(t) cos(2 \pi f_0 t)
	e^{-j 2 \pi f t} \dt =
\]
\[
	\footnote{$\cos(2 \pi f_0 t) = \frac{e^{j 2 \pi f_0 t} +
	e^{-j 2 \pi f_0 t}}{2}$} = \int\displaylimits_{-\infty}^{+\infty}
	x(t) \cdot \frac{e^{j 2 \pi f_0 t} + e^{-j 2 \pi f_0 t}}{2} \cdot
	e^{-j 2 \pi f t} \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	\frac{e^{-j 2 \pi (f - f_0) t} + e^{-j 2 \pi (f + f_0) t}}{2} \dt =
\]
\[
	= \frac{1}{2} \int\displaylimits_{-\infty}^{+\infty} x(t)
	e^{-j 2 \pi (f - f_0)t} \dt + \frac{1}{2}
	\int\displaylimits_{-\infty}^{+\infty} x(t) e^{-j 2 \pi (f + f_0) t} \dt
	=
\]
\[
	\frac{1}{2} X(f - f_0) + \frac{1}{2} X(f + f_0),
\]
che dimostra la tesi iniziale.
\end{proof}
Una prima conclusione che possiamo trarre \`e la seguente: se un segnale viene
moltiplicato per il fattore esponenziale complesso $e^{j 2 \pi f_0 t}$, la sua
trasformata di Fourier viene traslata attorno alla frequenza $f_0$. Questo
risultato rappresenta la cosiddetta propriet\`a di traslazione in frequenza
della trasformata e pu\`o essere riassunto in
\[
	x(t) e^{j 2 \pi f_0 t} \iff X(f - f_0).
\]
Allora, dato che abbiamo dimostrato che la trasformata del segnale modulato
$x(t) \cdot \cos(2 \pi f_0 t)$ pu\`o essere espresso come
\[
	\mathcal{F}[x(t) \cdot \cos(2 \pi f_0 t)] =
	\frac{X(f - f_0) + X(f + f_0)}{2},
\]
allo stesso modo \`e possibile ottenere
\[
	\mathcal{F}[x(t) \cdot \sin(2 \pi f_0 t)] =
	\frac{X(f - f_0) - X(f + f_0)}{2j},
\]
questo risultato verr\`a approfondito nella sezione successiva.

%-------------------------------------------------------------------------------
% Subsubsection: Modulazione con seno
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Modulazione con seno}
Se, come di consueto, $x(t) \iff X(f)$, allora definendo
\[
	y(t) = x(t) \cdot \sin(2 \pi f_0 t)
\]
segue che
\[
	Y(f) = \frac{1}{2j} \cdot X(f - f_0) - \frac{1}{2j} X(f + f_0) =
	\frac{X(f - f_0) - X(f + f_0)}{2j}.
\]
\begin{proof}
\[
	Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) e^{-j 2 \pi f t}
	\dt = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot 
	\sin(2 \pi f_0 t) \cdot e^{-j 2 \pi f t} \dt =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	\frac{e^{j 2 \pi f_0 t} - e^{-j 2 \pi f_0 t}}{2j} \cdot e^{-j 2 \pi f t}
	\dt =
\]
\[
	= \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	\frac{e^{-j 2 \pi (f - f_0) t}}{2j} \dt -
	\int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
	\frac{e^{-j 2 \pi (f + f_0) t}}{2j} \dt =
\]
\[
	= \frac{1}{2j} \int\displaylimits_{-\infty}^{+\infty} x(t)
	e^{-j 2 \pi (f - f_0) t} \dt - \frac{1}{2j}
	\int\displaylimits_{-\infty}^{+\infty} x(t) e^{-j 2 \pi (f + f_0) t}
	\dt =
\]
\[
    \frac{1}{2j} X(f - f_0) - \frac{1}{2j} X(f + f_0) =
    \frac{X(f - f_0) - X(f + f_0)}{2j}.
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Modulazione con cosinusoide generica
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Modulazione con cosinusoide generica}
Se, come di consueto, $x(t) \iff X(f)$, allora definendo
\[
    y(t) = x(t) \cdot \cos(2 \pi f_0 t + \varphi)
\]
segue che
\[
    Y(f) = \frac{e^{j \varphi}}{2} X(f - f_0) + \frac{e^{-j \varphi}}{2}
    X(f + f_0) = \frac{e^{j \varphi} X(f - f_0) + e^{-j \varphi} X(f + f_0)}{2}.
\]
\begin{proof}
\[
    Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) e^{-j 2 \pi f t} \dt =
    \int\displaylimits_{-\infty}^{+\infty} x(t) cos(2 \pi f_0 t + \varphi)
    e^{-j 2 \pi f t} \dt =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(t)
    \frac{e^{j(2 \pi f_0 t + \varphi)} + e^{-j(2 \pi f_0 t + \varphi)}}{2} 
    e^{-j 2 \pi f t} \dt =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(t) \frac{e^{j \varphi}
    e^{j 2 \pi f_0 t} + e^{-j \varphi} e^{-j 2 \pi f_0 t +}}{2}
    e^{-j 2 \pi f t} \dt =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(t) \frac{e^{j \varphi}
    e^{-j 2 \pi (f - f_0) t} + e^{-j \varphi} e^{-j 2 \pi (f + f_0) t}}{2} =
\]
\[
    = \frac{e^{j \varphi}}{2} \int\displaylimits_{-\infty}^{+\infty} x(t)
    e^{-j 2 \pi (f - f_0) t} \dt + \frac{e^{-j \varphi}}{2}
    \int\displaylimits_{-\infty}^{+\infty} x(t) e^{-j 2 \pi (f + f_0) t} \dt =
\]
\[
    = \frac{e^{j \varphi}}{2} X(f - f_0) + \frac{e^{-j \varphi}}{2} X(f + f_0) =
    \frac{e^{j \varphi} X(f - f_0) + e^{-j \varphi} X(f + f_0)}{2}.
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Modulazione con esponenziale complesso
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Modulazione con esponenziale complesso}
Se, come di consueto, $x(t) \iff X(f)$, allora definendo
\[
    y(t) = x(t) \cdot e^{j 2 \pi f_0 t}
\]
segue che
\[
    Y(f) = X(f - f_0).
\]
\begin{proof}
\[
    Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) e^{-j 2 \pi f t} \dt =
    \int\displaylimits_{-\infty}^{+\infty} x(t) e^{j 2 \pi f_0 t}
    e^{-j 2 \pi f t} \dt = 
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(t) e^{-j 2 \pi (f - f_0) t} \dt
    = X(f - f_0).
\]
\end{proof}
\`E Interessante a questo punto sottolineare due risultati importanti che
abbiamo ottenuto
\bigbreak
\textbf{Teorema del ritardo}
\[
    y(t) = x(t - t_0) \iff Y(f) = X(f) e^{-j 2 \pi f t_0}.
\]
\bigbreak
\textbf{Teorema della modulazione}
\[
    y(t) = x(t) e^{j 2 \pi f_0 t} \iff Y(f) = X(f - f_0).
\]
%-------------------------------------------------------------------------------
% Subsection: Teorema di derivazione
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema di derivazione}
Nell'elaborazione dei segnali a tempo continuo si effettuano spesso operazioni
di derivazione e/o integrazione. Sorge quindi la necesit\`a di determinare le
trasformate dei nuovi segnali ottenuti con tali operazioni.\\
Consideriamo dunque come di consueto un segnale $x(t)$ con trasformata $X(f)$.
Questo pu\`o essere espresso come integrale di Fourier:
\[
    x(t) = \int\displaylimits_{-\infty}^{+\infty} X(f) e^{j 2 \pi f t} \df.
\]
Se, inoltre, il segnale \`e derivabile,
\[
    \frac{d x(t)}{dt} = \frac{d}{dt} \int\displaylimits_{-\infty}^{+\infty}
    X(f) e^{j 2 \pi f t} \df.
\]
Procediamo al calcolo dell'integrale a secondo membro della precedente equazione
invertendo le operazioni di derivazione e integrazione
\[
    \frac{d}{dt} \int\displaylimits_{-\infty}^{+\infty} X(f) e^{j 2 \pi f t} \df
    = \int\displaylimits_{-\infty}^{+\infty} \frac{d}{dt} \left[X(f) 
    e^{j 2 \pi f t}\right] \df =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} X(f) \frac{d}{dt} e^{j 2 \pi f t}
    \df.
\]
Nell'ultimo passaggio \`e stato sfruttato il fatto che l'esponenziale \`e
l'unica funzione che dipende da $t$. Calcolando quindi la derivata si ottiene
\[
    \frac{d}{dt} e^{j 2 \pi f t} = (j 2 \pi f) e^{j 2 \pi f t},
\]
quindi
\[
    \frac{d x(t)}{dt} = \int\displaylimits_{-\infty}^{+\infty} X(f) (j 2 \pi f)
    e^{j 2 \pi f t} \df.
\]
Ponendo ora $Y(f) = (j 2 \pi f) X(f)$ nella precedente, si ottiene:
\[
    \frac{d x(t)}{dt} = \int\displaylimits_{-\infty}^{+\infty} Y(f)
    e^{j 2 \pi f t} \df,
\]
quindi - confrontando questo risultato con la nostra equazione di partenza
$x(t) = \int\displaylimits_{-\infty}^{+\infty} X(f) e^{j 2 \pi f t} \df$ -
possiamo affermare che $Y(f)$ \`e la trasformata della funzione
$\frac{d x(t)}{dt}$. Concludiamo allora con la relazione di corrispondeza che
prende il nome di \textbf{teorema di derivazione}
\[
    \frac{d x(t)}{dt} = j 2 \pi f \cdot X(f).
\]
L'operazione di derivazione temporale di un segnale si traduce, nel dominio
della frequenza, in una semplice operazione algebrica, e cio\`e in una
alterazione di tutte le componenti frequenziali secondo un fattore $j 2 \pi f$
proporzionale al valore della frequenza stessa.\\
Oltre ad uno sfasamento di $\pm \pi/2$ (a seconda del segno di $f$)
l'operazione di derivata comporta in particolare una esaltazione delle
componenti alle alte frequenze.

%-------------------------------------------------------------------------------
% Subsubsection: Teorema di derivazione nel tempo (Prof. Martorella)
%-------------------------------------------------------------------------------

\subsubsection{Teorema di derivazione nel tempo (Prof. Martorella)}
\[
    y(t) = \frac{d}{dt} x(t) \Longrightarrow Y(f) = j 2 \pi f \cdot X(f).
\]
\begin{proof}
\[
    y(t) = \frac{d}{dt} x(t) = \frac{d}{dt} \left[
        \int\displaylimits_{-\infty}^{+\infty} X(f) e^{j 2 \pi f t} \df
    \right] = \int\displaylimits_{-\infty}^{+\infty} X(f) \frac{d}{dt}
    \left[e^{j 2 \pi f t}\right] \df =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} X(f) \cdot (2 j \pi f) \cdot
    e^{j 2 \pi f t} \df = \int\displaylimits_{-\infty}^{+\infty}
    \left[j 2 \pi f \cdot X(f)\right] e^{j 2 \pi f t} \df,
\]
da cui
\[
    y(t) = ATCF\left[j 2 \pi f \cdot X(f)\right]
\]
\[
    Y(f) = j 2 \pi f \cdot X(f).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Teorema di derivazione in frequenza (Prof. Martorella)
%-------------------------------------------------------------------------------

\subsubsection{Teorema di derivazione in frequenza (Prof. Martorella)}
\[
    Y(f) = \frac{d}{df} X(f) \Longrightarrow y(t) = -j 2 \pi f \cdot x(t).
\]
\begin{proof}
\[
    Y(f) = \frac{d}{df} X(f) = \frac{d}{df} \left[
        \int\displaylimits_{-\infty}^{+\infty} x(t) e^{-j 2 \pi f t} \dt
    \right] = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot \frac{d}{df}
    \left[e^{-j 2 \pi f t}\right] \dt =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(t) (-j 2 \pi t) e^{-j 2 \pi f t}
    \dt
\]
da cui segue che
\[
    Y(f) = TCF\left[-j 2 \pi t \cdot x(t)\right] \Longrightarrow y(t) = -j 2
    \pi t \cdot x(t).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Teorema di integrazione
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema di integrazione}
Nell'elaborazione dei segnali a tempo continuo, si effettuano spesso operazioni
di derivazione e/o integrazione temporale dei segnali stessi. Sorge quindi la
necessit\`a di determinare le trasformate dei nuovi segnali ottenuti con tali
operazioni.\\
Risolviamo adesso il problema inverso a quello della derivazione trattato nella
sezione precedente. Indichiamo con $y(t)$ la funzione integrale (o segnale
integrale, o primitiva) di $x(t)$, definita come
\[
    \begin{dcases}
        y(t) = \int\displaylimits_{-\infty}^{t} x(\alpha) \ d \alpha\\
        \int\displaylimits_{-\infty}^{+\infty} x(t) \dt = 0
    \end{dcases}
    \Longrightarrow
    Y(f) = \frac{X(f)}{j 2 \pi f}.
\]
\begin{proof}
Essendo $y(t)$ la funzione integrale di $x(t)$, risulta ovviamente
\[
    x(t) = \frac{d}{dt} y(t),
\]
utilizzando il teorema della derivazione ora
\[
    X(f) = j 2 \pi f \cdot Y(f),
\]
da cui si ricava banalmente
\[
    Y(f) = \frac{X(f)}{2 j \pi f}.
\]
\end{proof}
Questo risultato \`e noto come \textbf{teorema di integrazione} e si riassume
con
\[
    \int\displaylimits_{-\infty}^{t} x(\alpha) \ d \alpha \iff \frac{X(f)
    }{j 2 \pi f}.
\]
Ancora una volta, una operazione di calcolo differenziale in ambito temporale si
traduce in una semplice operazione algebrica (una divisione per il fattore
$j 2 \pi f$) in ambito frequenziale.\\
In questo caso, dualmente al teorema di derivazione, vengono esaltate le
componenti a bassa frequenza nello spettro del segnale e attenuate quelle alle
alte frequenze.

%-------------------------------------------------------------------------------
% Subsection: Teorema di Integrazione nel tempo (Prof. Martorella)
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Teorema di Integrazione nel tempo (Prof. Martorella)}
\[
    \begin{dcases}
        y(t) = \int\displaylimits_{-\infty}^{t} x(\alpha) \ d \alpha\\
        \int\displaylimits_{-\infty}^{+\infty} x(t) \dt = 0
    \end{dcases}
    \Longrightarrow
    Y(f) = \frac{X(f)}{j 2 \pi f}.
\]
\begin{proof}
\[
    y(t) = \int\displaylimits_{-\infty}^{+\infty} x(\alpha) \ d \alpha
    \Longrightarrow
    x(t) = \frac{d}{dt} y(t)
    \Longrightarrow
\]
\[
    \Longrightarrow
    \boldsymbol{\begin{split}teorema \ \ della\\ derivazione\end{split}}
    \Longrightarrow X(f) = j 2 \pi f \cdot Y(f) \Longrightarrow Y(f) = \frac{
    X(f)}{j 2 \pi f}.
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Teorema di Integrazione in frequenza (Prof. Martorella)
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Teorema di Integrazione in frequenza (Prof. Martorella)}
\[
    \begin{dcases}
        Y(f) = \int\displaylimits_{-\infty}^{f} X(\alpha) \ d \alpha\\
        \int\displaylimits_{-\infty}^{+\infty} X(\alpha) \ d \alpha = 0
    \end{dcases}
    \Longrightarrow
    y(t) = - \frac{x(t)}{j 2 \pi t}.
\]
\begin{proof}
Dato che
\[
    Y(f) = \int\displaylimits_{-\infty}^{f} X(\alpha) \ d \alpha \Longrightarrow
    X(f) = \frac{d}{df} Y(f).
\]
Applicando il teorema della derivazione in frequenza a quanto abbiamo ottenuto
\[
    X(f) = \frac{d}{df} Y(f).
\]
otteniamo che
\[
    x(t) = - 2 j \pi t \cdot y(t)
\]
da cui segue infine che
\[
    y(t) = - \frac{x(t)}{2 j \pi t}.
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Teorema del Prodotto di Convoluzione
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema del Prodotto di Convoluzione}
Consideriamo adesso due segnali $x(t)$ e $y(t)$ con le rispettive trasformate di
Fourier date da $X(f)$ e $Y(f)$. Si vuole calcolare la trasformata del segnale
prodotto $z(t) = x(t) \cdot y(t)$. Essa \`e espressa da
\[
    Z(f) = \int\displaylimits_{-\infty}^{+\infty} z(t) \cdot 
    e^{-j 2 \pi f t} \dt = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot 
    y(t) \cdot e^{-j 2 \pi f t} \dt.
\]
Sostituendo a $x(t)$ la sua espressione come integrale di Fourier,
\[
    x(t) = \int\displaylimits_{-\infty}^{+\infty} X(f) \cdot e^{j 2 \pi f t}
    \df,
\]
si ricava
\[
    Z(f) = \int\displaylimits_{t = -\infty}^{+\infty} \left[
        \int\displaylimits_{\nu = -\infty}^{+\infty} X(\nu) \cdot
        e^{j 2 \pi \nu t} \ d \nu
    \right] \cdot y(t) \cdot e^{-j 2 \pi f t} \dt.
\]
Poich\`e nel passaggio precedente compaiono due operazioni di integrazione,
abbiamo esplicitamente indicato sugli estremi di ogni integrale la variabile di
integrazione cui si riferisce; inoltre, nell'espressione di $x(t)$ come
integrale di Fourier, si \`e usata una variabile di integrazione "muta" $\nu$ 
con un nome differente da $f$ per non creare conflitti con la variabile $f$ di
cui la trasformata $Z(f)$ risulta funzione. Se nella precedente equazione si
inverte l'ordine delle operazioni di integrazione, ammettendo che questa
operazione sia lecita, si ricava
\[
    Z(f) = \int\displaylimits_{\nu = -\infty}^{+\infty} X(\nu) \left[
        \int\displaylimits_{t = -\infty}^{+\infty} y(t) \cdot
        e^{-j 2 \pi (f - \nu) t} \dt
    \right] \ d \nu
\]
L'integrale entro parentesi quadre rappresenta la trasformata di $y(t)$
calcolata per il valore della frequenza pari a $(f - \nu)$; di conseguenza
l'equazione precedente pu\`o essere scritta come
\[
    Z(f) = \int\displaylimits_{\nu = -\infty}^{+\infty} X(\nu) \cdot Y(f - \nu)
    \ d \nu = X(f) \otimes Y(f).
\]
L'operazione indicata con il simbolo $\otimes$ prende il nome di integrale (o
talvola impropriamente di prodotto) di convoluzione, o \textit{convoluzione
tout-court}. La convoluzione, introdotta qui in ambito frequenziale per il
calcolo della trasformata del prodotto di due segnali, ha un'importanza 
cardinale nella teoria dei sistemi lineari stazionari.\\
Dunque, il risultato ottenuto pu\`o essere riassunto come:
\[
    x(t) \cdot y(t) \iff X(f) \otimes Y(f).
\]

%-------------------------------------------------------------------------------
% Subsubsection: Teorema del Prodotto (Prof. Martorella)
%-------------------------------------------------------------------------------

\subsubsection{Teorema del Prodotto (Prof. Martorella)}
\[
    z(t) = x(t) \cdot y(t) \Longrightarrow Z(f) = X(f) \otimes Y(f).
\]
\begin{proof}
\[
    Z(f) = \int\displaylimits_{-\infty}^{+\infty} z(t)
    e^{-j 2 \pi f t} \dt = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
    y(t) \cdot e^{-j 2 \pi f t} \dt =
\]
\[
    \footnote{Avendo posto $x(t) = \int\displaylimits_{\alpha = -\infty}^{
    +\infty} X(\alpha) e^{j 2 \pi \alpha t} \ d \alpha$} = 
    \int\displaylimits_{t = -\infty}^{+\infty} \left[
        \int\displaylimits_{\alpha = -\infty}^{+\infty} X(\alpha)
    e^{j 2 \pi \alpha t} \ d \alpha 
    \right] \cdot y(t) \cdot e^{-j 2 \pi f t} \dt =
\]
\[
    = \int\displaylimits_{\alpha = -\infty}^{+\infty} X(\alpha) \left[
        \int\displaylimits_{t = -\infty}^{+\infty} y(t) \cdot
        e^{-j 2 \pi (f - \alpha)} \dt
    \right] \ d \alpha =
\]
\[
    = \int\displaylimits_{\alpha = -\infty}^{+\infty} X(\alpha) Y(f - \alpha) =
    X(f) \otimes Y(f).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Teorema di Convoluzione
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema di Convoluzione}
Consideriamo ora il caso duale del precedente (teorema del prodotto di 
convoluzione), ovvero un segnale $z(t)$ dato dall'integrale di convoluzione in
ambito temporale tra $x(t)$ e $y(t)$:
\[
    z(t) = x(t) \otimes y(t) = \int\displaylimits_{\alpha = -\infty}^{+\infty}
    x(\alpha) \cdot y(t - \alpha) \ d \alpha
\]
\[
    = \int\displaylimits_{\alpha = -\infty}^{+\infty} y(\alpha) x(\alpha - t)
    \ d \alpha,
\]
e calcoliamone la trasformata di Fourier:
\[
    Z(f) = TCF\left[z(t)\right].
\]
Procedendo come nel caso della convoluzione in frequenza ricaviamo
\[
    Z(f) = \int\displaylimits_{t = -\infty}^{+\infty} z(t) \cdot
    e^{-j 2 \pi f t} \dt = \int\displaylimits_{t = -\infty}^{+\infty} \left[
        \int\displaylimits_{\alpha = -\infty}^{+\infty} x(\alpha) \cdot
        y(t - \alpha) \ d \alpha
    \right] e^{-j 2 \pi f t} \dt =
\]
\[
    = \int\displaylimits_{\alpha = -\infty}^{+\infty} x(\alpha)
    \int\displaylimits_{t = -\infty}^{+\infty} y(t - \alpha) \cdot
    e^{-j 2 \pi f t} \dt \ \ d \alpha =
\]
da cui, con la sostituzione $\beta = t - \alpha$, otteniamo
\[
    Z(f) = \int\displaylimits_{\alpha = -\infty}^{+\infty} x(\alpha)
    \int\displaylimits_{\beta = -\infty}^{+\infty} y(\beta) \cdot
    e^{-j 2 \pi f (\beta + \alpha)} \ d \beta \ \ d \alpha =
\]
\[
    = \int\displaylimits_{\alpha = -\infty}^{+\infty} x(\alpha)
    \int\displaylimits_{\beta = -\infty}^{+\infty} y(\beta) \cdot
    e^{-j 2 \pi f \beta} \ d \beta \ e^{-j 2 \pi f \alpha} \ d \alpha =
\]
\[
    = \int\displaylimits_{\alpha = -\infty}^{+\infty} x(\alpha) Y(f) \cdot
    e^{-j 2 \pi f \alpha} \ d \alpha = Y(f) \cdot
    \int\displaylimits_{\alpha = -\infty}^{+\infty} x(\alpha) \cdot
    e^{-j 2 \pi f \alpha} \ d \alpha = X(f) \cdot Y(f).
\]
Dunque il risultato ottenuto pu\`o essere riassunto come
\[
    x(t) \otimes y(t) \iff X(f) \cdot Y(f).
\]

%-------------------------------------------------------------------------------
% Subsubsection: Teorema di Convoluzione (Prof. Martorella)
%-------------------------------------------------------------------------------

\subsubsection{Teorema di Convoluzione (Prof. Martorella)}
\[
    z(t) = x(t) \otimes y(t) = \int\displaylimits_{-\infty}^{+\infty} x(\tau)
    \cdot y(t - \tau) \ d \tau \iff X(f) \cdot Y(f).
\]
\begin{proof}
\[
    Z(f) = \int\displaylimits_{-\infty}^{+\infty} z(t) \cdot e^{-j 2 \pi f t}
    \dt = \int\displaylimits_{t = -\infty}^{+\infty}
    \int\displaylimits_{\tau = -\infty}^{+\infty} x(\tau) \cdot y(t - \tau) \ 
    d \tau \cdot e^{-j 2 \pi f t} \dt =
\]
\[
    = \int\displaylimits_{t = -\infty}^{+\infty} x(\tau)
    \int\displaylimits_{\tau = -\infty}^{+\infty} y(t - \tau)
    \cdot e^{-j 2 \pi f t} \ d \tau \dt =
\]
\[
    \footnote{Per il teorema del ritardo $y(t - \tau) \iff Y(f) \cdot
    e^{-j 2 \pi f \tau}$}= \int\displaylimits_{\tau = -\infty}^{+\infty} x(\tau)
    \cdot Y(f) \cdot e ^{-j 2 \pi f \tau} \ \ d \tau =
\]
\[
    = Y(f) \cdot \int\displaylimits_{\tau = -\infty}^{+\infty} x(\tau)
    e^{-j 2 \pi f \tau} \ d \tau = X(f) \cdot Y(f).
\]
\end{proof}
\bigbreak
\textbf{Uso della $TCF$ per il calcolo del prodotto di convoluzione:}
\[
    x(t) \otimes y(t) = z(t)
\]
\[
    \downarrow \quads{2} \downarrow \quads{2} \downarrow
\]
\[
    X(f) \cdot Y(f) = Z(f)
\]

%-------------------------------------------------------------------------------
% Subsubsection: Proprieta' della Convoluzione
%-------------------------------------------------------------------------------

\subsubsection{Propriet\`a della Convoluzione}
\textbf{Commutativa:}
\[
    x(t) \otimes y(t) = y(t) \otimes x(t)
\]
\begin{proof}
\[
    x(t) \otimes y(t) = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    y(t - \tau) \ d \tau =\footnote{Poniamo $\alpha = t - \tau \Longrightarrow
    \tau = t - \alpha, \ d \alpha = - \ d \tau$. $\alpha$ va da $+\infty$ a
    $-\infty$} \int\displaylimits_{+\infty}^{-\infty} x(t - \alpha) \cdot
    y(\alpha) \ - d \alpha = 
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} y(\alpha) \cdot x(t - \alpha) \ d 
    \alpha = y(t) \otimes x(t).
\]
\end{proof}
\bigbreak\noindent
\textbf{Distributiva}
\[
    x(t) \otimes \left[y(t) + z(t)\right] = x(t) \otimes y(t) + x(t) \otimes
    z(t).
\]
\begin{proof}
\[
    x(t) \otimes \left[y(t) + z(t)\right] =
    \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot \left[
        y(t - \tau) + z(t - \tau)
    \right] \ d \tau =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot y(t - \tau) \ d \tau
    + \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot z(t - \tau) \ d \tau
    =
\]
\[
    = x(t) \otimes y(t) + x(t) \otimes z(t).
\]
\end{proof}
\bigbreak\noindent
\textbf{Associativa}
\[
    x(t) \otimes \left(y(t) \otimes z(t)\right) = (x(t) \otimes y(t)) \otimes 
    z(t).
\]
\begin{proof}
Dal teorema della convoluzione sappiamo che
\[
    z(t) = x(t) \otimes y(t) \iff Z(f) = X(f) \cdot Y(f).
\]
Ora, ponendo $w(t) = y(t) \cdot z(t)$ possiamo scrivere:
\[
    x(t) \otimes \left(y(t) \otimes z(t)\right) = x(t) \otimes w(t).
\]
Calcoliamo ora la trasformata di Fourier
\[
    x(t) \otimes w(t) \iff X(f) \cdot W(f)
\]
con
\[
    W(f) = Y(f) \cdot Z(f).
\]
Quindi possiamo scrivere
\[
    TCD\left[x(t) \otimes \left(y(t) \otimes z(t)\right)\right] = X(f) \cdot
    \left(Y(f) \cdot Z(f)\right) = \left(X(f) \cdot Y(f)\right) \cdot Z(f) =
\]
\[
    \footnote{$ATCF\left[\left(X(f) \cdot Y(f)\right) \cdot Z(f)\right] = \left(
    x(t) \otimes y(t)\right) \otimes z(t)$.}=
    \left(x(t) \otimes y(t)\right) \otimes z(t).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Formule di Somma di Poisson
%-------------------------------------------------------------------------------

\newpage
\subsection{Formule di Somma di Poisson}
Consideriamo un segnale aperiodico $x(t)$ e costruiamo il segnale $y(t)$
periodico di periodo $T_0$ secondo la relazione di \textit{periodicizzazione}
\[
    y(t) = \sum\displaylimits_{n = -\infty}^{+\infty} x(t - nT),
\]
gi\`a incontrata pi\`u volte precedentemente. Il segnale $y(t)$ pu\`o allora
essere sviluppato in serie di Fourier
\[
    y(t) = \sum\displaylimits_{k = -\infty}^{+\infty} Y_k \cdot e^{j 2 \pi k
    f_0 t}.
\]
dove $f_0 = 1/T_0$ e
\[
    Y_k = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} y(t) \cdot e^{-j 2
    \pi k f_0 t} \dt.
\]
Vediamo come stabilire una relazione fra il \textit{coefficiente} $Y_k$ dello
sviluppo in serie del segnale \textit{periodico} $y(t)$ e la
\textit{trasformata} $X(f)$ del segnale base \textit{aperiodico} $x(t)$.
Sostituiamo dunque la periodicizzazione che fornisce il segnale  $y(t)$
nell'espressione del coefficiente $Y_k$:
\[
    Y_k = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} y(t) \cdot e^{-j 2
    \pi k f_0 t} \dt = \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
    \sum\displaylimits_{n = -\infty}^{+\infty} x(t - nT) \cdot
    e^{-j 2 \pi k f_0 t} \dt
\]
scambiando poi l'operazione di sommatoria e di integrazione, si ha
\[
    Y_k = \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty}
    \int\displaylimits_{-T_0/2}^{T_0/2} x(t - nT) \cdot
    e^{-j 2 \pi k f_0 t} \dt =
\]
\[
    = \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty}
    \int\displaylimits_{-T_0/2 - n T_0}^{T_0/2 - nT_0} x(\alpha) \cdot
    e^{-j 2 \pi k f_0 (\alpha + nT_0)} \ d \alpha =
\]
\[
    = \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty}
    \int\displaylimits_{-T_0/2 - nT_0}^{T_0/2 - nT_0} x(\alpha) \cdot
    e^{-j 2 \pi k f_0 \alpha}
\]
dove si \`e giunti al risultato finale dopo aver effettuato il cambiamento di
variabile $\alpha = t - nT_0$ e avendo osservato che $e^{-j 2 \pi k n f_0 T_0} =
e^{-j 2 \pi k n} \equiv 1$.\\
La funzione integranda a secondo membro non dipende dall'indice della serie $n$;
tale indice agisce sugli \textit{estremi di integrazione}. Ci si rende allora
conto facilmente che, al variare di $n$ tra $-\infty$ e $+\infty$, gli
intervalli di integrazione $(-T_0/2 - nT_0, T_0/2 - nT_0)$ \textit{della stessa
funzione integranda} ricoprono tutto l'asse reale senza sovrapposizioni.
Pertanto \`e possibile semplificare la precedente relazione in
\[
    Y_k = \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty}
    \int\displaylimits_{-T_0/2 - nT_0}^{T_0/2 - nT_0} x(\alpha) \cdot
    e^{-j 2 \pi k f_0 \alpha} =
    \frac{1}{T_0} \int\displaylimits_{-\infty}^{+\infty} x(\alpha) \cdot
    e^{-j 2 \pi k f_0 \alpha} \ d \alpha = 
\]
\[
    = \frac{1}{T_0} \cdot X(k f_0) = f_0 \cdot X(k f_0) = \frac{1}{T_0} \cdot
    X\left(\frac{k}{T_0}\right),
\]
che stabilisce la relazione cercata, detta di \textit{campionamento in
frequenza}. I coefficienti di Fourier del segnale periodico $y(t)$ sono dunque,
a meno del fattore $1/T_0$, i valori (campioni) della trasformata continua del
segnale-base $x(t)$ presi in corrispondeza delle \textit{frequenze armoniche}
$k f_0$.

%-------------------------------------------------------------------------------
% Subsubsection: Prima Formula di somma di Poisson
%-------------------------------------------------------------------------------

\subsubsection{Prima Formula di somma di Poisson}

Se usiamo l'espressione appena ricavata del coefficiente di Fourier nel
corrispondente sviluppo in serie si ottiene
\[
    y(t) = \sum\displaylimits_{n = -\infty}^{+\infty} x(t - nT) =
    \sum\displaylimits_{k = -\infty}^{+\infty} Y_k \cdot e^{j 2 \pi k f_0 t} =
\]
\[
    = \sum\displaylimits_{k = -\infty}^{+\infty} \frac{1}{T_0} \cdot
    X\left(\frac{k}{T_0}\right) \cdot e^{j \cdot \frac{2 \pi k t}{T_0}}
\]
che \`e nota come \textit{prima formula di somma di Poisson}.

%-------------------------------------------------------------------------------
% Subsubsection: Seconda Formula di somma di Poisson
%-------------------------------------------------------------------------------

\subsubsection{Seconda Formula di somma di Poisson}

Applichiamo adesso il teorema della dualit\`a
\[
    x(t) \iff X(f) \quads{2} \Longrightarrow \quads{2} X(t) \iff x(-f).
\]
alla prima formula di Poisson, ottenendo
\[
    \sum\displaylimits_{n = -\infty}^{+\infty} X(t - nT_0) =
    \sum\displaylimits_{k = -\infty}^{+\infty} \frac{1}{T_0}
    x\left(-\frac{k}{T_0}\right) \cdot e^{j \cdot \frac{2 \pi k t}{T_0}}
\]
\[
    \Rightarrow \sum\displaylimits_{n = -\infty}^{+\infty} X(t - nT_0) =
    \sum\displaylimits_{k = -\infty}^{+\infty} \frac{1}{T_0}
    x\left(\frac{k}{T_0}\right) \cdot e^{-j \cdot \frac{2 \pi k t}{T_0}}
\]
\[
    \Rightarrow \sum\displaylimits_{n = -\infty}^{+\infty} X\left(t -
    \frac{n}{T}\right) = T \sum\displaylimits_{k = -\infty}^{+\infty}
    x(kT) \cdot e^{-j 2 \pi k t T}
\]
avendo cambiato segno all'indice di sommatoria nel secondo membro e avendo posto
$T = 1/T_0$. Se adesso, dal punto di vista puramente formale, cambiamo nome alla
variabile corrente da $t$ a $f$, otteniamo un'espressione "duale" alla prima
formula di Poisson che costituisce la \textit{seconda formula di somma di
Poisson}
\[
    \sum\displaylimits_{n = -\infty}^{+\infty} x(nT) \cdot e^{-j 2 \pi n f T} =
    \frac{1}{T} \sum\displaylimits_{k = -\infty}^{+\infty} X\left(f -
    \frac{k}{T}\right)
\]
la quale riveste importanza fondamentale nel \textit{campionamento} dei segnali
a tempo continuo.

%-------------------------------------------------------------------------------
% Subsubsection: Applicazione delle formule di Poisson alla Delta di Dirac
%-------------------------------------------------------------------------------

\subsubsection{Applicazione delle formule di Poisson alla Delta di Dirac}
Sia
\[
    x(t) = \delta(t) \iff X(f) = 1
\]
allora, dalla \textbf{prima formula di Poisson} segue che
\[
    \sum\displaylimits_{n = -\infty}^{+\infty} \delta(t - nT_0) = \frac{1}{T_0}
    \sum\displaylimits_{n = -\infty}^{+\infty} X\left(\frac{n}{T_0}\right)
    e^{j 2 \pi n f_0 t} = \frac{1}{T_0}
    \sum\displaylimits_{n = -\infty}^{+\infty} e^{j 2 \pi n f_0 t},
\]
e dalla \textbf{seconda formula di Poisson} segue che
\[
    \sum\displaylimits_{n = -\infty}^{+\infty} e^{-j 2 \pi n f T_0} =
    \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty} \delta\left(f -
    \frac{n}{T_0}\right).
\]

%-------------------------------------------------------------------------------
% Subsection: Dimostrare la TCF di un segnale periodico
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare la $TCF$ di un segnale periodico}
Sia $x(t)$ un segnale aperiodico che ammette $TCF$
\[
    x(t) \iff X(f)
\]
e sia $y(t)$ il segnale ottenuto per periodicizzazione di $x(t)$
\[
    y(t) = \sum\displaylimits_{n = -\infty}^{+\infty} x(t - n T_0), \ \ \ T_0
    \in \mathbb{R}^{+}.
\]
Allora
\[
    y(t) \iff Y(f) = \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty}
    X\left(\frac{n}{T_0}\right) \cdot \delta\left(f - \frac{n}{T_0}\right) =
\]
\[
    = \sum\displaylimits_{n = -\infty}^{+\infty} Y_n \cdot \delta\left(f -
    \frac{n}{T_0}\right)
\]
dove
\[
    y(t) \iff Y_n = \frac{1}{T_0} \cdot X\left(\frac{n}{T_0}\right).
\]
\begin{proof}
\[
    Y(f) = \int\displaylimits_{-\infty}^{+\infty} y(t) \cdot e^{-j 2 \pi f t}
    \dt = \int\displaylimits_{-\infty}^{+\infty}
    \sum\displaylimits_{n = -\infty}^{+\infty} x(t - nT_0) \cdot
    e^{-j 2 \pi f t} \dt =
\]
\[
    \footnote{Poniamo $t' = t - nT_0$}= \int\displaylimits_{-\infty}^{+\infty}
    \sum\displaylimits_{n = -\infty}^{+\infty} x(t') \cdot
    e^{-j 2 \pi f (t' + nT_0)} \dt' = \sum\displaylimits_{n = -\infty}^{+\infty}
    \int\displaylimits_{-\infty}^{+\infty} x(t') \cdot e^{-j 2 \pi f t'} \dt'
    \cdot e^{-j 2 \pi n f T_0} =
\]
\[
    = \sum\displaylimits_{n = -\infty}^{+\infty} X(f) \cdot e^{-j 2 \pi n f T_0}
    = X(f) \sum\displaylimits_{n = -\infty}^{+\infty} e^{-j 2 \pi n f T_0} =
\]
\[
    \footnote{Per la seconda formula di Poisson.}= X(f) \cdot \frac{1}{T_0}
    \sum\displaylimits_{n = -\infty}^{+\infty} \delta\left(
        f - \frac{n}{T_0}
    \right) \footnote{$f = n \cdot f_0 = \frac{n}{T_0}$}= \frac{1}{T_0}
    \sum\displaylimits_{n = -\infty}^{+\infty}
    X\left(\frac{n}{T_0}\right) \cdot \delta\left(f - \frac{n}{T_0}\right) =
\]
\[
    \footnote{Dalla relazione tra $TSF$ e $TCF$ per un segnale $y(t)$ ottenuto
    tramite periodicizzazione di un segnale aperiodico $x(t)$, sappiamo che
    $Y_n = \frac{1}{T_0} \cdot X\left(\frac{n}{T_0}\right) = f_0 \cdot X(n \cdot
    f_0)$} = \sum\displaylimits_{n = -\infty}^{+\infty} Y_n \cdot \delta\left(
        f - \frac{n}{T_0}
    \right).
\]
\end{proof}
Da questo risultato segue che periodicizzare nel tempo corrisponde a campionare
in frequenza.

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che la TSF di x(t) = sum of x_0(t - nT) e' scrivibile
%             tramite la TCF di x_0(t).
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che la $TSF$ di $x(t) = \sum\displaylimits_{n}
x_0(t - nT)$ \`e scrivibile tramite la $TCF$ di $x_0(t)$}
Consideriamo un segnale $x(t)$ periodico
\[
    x(t) \sum\displaylimits_{m = -\infty}^{+\infty} x_0(t - mT_0),
\]
\[
    x(t) \iff X_n,
\]
\[
    X_n = TSF\{x(t)\} \triangleq \frac{1}{T_0} \int\displaylimits_{-T_0/2}
    ^{T_0/2} x(t) \cdot e^{-j 2 \pi n f_0 t} \dt
\]
La $TSF$ di $x(t)$ pu\`o alternativamente essere calcolata come
\[
    X_n = \frac{1}{T_0} \cdot X_0\left(\frac{n}{T_0}\right)
\]
con
\[
    X_0(f) = TCF\{x_0(t)\}.
\]
Quindi, per calcolare la $TSF$ di un segnale periodico basta calcolare la $TCF$
del segnale aperiodico di base.
\begin{proof}
\[
    X_n = \frac{1}{T_0} \cdot \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot
    e^{-j 2 \pi n f_0 t} \dt = 
\]
\[
    = \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot
    \sum_{m = -\infty}^{+\infty} x_0(t - mT_0) \cdot e^{-j 2 \pi n f_0 t} \dt =
\]
\[
    = \frac{1}{T_0} \sum_{m = -\infty}^{+\infty} \int\displaylimits_{-T_0/2}
    ^{T_0/2} x_0(t - mT_0) \cdot e^{-j 2 \pi n f_0 t} \dt =
\]
\[
    \footnote{Pongo $t' = t - mT_0 \Longrightarrow t = t' + mT_0$.}=
    \frac{1}{T_0} \sum_{m = -\infty}^{+\infty}
    \int\displaylimits_{-T_0/2 - mT_0}^{T_0/2 - mT_0} x_0(t') \cdot
    e^{-j 2 \pi n f_0 (t' + mT_0)} \dt' =
\]
\[
    \footnote{$\sum_{m = -\infty}^{+\infty} \int\displaylimits_{-T_0/2 - mT_0}
    ^{T_0/2 - mT_0} = \int\displaylimits_{-\infty}^{+\infty}$.}= \frac{1}{T_0}
    \int\displaylimits_{-\infty}^{+\infty} x_0(t') \cdot e^{-j 2 \pi n f_0 t'}
    \dt' \cdot e^{-j 2 \pi n f_0 m T_0} =
\]
\[
    \footnote{$e^{-j 2 \pi n f_0 m T_0} = e^{-j 2 \pi n m} = 1$.}=
    \frac{1}{T_0} \int\displaylimits_{-\infty}^{+\infty} x_0(t') \cdot
    e^{-j 2 \pi n f_0 t'} \dt' =
\]
\[
    = \frac{1}{T_0} X_0(n f_0) = \frac{1}{T_0} X_0\left(\frac{n}{T_0}\right).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Teorema di Parseval
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema di Parseval}
In analisi complessa il teorema di Parseval o identit\`a di Rayleigh, il cui 
nome \`e dovuto a Marc-Antoine Parseval, \`e un teorema che stabilisce che la 
sommatoria del prodotto dei coefficienti di Fourier di due funzioni periodiche 
\`e uguale all'integrale del loro prodotto.\\
Nonostante il termine "teorema di Parseval" sia spesso utilizzato per descrivere
l'unitariet\`a di ogni trasformata di Fourier, in particolar modo in fisica e in
ingegneria, la forma pi\`u generale di questa propriet\`a \`e data dal teorema
di Plancherel.

%-------------------------------------------------------------------------------
% Subsubsection: Definire e dimostrare il teorema di Parseval per segnali
%                aperiodici
%-------------------------------------------------------------------------------

\subsubsection{Definire e dimostrare il teorema di Parseval per segnali
aperiodici}
Se,
\[
    x(t) \iff X(f),
\]
e
\[
    y(t) \iff Y(f),
\]
allora
\[
    \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot y^*(t) \dt =
    \int\displaylimits_{-\infty}^{+\infty} X(f) \cdot Y^*(f) \df.
\]
\begin{proof}
\[
    \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot y^*(t) \dt =
    \int\displaylimits_{t = -\infty}^{+\infty}
    \int\displaylimits_{f = -\infty}^{+\infty} X(f) \cdot e^{j 2 \pi f t}
    \df \cdot y^*(t) \dt =
\]
\[
    = \int\displaylimits_{f = -\infty}^{+\infty} X(f) \cdot \left[
        \int\displaylimits_{t = -\infty}^{+\infty} y(t) \cdot e^{-j 2 \pi f t}
        \dt
    \right]^* \df = \int\displaylimits_{-\infty}^{+\infty} X(f) \cdot Y^*(f) \df
    .
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Definire e dimostrare il teorema di Parseval per segnali
%                periodici
%-------------------------------------------------------------------------------

\subsubsection{Definire e dimostrare il teorema di Parseval per segnali
periodici (con $TSF$)}
Consideriamo due segnali $x(t)$ e $y(t)$ periodici di periodo $T_0$ che
ammettono $TSF$
\[
    x(t) \iff X_n,
\]
\[
    y(t) \iff Y_n.
\]
Allora
\[
    \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot y^*(t) \dt =
    \sum\displaylimits_{n = -\infty}^{+\infty} X_n \cdot Y^*_n.
\]
\begin{proof}
\[
    \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot y^*(t) \dt =
    \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2}
    \sum\displaylimits_{n = -\infty}^{+\infty} X_n \cdot e^{j 2 \pi n f_0 t}
    \cdot y^*(t) \dt =
\]
\[
    = \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty} X_n \cdot
    \int\displaylimits_{-T_0/2}^{T_0/2} y^*(t) \cdot e^{j 2 \pi n f_0 t} \dt =
    \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty} X_n \cdot
    \left[
        \int\displaylimits_{-T_0/2}^{T_0/2} y(t) \cdot e^{-j 2 \pi n f_0 t}
    \right]^* =
\]
\[
    = \sum\displaylimits_{n = -\infty}^{+\infty} X_n \cdot
    \frac{1}{T_0} \cdot \left[
        \int\displaylimits_{-T_0/2}^{T_0/2} y(t) \cdot e^{-j 2 \pi n f_0 t}
    \right]^* =
    \sum\displaylimits_{n = -\infty}^{+\infty} X_n \cdot Y^*_n.
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Definire e dimostrare il teorema di Parseval per segnali
%                periodici
%-------------------------------------------------------------------------------

\subsubsection{Definire e dimostrare il teorema di Parseval per segnali
periodici (con $TCF$)}
Tenendo a mente quanto dimostrato usando la $TSF$ sella sezione precedente,
consideriamo due segnali $x(t)$ e $y(t)$ periodici di periodo $T_0$ che
ammettono $TSF$. Prendiamo ora in esame il segnale base da cui vengono ottenuti
per \textit{periodicizzazione} questi due segnali
\[
    x_0(t) = x(t) \cdot rect\left(\frac{t}{T_0}\right),
\]
\[
    y_0(t) = y(t) \cdot rect\left(\frac{t}{T_0}\right).
\]
Essendo aperiodici, $x_0(t)$ e $y_0(t)$ ammettono $TCF$
\[
    x_0(t) \iff X_0(f),
\]
\[
    y_0(t) \iff Y_0(f).
\]
Da quanto premesso, segue quindi che
\[
    x(t) \iff X_n = \frac{1}{T_0} X_0\left(\frac{k}{T_0}\right),
\]
\[
    y(t) \iff y_n = \frac{1}{T_0} Y_0\left(\frac{k}{T_0}\right).
\]
Allora
\[
    \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot y^*(t) \dt =
    = \sum\displaylimits_{n = -\infty}^{+\infty} \frac{1}{T_0}
    X_0\left(\frac{n}{T_0}\right) \cdot \frac{1}{T_0}
    Y_0^*\left(\frac{n}{T_0}\right) =
\]
\[
    =\frac{1}{T_0^2} \sum\displaylimits_{n = -\infty}^{+\infty}
    X_0\left(\frac{n}{T_0}\right) \cdot Y_0^*\left(\frac{n}{T_0}\right).
\]
\begin{proof}
\[
    \frac{1}{T_0} \int\displaylimits_{-T_0/2}^{T_0/2} x(t) \cdot y^*(t) \dt =
    \sum\displaylimits_{n = -\infty}^{+\infty} X_n \cdot Y^*_n = \frac{1}{T_0^2}
    \sum\displaylimits_{n = -\infty}^{+\infty} X_0\left(\frac{n}{T_0}\right)
    Y_0^*\left(\frac{n}{T_0}\right).
\]
\end{proof}
In pratica, si tratta di una nuova relazione ottenuta sfruttando la relazione
tra $TSF$ e $TCF$ precedentemente dimostrata e applicando il teorema di Parseval
per segnali periodici (con $TSF$).

%-------------------------------------------------------------------------------
% Subsubsection: Definire e dimostrare il teorema di Parseval per le sequenze
%-------------------------------------------------------------------------------

\subsubsection{Definire e dimostrare il teorema di Parseval per le sequenze}
Se
\[
    x[n] \iff \overline{X}(f)
\]
e
\[
    y[n] \iff \overline{Y}(f)
\]
allora
\[
    \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot y^*[n] = T
    \int\displaylimits_{-1/2T}^{1/2T} \overline{X}(f) \cdot \overline{Y}^*(f)
    \df
\]
\begin{proof}
\[
    \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot y^*[n] \  
    \footnote{L'antitrasformata di Fourier di una sequenza $x[n] = T
    \int\displaylimits_{-1/2T}^{1/2T} \overline{X}(f) \cdot e^{j 2 \pi f n T}
    \df$.}= \sum\displaylimits_{n = -\infty}^{+\infty} T
    \int\displaylimits_{-1/2T}^{1/2T} \overline{X}(f) \cdot e^{j 2 \pi f n T}
    \df \cdot y^*[n] =
\]
\[
    = T \int\displaylimits_{-1/2T}^{1/2T} \overline{X}(f) \cdot
    \sum\displaylimits_{n = -\infty}^{+\infty} y^*[n] \cdot e^{j 2 \pi f n T}
    \df =
\]
\[
    = T \int\displaylimits_{-1/2T}^{1/2T} \overline{X}(f) \cdot \left[
    \sum\displaylimits_{n = -\infty}^{+\infty} y[n] \cdot e^{j 2 \pi f n T}
    \right]^* \df =
\]
\[
    = T \int\displaylimits_{-1/2T}^{1/2T} \overline{X}(f) \cdot
    \overline{Y}^*(f) \df.
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Definire l'operazione di convoluzione ed illustrarne le proprieta'
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire l'operazione di convoluzione ed illustrarne le propriet\`a}
In matematica, in particolare nell'analisi funzionale, la convoluzione \`e
un'operazione tra due funzioni di una variabile che consiste nell'integrare il
prodotto tra la prima e la seconda traslata di un certo valore. Ha una forte
somiglianza con la correlazione incrociata.\\
La convoluzione viene utilizzata in vari campi della fisica, della statistica,
dell'elettronica, dell'analisi d'immagini e della grafica computerizzata.
Quando si studiano sistemi dinamici lineari stazionari, l'uscita \`e data dalla
convoluzione tra il segnale in ingresso e la risposta all'impulso del sistema,
la cui trasformata di Laplace (o la trasformata di Fourier) \`e la funzione di
trasferimento del sistema. 
\bigbreak
Si considerino due segnali $x(t)$ e $y(t)$. Si definisce convoluzione di $x(t)$
e $y(t)$ la funzione definita nel seguente modo
\[
    z(t) = x(t) * y(t) = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    y(t - \tau) \ d\tau
\]

%-------------------------------------------------------------------------------
% Subsubsection: Proprieta' della Convoluzione
%-------------------------------------------------------------------------------

\subsubsection{Propriet\`a della Convoluzione}
\textbf{Propriet\`a Commutativa}\\
\[
    x(t) * y(t) = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    y(t - \tau) \ d\tau \footnote{Poniamo $\tau' = t - \tau$.}=
    \int\displaylimits_{+\infty}^{-\infty} x(t - \tau') \cdot y(\tau')
    \ -d\tau' =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(t - \tau') \cdot y(\tau')
    \ d\tau' = y(t) * x(t)
\]
\bigbreak
\noindent
\textbf{Propriet\`a Distributiva}\\
\[
    x(t) * [y(t) + z(t)] = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    [y(t - \tau) \cdot z(t - \tau)] \ d\tau =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot y(t - \tau) \ d\tau +
    \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot z(t - \tau) \ d\tau =
\]
\[
    = x(t) * y(t) + x(t) * z(t).
\]
\bigbreak
\noindent
\textbf{Propriet\`a Associativa}\\
Grazie al teorema della convoluzione, possiamo scrivere che
\[
    x(t) * [y(t) * z(t)] \iff X(f) \cdot [Y(f) \cdot Z(f)] =
\]
\[
    = [X(f) \cdot Y(f)] \cdot Z(f) \iff [x(t) * y(t)] * z(t).
\]

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che la TFS di x[n] = x(nT) e' scrivibile tramite la TCF
%             di x(t)
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che la TFS di $x[n] = x(nT)$ \`e scrivibile tramite la
$TCF$ di $x(t)$}
Prendiamo in considerazione il campionamento di un segnale aperiodico $x(t)$:
\[
    x[n] = x(nT)
\]
Dato che $x(t)$ \`e un segnale aperiodico ammette $TCF$:
\[
    x(t) \iff X(f)
\]
mentre $x[n]$ ammette $TFS$:
\[
    x[n] \iff \overline{X}(f).
\]
Possiamo allora scrivere che
\[
    \overline{X}(f) = \frac{1}{T} \sum\displaylimits_{n = -\infty}^{+\infty}
    X\left(f - \frac{n}{T}\right).
\]
\[
    \overline{X}(f) = \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot
    e^{-j 2 \pi f n T} =
\]
\[
    = \sum\displaylimits_{n = -\infty}^{+\infty} x(nT) \cdot
    e^{-j 2 \pi f n T} =
\]
\[
    = \sum\displaylimits_{n = -\infty}^{+\infty}
    \int\displaylimits_{-\infty}^{+\infty} X(\alpha) \cdot e^{j 2 \pi \alpha n
    T} \cdot \ d\alpha \ \ e^{-j 2 \pi f n T} =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} X(\alpha) 
    \sum\displaylimits_{n = -\infty}^{+\infty} e^{j 2 \pi \alpha n T}
    \cdot e^{-j 2 \pi f n T} \ d\alpha =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} X(\alpha) 
    \sum\displaylimits_{n = -\infty}^{+\infty} e^{-j 2 \pi (f - \alpha) n T}
    \ d\alpha =
\]
\[
    \footnote{Segue dall'applicazione della seconda formula di Poisson alla
    Delta di Dirac $\sum_{n = -\infty}^{+\infty} e^{-j 2 \pi n f T_0} =
    \frac{1}{T_0} \sum\displaylimits_{n = -\infty}^{+\infty} \delta\left(f -
    \frac{n}{T_0}\right)$.} = \int\displaylimits_{-\infty}^{+\infty} X(\alpha)
    \ \frac{1}{T} \sum\displaylimits_{n = -\infty}^{+\infty} \delta\left(
        f - \alpha - \frac{n}{T}
    \right) \ d\alpha =
\]
\[
    \footnote{Segue dalla propriet\`a di parit\`a della delta di Dirac
    $\int\displaylimits_{-\infty}^{+\infty} x(t) \cdot \delta(t) \dt =
    \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot \delta(-t)$.} =
    \int\displaylimits_{-\infty}^{+\infty} X(\alpha)
    \ \frac{1}{T} \sum\displaylimits_{n = -\infty}^{+\infty} \delta\left(
        \alpha - f + \frac{n}{T}
    \right) \ d\alpha =
\]
\[
    = \frac{1}{T} \sum\displaylimits_{n = -\infty}^{+\infty}
    \int\displaylimits_{-\infty}^{+\infty} X(\alpha) \cdot \delta\left(
        \alpha - \left(f - \frac{n}{T}\right)
    \right) \ d\alpha =
\]
\[
    \footnote{Per la propriet\`a di traslazione della delta di Dirac $x(t) *
    \delta(t) - t_0 = x(t - t_0)$.}= \frac{1}{T}
    \sum\displaylimits_{n = -\infty}^{+\infty} X\left(f - \frac{n}{T}\right)
\]

%-------------------------------------------------------------------------------
% Subsection: Definire l'operazione di autocorrelazione per segnali aperiodici
%             ed illustrarne le proprieta'
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire l'operazione di autocorrelazione per segnali aperiodici ed
illustrarne le propriet\`a}

%-------------------------------------------------------------------------------
% Subsection: Definire la cross-correlazione e la convoluzione tra due segnali
%             x(t) e y(t).
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire la cross-correlazione e la convoluzione tra due segnali
$x(t)$ e $y(t)$}
La convoluzione tra due segnali $x(t)$ e $y(t)$ \`e data da
\[
    x(t) * y(t) \triangleq \int\displaylimits_{-\infty}^{+\infty} x(\tau)
    \cdot y(t - \tau) \ d\tau,
\]
mentre la cross-correlazione \`e definita come
\[
    C_{xy} (t) \triangleq \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    y^*(\tau - t) \ d\tau.
\]

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che la cross-correlazione tra x(t) e y(t) e' scrivibile
%             in termini di convoluzione tra i due segnali.
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che la cross-correlazione tra $x(t)$ e $y(t)$ \`e
scrivibile in termini di convoluzione tra i due segnali}
Si ricorda che la convoluzione tra due segnali $x(t)$ e $y(t)$ \`e data da
\[
    x(t) * y(t) \triangleq \int\displaylimits_{-\infty}^{+\infty} x(\tau)
    \cdot y(t - \tau) \ d\tau,
\]
mentre la cross-correlazione \`e definita come
\[
    C_{xy} (t) \triangleq \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    y^*(\tau - t) \ d\tau.
\]
\begin{proof} La relazione tra la cross-correlazione e la convoluzione \`e data
\[
    x(t) * y^*(-t) = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    y^*[-(t - \tau)] \dt = \int\displaylimits_{-\infty}^{+\infty} x(\tau) \cdot
    y^*(\tau - t) \dt = C_{xy}(t)
\]
\[
    \Longrightarrow C_{xy}(t) = x(t) * y^*(-t).
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che la TFS di x[n] = x(nT) e' periodica di periodo 1/T
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che la TFS di $x[n] = x(nT)$ \`e periodica di periodo
$1/T$}
Dato
\[
    x[n] \iff \overline{X}(f)
\]
sappiamo che per definizione
\[
    \overline{X}(f) \triangleq \sum\displaylimits_{n} x[n] \cdot
    e^{-j 2 \pi f n T}
\]
Dobbiamo ora dimostrare che
\[
    \overline{X}(f) = \overline{X}\left(f - \frac{k}{T}\right) \quads{2} \forall
    \ \ k \in \mathbb{Z}
\]
\begin{proof}
\[
    \overline{X}\left(f - \frac{k}{T}\right) = \sum\displaylimits_{n = -\infty}
    ^{+\infty} x[n] \cdot e^{-j 2 \pi n (f - \frac{k}{T}) T} =
    \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot
    e^{-j 2 \pi n f T} \cdot e^{j 2 \pi n k} =
\]
\[
    \footnote{Dato che $e^{j 2 \pi n k} = 1$.} =
    \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot e^{-j 2 \pi n f T} 
    = \overline{X}(f)
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Dare la definizione di Densita' spettrale di Energia e dimostrare
%             che Sx(f) = |X(f)|^2
%-------------------------------------------------------------------------------

\newpage
\subsection{Dare la definizione di Densit\`a spettrale di Energia e dimostrare
che $S_x(f) = \abs{X(f)}^2$}

%-------------------------------------------------------------------------------
% Subsection: Definire la funzione delta di Dirac e illustrare le sue proprieta'
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire la funzione delta di Dirac e illustrare le sue propriet\`a}
Consideriamo di nuovo la funzione \textit{gradino unitario} rappresentata nella
seguente figura
\[
    u(t) =
        \begin{dcases}
            1 \quads{4} t > 0\\
            1/2 \quads{3} t = 0\\
            0 \quads{4} t < 0
        \end{dcases}
\]
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.5\textwidth]{images/heaviside_function.png}
    \caption{Segnale Gradino Unitario $u(t)$.}
\end{figure}
Evidentemente, il segnale gradino unitario presenta una discontinuit\`a di prima
specie per $t = 0$. Il valore assegnato alla funzione in questo punto pu\`o
essere scelto arbitrariamente: per consisenza con il criterio di Dirichlet, si
scegli la semisomma dei limiti destro e sinistro, cio\`e si pone $u(0) = 1/2$.
\bigbreak
Consideriamo allora come migliore approssimazione una fnuzione gradino "reale"
$u_{\epsilon}(t)$ illustrata in figura e definita dalla relazione
% IMMAGINE
\[
    u_{\epsilon} =
        \begin{dcases}
            0 \quads{5} t < -\epsilon\\
            \frac{1 + t/\epsilon}{2} \quad -\epsilon \leq t \leq \epsilon\\
            1 \quads{5} t > \epsilon
        \end{dcases}
\]
L'andamento del gradino durante l'intervallo di "salita" $(-\epsilon, \epsilon)$
\`e stato preso linere per semplicit\`a. Le considerazioni che stiamo per fare
non cambierebbero per\`o anche se tale andamento fosse ancora pi\`u simile a
quello di un segnale reale (in particolare ancora pi\`u regolare). Il segnale
$u_{\epsilon}(t)$ \`e adesso ovunque derivabile e la sua derivata
\[
    \delta_{\epsilon}(t) = \frac{d u_{\epsilon}(t)}{dt}
\]
\`e rappresentata nella seguente figura:
% IMMAGINE
Evidentemente,
\[
    \delta_{\epsilon}(t) = \frac{1}{2 \epsilon} rect\left(\frac{t}{2 \epsilon}
    \right)
\]
e inoltre
\[
    \delta_{\epsilon}(t) = \frac{d u_{\epsilon}(t)}{dt} \Longrightarrow
    u_{\epsilon}(t) = \int\displaylimits_{-\infty}^{t} \delta_{\epsilon}(\alpha)
    \ d\alpha
\]
senz'alcuna ambiguit\`a.
\bigbreak
Osserviamo che riducendo il valore del parametro $\epsilon$, ossia riducendo il
tempo di salita del segnale, si ottiene una approssimazione sempre migliore del
gradino ideale, tant'\`e che si pu\`o scrivere
\[
    u(t) = \lim_{\epsilon \rightarrow 0} u_{\epsilon}(t) =
    \lim_{\epsilon \rightarrow 0}
    \int\displaylimits_{-\infty}^{t} \delta_{\epsilon}(\alpha) \ d\alpha
\]
da cui segue
\[
    u(t) = \lim_{\epsilon \rightarrow 0}
    \int\displaylimits_{-\infty}^{t} \delta_{\epsilon}(\alpha) \ d\alpha
\]
% IMMAGINE
Il problema di trovare la "derivata della funzione gradino" sarebbe risolto se
nella precedente relazione potessimo eseguire l'operazione di limite
\textit{sotto il segno d'integrale} ponendo dunque
\[
    \frac{d u(t)}{dt} \triangleq \lim_{\epsilon \rightarrow 0}
    \delta_{\epsilon}(t) = \delta(t).
\]
Otterremmo cos\`i una funzione $\delta(t)$ con l'apparenza della derivata del
gradino ideale come limite della successione di funzioni $\delta_{\epsilon}(t)$.
Purtroppo, come \`e evidente dalla precedente immagine, il limite di tale
successione \textit{non \`e una funzione} nel senso ordinario dell'analisi
matematica. Infatti tale funzione dovrebbe assumere ovunque valore nullo al di
fuori del punto $t = 0$ e, se integrata, dovrebbe restituire un valore finito
diverso da zero. Non \`e corretto quindi affermare che $\delta(t)$ \`e il limite
della successione di funzioni $\delta_{\epsilon}(t)$. Tuttavia, per estensione,
ammettiamo di definire una \textit{funzione generalizzata impulso unitario}
$\delta(t)$ o funzione $\delta$ di Dirac, attraverso una sua propriet\`a di
carattere integrale:
\[
    u(t) = \int\displaylimits_{-\infty}^{t} \delta(\alpha) \ d\alpha
\]
che la identifica come "derivata della funzione gradino".
\bigbreak
Come abbiamo gi\`a discusso, non esiste alcuna funzione ordinaria che possa
soddisfare la definizione e quindi dobbiamo ammettere che l'impulso unitario sia
un'entit\`a matematica di carattere analogo a quello di una funzione ordinaria,
\textit{ma che assume un significato solo quando se ne consideri una qualche
propriet\`a integrale} come nella definizione stessa.

%-------------------------------------------------------------------------------
% Subsubsection: Proprieta' della Delta di Dirac
%-------------------------------------------------------------------------------

\subsubsection{Propriet\`a della Delta di Dirac}
\textbf{1) Area unitaria}\\
\[
    \int\displaylimits_{-\infty}^{+\infty} \delta(t) \dt =
    \int\displaylimits_{-\infty}^{+\infty} \lim_{\epsilon \rightarrow 0}
    \delta_{\epsilon}(t) \dt =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} \lim_{\epsilon \rightarrow 0}
    \frac{1}{\epsilon} \cdot rect\left(\frac{t}{\epsilon}\right) \dt =
    \lim_{\epsilon \rightarrow 0} \frac{1}{\epsilon}
    \int\displaylimits_{-\epsilon/2}^{\epsilon/2} 1 \dt =
    \lim_{\epsilon \rightarrow 0} \frac{1}{\epsilon} \cdot \epsilon = 1.
\]
\bigbreak
\noindent
\textbf{2) Propriet\`a Campionatrice}\\
Se $x(t)$ risulta continuo in $t = 0$, allora
\[
    \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot \delta(t) \dt =
    x(t)|_{t = 0} = x(0).
\]
\begin{proof}
\[
    \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot \delta(t) \dt =
    \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot
    \lim_{\epsilon \rightarrow 0} \delta_{\epsilon}(t) \dt =
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty} x(t) \cdot 
    \lim_{\epsilon \rightarrow 0}
    \frac{1}{\epsilon} \cdot rect\left(\frac{t}{\epsilon}\right) \dt =
    \lim_{\epsilon \rightarrow 0} \frac{1}{\epsilon}
    \int\displaylimits_{-\epsilon/2}^{\epsilon/2} x(t) \dt =
\]
\[
    \footnote{Per il teorema della media: il teorema della media itnegrale
    stabilisce che la media integrale della funzione sia un valore incluso
    nell'intervallo immagine.}= \lim_{\epsilon \rightarrow 0}
    \frac{1}{\epsilon} \cdot \epsilon x(\overline{t}) =
    \lim_{\epsilon \rightarrow 0} x(\overline{t}) = x(0).
\]
\end{proof}
\bigbreak
\noindent
\textbf{3) Parit\`a}
\[
    \int\displaylimits_{-\infty}^{+\infty} x(t) \delta(t) \dt =
    \int\displaylimits_{-\infty}^{+\infty} x(t) \delta(-t) \dt
\]
\begin{proof}
\[
    \int\displaylimits_{-\infty}^{+\infty} x(t) \delta(-t) \dt \ 
    \footnote{Poniamo $t = -t'$.}=
    \int\displaylimits_{+\infty}^{-\infty} x(-t') \delta(t') -\dt' =
    \int\displaylimits_{-\infty}^{+\infty} x(-t') \delta(t') \dt' =
\]
\[
    \footnote{Per la propriet\`a campionatrice.}=
    x(-0) = x(0) = \int\displaylimits_{-\infty}^{+\infty} x(t) \delta(t) \dt.
\]
\end{proof}
\change{To be continued.}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare la condizione sufficiente per l'esistenza della TFS
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare la condizione sufficiente per l'esistenza della TFS}

%-------------------------------------------------------------------------------
% Section: Segnali Aleatori
%-------------------------------------------------------------------------------

\newpage	
\section{Segnali Aleatori}
In moltissimi casi non \`e possibile conoscere con esattezza \textit{a priori}
il valore assunto da un segnale in un certo istante. Si pensi per esempio al
segnale geofisico colto da sensori posti sul terreno per effettuare rilevazioni
minerarie. Tale segnale non \`e noto \textit{a priori} completamente, in
particolare non se ne conosce l'evoluzione futura se non dopo l'osservazione,
cio\`e \textit{a posteriori}. Prima dell'osservazione, si ha solo una conoscenza
generica di alcune propriet\`a di massima di tale segnale, derivante
dall'esperienza pregressa in casi simili. Stessa osservazione pu\`o farsi a
proposito delle tensioni di disturbo (\textit{rumore}) presenti nei componenti
elettronici attivi e passivi e prodotte da fenomeni incontrollabili, tipicamente
di origine quantistica. Diremo quindi che questi segnali sono \textit{aleatori},
intendendo che il valore assunto da essi \`e affetto da un certo grado di
improbabilit\`a (alea) che ne impedisce una conoscenza esatta. 
Per modellare e studiare i segnali aleatori \`e indispensabile quindi ricorrere
a tecniche basate sulla \textit{teoria della probabilit\`a e dei processi
aleatori}.

%-------------------------------------------------------------------------------
% Subsection: Teorema di Bayes
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema di Bayes}
Il teorema di Bayes (conosciuto anche come formula di Bayes o teorema della
probabilit\`a delle cause), prodotto da Thomas Bayes, deriva da due teoremi
fondamentali delle probabilit\`a: il teorema della probabilit\`a composta e il
teorema della probabilit\`a assoluta. Viene impiegato per calcolare la
probabilit\`a di una causa che ha scatenato l'evento verificato.\\
Formalmente, il teorema di Bayes \`e valido in tutte le interpretazioni della
probabilit\`a. In ogni caso, l'importanza di questo teorema per la statistica
\`e tale che la divisione tra le due scuole (statistica bayesiana e statistica
frequentista) nasce dall'interpretazione che si da al teorema stesso.
\bigbreak\noindent
\textbf{Enunciato}\\
Consideriamo la coppia di eventi $A$ e $B$, ciascuno avente probabilit\`a non
nulla. La probabilit\`a condizionata $Pr(A | B)$ pu\`o essere ricavata con la
relazione
\begin{equation}
    Pr(A | B) = \frac{Pr(B | A) \cdot Pr(A)}{Pr(B)},
\end{equation}
nota come teorema (o formula) di Bayes.
\begin{proof}
Il teorema deriva dalla definizione di probabilit\`a condizionata. La
probabilit\`a probabilit\`a di un evento $A$, noto un evento $B$, risulta:
\[
    Pr(A | B) = \frac{Pr(A \cap B)}{Pr(B)} = \frac{Pr(AB)}{Pr(B)}.
\]
In modo analogo, la probabilit\`a di un evento $B$ noto un evento $A$
\[
    Pr(B | A) = \frac{Pr(A \cap B)}{Pr(A)} = \frac{Pr(AB)}{Pr(A)},
\]
dalla quale otteniamo
\[
    Pr(A \cap B) = Pr(B | A) Pr(A).
\]
Sostituendo questo risultato nella prima uguaglianza, si trova il teorema di
Bayes
\[
    Pr(A | B) = \frac{Pr(A \cap B)}{Pr(B)} = \frac{Pr(B | A) \cdot
    Pr(A)}{Pr(B)}.
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsubsection: Esempio Teorema di Bayes
%-------------------------------------------------------------------------------

\subsubsection{Esempio Teorema di Bayes}
Si consideri una scuola che ha il $60 \%$ di studenti maschi e il $40 \%$ di
studentesse femmine.\\
Le studentesse indossano in egual numero gonne e pantaloni; gli studenti
indossano tutti quanti i pantaloni. Un osservatore, da lontano, nota un generico
studente coi pantaloni. Qual \`e la probabilit\`a che quello studente sia una
fenmina?
\bigbreak
Il problema pu\`o essere risolto con il teorema di Bayes, ponendo l'evento $A$
che lo studente osservato sia femmina, e l'evento $B$ che lo studente osservato
indossi i pantaloni. Per calcolare la probabilit\`a $Pr(A | B)\footnote{$Pr(A |
B) =$ probabilit\`a dell'evento che lo studente sia femmina una volta osservato
l'evento che lo studente visto indossa i pantaloni.}$, dovremo sapere:
\begin{itemize}
    \item $Pr(A)$, ovvero la probabilit\`a che lo studente sia femmina senza
        nessun'altra informazione. Dato che l'osservatore vede uno studente a
        caso, ci\`o significa che tutti gli studenti hanno la stessa
        probabilit\`a di essere osservati. Essendo le studentesse il $40 \%$ del
        totale, la probabilit\`a risulter\`a
        \[
            Pr(A) = 40 \% = \frac{40}{100} = 0.4 = \frac{4}{10} = \frac{2}{5}.
        \]
    \item $Pr(\overline{A})$, la probabilit\`a dell'evento complementare ad $A$,
        ovvero la probabilit\`a che lo studente sia maschio senza nessun'altra 
        informazione. Essendo $\overline{A}$ l'evento complementare di $A$,
        risulta
        \[
            Pr(\overline{A}) = 1 - Pr(A) = 1 - \frac{2}{5} = \frac{3}{5}.
        \]
        Equivalentemente:
        \[
            60 \% = \frac{60}{100} = 0.6 = \frac{6}{10} = \frac{3}{5}.
        \]
    \item $Pr(B | A)$, ovvero la probabilit\`a che uno studente femmina indossi
        i pantaloni (ossia la probabilit\`a che, verificato che lo studente
        osservato \`e una femmina, si verifichi l'evento che indossi i
        pantaloni). Poich\`e le studentesse indossano gonne e pantaloni in egual
        numero, la probabilit\`a sar\`a $\frac{1}{2}$.
    \item $Pr(B | \overline{A})$, ovvero la probabilit\`a che uno studente
        indossi i pantaloni, noto che lo studente osservato \`e un maschio.
        Tutti gli studenti maschi indossano i pantaloni, quindi l'evento \`e
        certo:
        \[
            Pr(B | \overline{A}) = 1.
        \]
    \item $Pr(B)$, ovvero la probabilit\`a che uno studente qualsiasi (maschio o
        femmina) indossi i pantaloni. Poich\`e il numero di coloro che indossano
        i pantaloni \`e $80 \%$ (il $60 \%$ di maschi e il $40/2 \%$ di femmine)
        sul totale $100 \%$ di studenti, la probabilit\`a $Pr(B)$ \`e di
        $80 \%/100 \% = 8/10 = 4/5$.
\end{itemize}
Ci\`o detto, possiamo applicare il teorema:
\[
    Pr(A | B) = \frac{Pr(B | A) \cdot Pr(A)}{Pr(B)} = \frac{\frac{1}{2} \cdot
    \frac{2}{5}}{\frac{4}{5}} = \frac{\frac{1}{5}}{\frac{4}{5}} = \frac{1}{5}
    \cdot \frac{5}{4} = \frac{1}{4}.
\]
\bigbreak\noindent
\textbf{\underline{Note Aggiuntive}}\\
\textbf{Conversione Percentuale $\Longrightarrow$ Frazione:}\\
\begin{enumerate}
    \item Dividere il valore in percentuale per $100$:
        $\frac{percentuale}{100}$;
    \item Se si ottiene un numero non intero, moltiplicare e dividere per $10$
        sino ad ottenere una frazione;
    \item Ridurre la frazione ottenuta.
\end{enumerate}
Ad esempio, per ottenere la frazione corrispondente al $40 \%$:
\begin{enumerate}
    \item $\frac{40}{100} = 0.4$;
    \item $0.4 \cdot \frac{10}{10} = \frac{4}{10}$;
    \item $\frac{4}{10} = \frac{2}{5}$.
\end{enumerate}
\bigbreak\noindent
\textbf{Evento Complementare:}\\
Dato un evento $A$, la probabilit\`a dell'evento complementare $\overline{A}$
\`e data dal complemento a uno di $Pr(A)$:
\[
    Pr(\overline{A}) = 1 - Pr(A).
\]
\bigbreak\noindent
\textbf{\underline{Assiomi utilizzati}}
\begin{itemize}
    \item \textbf{Assioma di Normalizzazione}\\
        La probabilit\`a dell'evento certo \`e unitaria:
        \[
            Pr(\Omega) = 1.
        \]
\end{itemize}

%-------------------------------------------------------------------------------
% Subsection: Teorema della Probabilita' Totale
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema della Probabilit\`a Totale}
Il teorema della probabilit\`a totale consente di calcolare la probabilit\`a che
si verifichi almeno uno di due o pi\`u eventi, ovvero la probabilit\`a
dell'unione di essi.
\bigbreak
Il teorema di Bayes \`e spesso usato insieme al teorema della probabilit\`a
totale che esaminiamo di seguito.
\begin{proof}
Costruiamo una partizione dello spazio $\Omega$ scegliendo $N$ eventi,
$i = 1, 2, \dots, N$ con le seguenti propriet\`a:
\[
    \begin{dcases}
        B_i \cap B_k = \emptyset \ \ se \ \ i \neq k\\
        \cup_{i = 1}^{N} B_i = \Omega
    \end{dcases}
\]
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.5\textwidth]{images/set_partition.jpg}
    \caption{Una partizione dello spazio $\Omega$ che rispetta le propriet\`a
    date.}
\end{figure}
\noindent
La probabilit\`a di un evento $A$ pu\`o allora essere calcolata come segue
\[
    Pr(A) = Pr(A \cap \Omega) = Pr(A \cap \cup_{i = 1}^{N} B_i) =
    Pr(\cup_{i = 1}^{N} A \cap B_i) =
\]
\[
    = \sum_{i = 1}^{N} Pr(A \cap B_i).
\]
Se in questa relazione si esprime poi ciascuna probabilit\`a congiunta
$Pr(A \cap B_i)$ come prodotto tra la probabilit\`a condizionata $Pr(A | B_i)$ e
la probabilit\`a marginale $Pr(B_i)$, si ricava l'uguaglianza
\[
    Pr(A) = \sum_{i = 1}^{N} Pr(A|B_i) Pr(B_i)
\]
che rappresenta appunto l'enunciato del teorema della probabilit\`a totale.
\end{proof}
\bigbreak\noindent
\textbf{N.B.:} $A$ \`e un sottoinsieme do $\Omega$, quindi
\[
    A = A \cap \Omega = A \cdot \Omega = A \cdot \left(\sum_{i = 1}^{N} B_i
    \right) = \sum_{i = 1}^{N} A \cdot B_i.
\]

%-------------------------------------------------------------------------------
% Subsection: Esempio Teorema di Bayes e Teorema della Probabilita' Totale
%-------------------------------------------------------------------------------

\subsubsection{Esempio Teorema di Bayes \& Teorema della Probabilit\`a Totale}
In una rete radio cellulare, un terminale mobile (telefonico) trasmette verso la
stazione radio base un flusso di dati binario: in tale flusso la probabilit\`a
che ciscun bit assuma valore logico $1$ \`e pari a $\rho = 0.3$, mentre quella
che assuma valore logico $0$ \`e pari a $1 - \rho = 0.7$.\\
A causa di disturbi introdotti dal canale radio, la probabilit\`a di errore sul
bit, cio\`e la probabilit\`a che un bit venga ricevuto erroneamente, \`e pari a
$P_e = 0.01$. Supponiamo ora che il ricevitore della stazione radio base abbia
rivelato la trasmissione di un bit pari a zero e valutiamo la probabilit\`a che
il trasmettitore abbia effettivamente trasmesso un dato nullo.\\
Per risolvere questo problema con gli strumenti del calcolo delle probabilit\`a,
definiamo innanzitutto gli eventi:
\begin{itemize}
    \item $A$ = \{\`e stato tramesso un bit 0\};
    \item $\overline{A}$ = \{\`e stato trasmesso un bit 1\};
    \item $B$ = \{\`e stato ricevuto un bit 0\};
    \item $\overline{B}$ = \{\`e stato ricevuto un bit 1\}.
\end{itemize}
E osserviamo poi che, sulla base delle definizioni date e dei dati iniziali del
problema, risulta
\[
    Pr(\overline{A}) = 0.3
\]
\[
    Pr(A) = 1 - Pr(\overline{A}) = 0.7
\]
e
\[
    Pr(B | \overline{A}) = Pr(\overline{B} | A) = P_e = 0.01.
\]
Occorre determinare la probabilit\`a che il trasmettitore (telefono mobile)
abbia effettivamente trasmesso un bit avente valore nullo quando sia stato
ricevuto un dato dello stesso tipo (dalla stazione radio base), cio\`e la
probabilit\`a condizionata $Pr(A | B)$; a tal fine possiamo usare la formula di
Bayes:
\[
    Pr(A | B) = \frac{Pr(B | A) \cdot Pr(A)}{Pr(B)}.
\]
Il calcolo di $Pr(A | B)$ mediante questa formula richiede la conoscienza delle
probabilit\`a $Pr(B | A)$ e $Pr(B)$ (dato che $Pr(A)$ \`e gi\`a nota). La prima
\`e ottenibile dai dati del problema:
\[
    Pr(B | A) = 1 - Pr(B | \overline{A}) = 1 - Pr(\overline{B} | A) = 1 - P_e =
    0.99,
\]
e rappresentata la probabilit\`a di corretta decisione, inversa alla
probabilit\`a di errore.\\
Mentre la seconda pu\`o essere ottenuta mediante il teorema della probabilit\`a
totale:
\[
    \Omega = A \cup \overline{A}
\]
\[
    Pr(B) = Pr(B | A) \cdot Pr(A) + Pr(B | \overline{A}) \cdot Pr(A)
\]
\[
    Pr(B) = Pr(B | A) \cdot Pr(A) + Pr(B | \overline{A}) \cdot Pr(A) = 0.99
    \cdot 0.7 + 0.01 \cdot 0.3 = 0.696.
\]
Infine, possiamo calcolare $Pr(A | B)$ sostituendo questi valori nell'equazione
iniziale della formula di Bayes:
\[
    Pr(A | B) = \frac{Pr(B | A) \cdot Pr(A)}{Pr(B)} =
    \frac{0.99 \cdot 0.7}{0.696}.
\]
\`E chiaro che, quando si decide a favore di un valore $0$ di un bit, \`e stato
effettivamente trasmesso con elevata probabilit\`a proprio un bit con quel
valore logico (la probabilit\`a \`e quasi pari a quella dell'evento certo. La
terza cifra dopo la virgola di $Pr(A | B)$ pu\`o essere motivata ricordando che
il bit $0$ ha probabilit\`a $0.7$ contro il $0.3$ del bit $1$ di presentarsi).\\
Inoltre, questa "affidabilit\`a" \`e motivata dal piccolo valore della
probabilit\`a di errore, cio\`e dalla scarsa frequenza di eventi di rivelazione
errata dei dati. Infatti, se $P_e = 0$ (cio\`e la trasmissione \`e priva di
errori) risulta chiaramente $Pr(A | B) = Pr(B | A) = 1$ qualunque sia $\rho$. In
questo caso infatti, se il ricevitore ha ricevuto un bit di valore nullo, con
certezza \`e stato trasmesso un bit con quel valore, in quanto la probabilit\`a
di errore \`e nulla. Il caso limite opposto \`e quello in cui il canale di
trasmissione radio \`e cos\`i disturbato da provocare $P_e = 0.5$. In questo
caso, la probabilit\`a di rilevare un bit nullo quando il trasmettitore ha
inviato proprio un bit di quel valore ($Pr(B | A)$) \`e esattamente uguale alla
probabilit\`a di ricevere un bit nullo quando \`e stato trasmesso un bit di
valore complementare ($Pr(B | \overline{A})$) ed entrambe queste probabilit\`a
sono pari a $0.5$
\bigbreak\noindent
\textbf{1.} Nel caso in cui $P_e = 0.5$, il rilevare un particolare valore di un
bit non aggiunge alcuna informazione, cosicch\`e $Pr(A | B) = Pr(A)$ qualunque
sia il valore di $\rho$:
\begin{proof}
\[
    Pr(A) = 1 - Pr(\overline{A}) = 1 - \rho = 0.7, \ \ \ \ avendo \ \ preso \ \ 
    \rho = 0.3.
\]
\[
    Pr(B | \overline{A}) = Pr(\overline{B} | A) = P_e = 0.5.
\]
\[
    Pr(B | A) = 1 - Pr(B | \overline{A}) = 1 - Pr(\overline{B} | A) = 1 - P_e =
    0.5.
\]
\[
    Pr(B) = Pr(B|A) \cdot Pr(A) + Pr(B | \overline{A}) \cdot Pr(\overline{A}) =
    0.5 \cdot 0.7 + 0.5 \cdot 0.3 = 0.5.
\]
In conclusione:
\[
    Pr(A | B) = \frac{Pr(B | A) \cdot Pr(A)}{Pr(B)} = \frac{0.5 \cdot 0.7}{0.5}
    = 0.7 = Pr(A),
\]
che dimostra la tesi iniziale.
\end{proof}
\bigbreak\noindent
\textbf{2.} Nello scenario in esame il caso peggiore corrisponde proprio alla
scelta $P_e = 0.5$, anche se $P_e$ pu\`o assumere un valore arbitrario
nell'intervallo $[0, 1]$.
\begin{proof}
A partire dalle formule del punto 1:
\[
    Pr(A | B) = \frac{Pr(B | A) \cdot Pr(A)}{Pr(B)} = \frac{(1 - P_e) \cdot 
    (1 - \rho)}{Pr(B | A) \cdot Pr(A) + Pr(B | \overline{A}) \cdot
    Pr(\overline{A})} =
\]
\[
    = \frac{(1 - P_e) \cdot (1 - \rho)}{(1 - P_e) \cdot (1 - \rho) + P_e \cdot
    \rho} = \frac{(1 - P_e) \cdot (1 - \rho)}{1 - \rho - P_e \cdot (1 - \rho) +
    P_e \cdot \rho} =
\]
\[
    = \frac{(1 - P_e) \cdot (1 - \rho)}{1 - \rho - P_e \cdot (1 - \rho + \rho)}
    = \frac{(1 - P_e) \cdot (1 - \rho)}{(1 - \rho) - P_e} = \frac{(1 - P_e)
    \cdot Pr(A)}{Pr(A) - P_e}.
\]
A partire da questo risultato si dimostra la tesi iniziale con una serie di
\textbf{Considerazioni}:\\
\begin{itemize}
    \item $P_e \rightarrow 0$, errore nullo, $(1 - P_e) \rightarrow 1$,
        $Pr(A | B) = \frac{Pr(A)}{Pr(A)} = 1$.
    \item $P_e \rightarrow 1$, errore massimo, $(1 - P_e) \rightarrow 0$,
        $Pr(A | B) = 0$.
\end{itemize}
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Legame tra Varianza sigma_X^2 e Valore quadratico medio
%-------------------------------------------------------------------------------

\newpage
\subsection{Legame tra Varianza $\sigma_X^2$ e Valor quadratico medio $m_X^2$}
La conoscienza della funzione densit\`a (o distribuzione) di probabilit\`a di
una variabile aleatoria rappresenta il massimo di informazione che si pu\`o
avere sul comportamento statistico dei valori assunti dalla variabile stessa.
Naturalmente, per\`o, non sempre \`e possibile arrivare a una conoscenza cos\`i
completa riguardo a un problema aleatoria che si trattando. Molti pi\`u spesso,
ci si accontenta della conoscenza di alcuni \textit{parametri statistici
semplificati} o \textit{indici} relativi alla distribuzione di probabilit\`a
presentata dalla variabile.\\
Il \textit{valore atteso} (chiamato anche \textit{valor medio},
\textit{speranza}, \textit{attesa}) $\eta_X$ di una variabile aleatoria $X$
con densit\`a di probabilit\`a $f_X(x)$ \`e definito dalla relazione
\[
    \eta_X \triangleq \int\displaylimits_{-\infty}^{+\infty} x \cdot f_X(x)
    \ dx,
\]
e rappresenta in un certo senso un valore "baricentro" attorno al quale si
distribuiscono i valori della variabile aleatoria stessa (indice di
\textit{posizione}).
\bigbreak
Quando si ha a che fare con un problema di \textit{trasformazione} di una
variabile aleatoria $Y = g(X)$, si introduce il cosiddetto \textit{operatore
valor medio}:
\[
    E\{g(X)\} \triangleq \int\displaylimits_{-\infty}^{+\infty} g(x) \cdot
    f_X(x) \ dx.
\]
La lettera $E$ nell'operatore valori medio $E\{\cdot\}$ \`e l'iniziale della
parola inglese \textit{Expectation} che traduce l'italiano "aspettativa".
Notiamo che anche il valore atteso $\eta_X$ pu\`o essere riscritto formalmente
usando questo operatore
\[
    \eta_X = \int\displaylimits_{-\infty}^{+\infty} x \cdot f_X(x) \ dx = E\{X\},
\]
cio\`e il valore atteso \`e anche il valor medio della variabile aleatoria
stessa.
\bigbreak
Si definisce la \textit{varianza} $\sigma_X^2$ di una variabile aleatoria $X$
come:
\[
    \sigma_X^2 \triangleq E\{(X - \eta_X)^2\} =
    \int\displaylimits_{-\infty}^{+\infty} (x - \eta_X)^2 \cdot f_X(x) \ dx.
\]
Il parametro $\eta_X$, redice quadrata della varianza, \`e la cosiddetta
\textit{derivazione standard}. A maggiore varianza della variabile aleatoria
corrispondono valori molto disparsi attorno al valor medio, e viceversa.
\bigbreak
Il \textit{valore quadratico medio} (tavolta chiamato anche \textit{potenza}) di
una variabile aleatoria \`e infine definito come
\[
    m_X^2 \triangleq E\{X^2\} = \int\displaylimits_{-\infty}^{+\infty} x^2 \cdot
    f_X(x) \ dx.
\]
Poich\`e l'operatore valor medio $E\{\cdot\}$ \`e un operatore \textit{lineare},
\`e facile trovare il legame tra la varianza $\sigma_X^2$ e il valore quadratico
medio $m_X^2$:
\[
    \sigma_X^2 \triangleq E\{(X - n_X)^2\} = E\{X^2 + \eta_X^2 - 2 \cdot
    \eta_X \cdot X\} =
\]
\[
    \footnote{Il valore medio \`e una funzione lineare.}
    = E\{X^2\} + E\{\eta_X^2\} - E\{2 \cdot \eta_X \cdot X\} = 
\]
\[
    \footnote{$E\{\eta_x^2\} \int\displaylimits_{-\infty}^{+\infty} \eta_x^2
    f_X(x) \ dx = \eta_x^2 \int\displaylimits_{-\infty}^{+\infty} f_X(x) \ dx =
    \eta_x^2$.}
    \footnote{$E\{2 \eta_x X\} = \int\displaylimits_{-\infty}^{+\infty} 2 \cdot 
    x \cdot \eta_x \cdot f_X(x) \ dx = 2 \cdot \eta_x
    \int\displaylimits_{-\infty}^{+\infty} x \cdot f_X(x) \ dx = 2 \cdot \eta_x
    E\{x\}$.}
    = m_X^2 + \eta_X^2 - 2 \cdot \eta_X^2 =
\]
\[
    = m_X^2 - \eta_X^2.
\]

%-------------------------------------------------------------------------------
% Subsection: Teorema Fondamentale della Probabilita'
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema Fondamentale della Probabilit\`a}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare il teorema Fondamentale della Probabilita' per funzioni
%             monotone
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare il teorema Fondamentale della Probabilit\`a per funzioni
monotone}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare il teorema dell'Aspettazione per variabili discrete $X$
%             e $Y = g(X)$
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare il teorema dell'Aspettazione per variabili discrete
$X$ e $Y = g(X)$}

%-------------------------------------------------------------------------------
% Subsection: Teorema del Limite Centrale
%-------------------------------------------------------------------------------

\newpage
\subsection{Teorema del Limite Centrale}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che due variabili aleatorie gaussiane incorrelate sono
%             anche indipendenti
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che due variabili aleatorie gaussiane incorrelate sono
anche indipendenti}

%-------------------------------------------------------------------------------
% Subsection: Dare la definizione di Funzione di distribuzione e le sue
%             proprieta', specificare come e' fatta nel caso di variabili
%             aleatorie discrete e continue
%-------------------------------------------------------------------------------

\newpage
\subsection{Dare la definizione di Funzione di distribuzione e le sue
propriet\`a, specificare come \`e fatta nel caso di variabili aleatorie discrete
e continue}

%-------------------------------------------------------------------------------
% Subsection: Definire Valore Medio, Varianza, Momento Ordinario di Ordine r e
%             Valore Quadratico Medio
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire Valore Medio, Varianza, Momento Ordinario di Ordine $r$ e
Valore Quadratico Medio}

%-------------------------------------------------------------------------------
% Subsubsection: Valore Medio
%-------------------------------------------------------------------------------

\subsubsection{Valore Medio}

%-------------------------------------------------------------------------------
% Subsubsection: Varianza
%-------------------------------------------------------------------------------

\subsubsection{Varianza}

%-------------------------------------------------------------------------------
% Subsection: Momento Ordinario di Ordine r
%-------------------------------------------------------------------------------

\subsection{Momento Ordinario di Ordine $r$}

%-------------------------------------------------------------------------------
% Subsubsection: Valore Quadratico Medio
%-------------------------------------------------------------------------------

\subsubsection{Valore Quadratico Medio}

%-------------------------------------------------------------------------------
% Subsection: Modelli per Variabili Aleatorie
%-------------------------------------------------------------------------------

\newpage
\subsection{Modelli per Variabili Aleatorie: DDP, FDd, Valor Medio, Valore
Quadratico Medio, Varianza}

%-------------------------------------------------------------------------------
% Subsubsection: Modello Uniforme
%-------------------------------------------------------------------------------

\subsubsection{Modello Uniforme}

%-------------------------------------------------------------------------------
% Subsubsection: Modello Gaussiano
%-------------------------------------------------------------------------------

\subsubsection{Modello Gaussiano}

%-------------------------------------------------------------------------------
% Subsubsection: Modello Esponenziale Negativo
%-------------------------------------------------------------------------------

\subsubsection{Modello Esponenziale Negativo}

%-------------------------------------------------------------------------------
% Subsection: Definire la correlazione e la covarianza fra variabile aleatorie
%             e dire quando sono incorrelate e indipendenti
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire la correlazione e la covarianza fra variabili aleatorie e
dire quando sono incorrelate e indipendenti}

%-------------------------------------------------------------------------------
% Subsection: Proprieta' della funzione di autocorrelazione per processi
%             aleatori
%-------------------------------------------------------------------------------

\newpage
\subsection{Propriet\`a della funzione di autocorrelazione per processi
aleatori}

%-------------------------------------------------------------------------------
% Subsection: Definizione e proprieta' della DSP per processi aleatori
%-------------------------------------------------------------------------------

\newpage
\subsection{Definizione e propriet\`a della DSP per processi aleatori}

%-------------------------------------------------------------------------------
% Section: Sistemi Monodimensionali
%-------------------------------------------------------------------------------

\newpage
\section{Sistemi Monodimensionali}
L'analisi e in generalo lo studio dei segnali a tempo continuo intrapresi nei
capitoli precedenti originano un certo numero di domande: \textit{In quale
contesto si manifestano tali segnali? Dove possono essere osservati? Che cosa
\`e responsabile della produzione dei segnali stessi? Come questti possono
essere elaborati?} Nei capitoli precedenti abbiamo gi\`a visto alcune risposte
parziali a queste domande. Abbiamo infatti preso in considerazione segnali
prodotti da fenomeni fisici, da circuiti elettrici, da apparati in generale sia
naturali sia artificiali. Tutti questi esempi possono essere accomunati in un
solo concetto, ovvero quello di \textit{sistema} che produce o elabora un
segnale.

%-------------------------------------------------------------------------------
% Subsection: Dare la definizione di sistema e illustrane le proprieta'
%-------------------------------------------------------------------------------

\newpage
\subsection{Dare la definizione di sistema e illustrarne le propriet\`a}
Cos\`i come nel caso della definizione di segnale discussa precedentemente,
anche la definizione di \textit{sistema} \`e abbastanza articolata, per lo meno
dal punto di vista del linguaggio ordinario. In senso lato, possiamo chiamare
\textit{sistema monodimensionale} (altrimenti detto \textit{a un ingresso e una
uscita}) un qualunque dispositivo, o interconnessione di dispositivi, o
apparato, che produce un segnale di \textit{uscita} (chiamato anche
\textit{risposta} o \textit{effetto}) in corrispondenza di un segnale di
\textit{ingresso} (detto anche \textit{sollecitazione}, \textit{eccitazione}
o \textit{causa}). Questa definizione \`e intenzionalmente molto vaga, in modo
che sotto il termine di \textit{sistema} possano rientrare i casi pi\`u
disparati. \`E chiaro che un circuito elettronico per il trattamento del segnale
\`e un caso tipico di sistema (per esempio un amplificatore in un sistema di
riproduzione audio ad alta fedelt\`a). A buon diritto per\`o pu\`o classificarsi
come tale anche un sistema di \textit{controllo}: la potenza erogata dal motore
a scoppio di una autovettura (segnale di \textit{uscita}), controllata dal
sistema di iniezione di carburante, \`e determinata dalla posizione che istante
per istante assume il pedale dell'acceleratore (segnale di \textit{ingresso});
la temperatura del nocciolo di una centrale termonucleare (segnale di uscita)
\`e determinata dalla portata con cui il liquido refrigerante affluisce al
nocciolo stesso (segnale di ingresso), e cos\`i via.
\bigbreak
Dal punto di vista matematico, che \`e quello che riguarda pi\`u da vicino la
teoria dei segnali, la definizione di sistema \`e assai meno vaga. In questo
contesto, un \textit{sistema} \`e una \textit{trasformazione} (o, con il
linguaggio nell'analisi funzionale, una \textit{funzionale}) ce a un segnale
di ingresso $x(t)$ fa corrispondere un ben determinato e unico segnale d'uscita
$y(t)$. La trasformata del segnale $x(t)$ nel segnale $y(t)$ si denota nel modo
seguente:
\[
    y(t) = \mathcal{T}[x(\alpha); t]
\]
dove con questa notazione si intende che il valore dell'uscita all'istante $t$
dipende in generale, oltre che da $t$ stesso, dall'andamento
\textit{complessivo} del segnale d'ingresso $x(t)$, cio\`e da \textit{tutti}
i suoi valori $x(\alpha)$, con $-\infty < \alpha < +\infty$ (per esempio
$y(t) = \int\displaylimits_{-\infty}{t} x\alpha) \ d\alpha$). Quando non ci sono
per\`o particolari questioni di ambiguit\`a, si pu\`o usare anche la notazione
semplificata
\[
    y(t) = \mathcal{T}[x(t)].
\]
Una rappresentazione grafica di questa trasformazione \`e quella della figura
seguente
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.9\textwidth]{images/siso_system.png}
    \caption{Sistema che trasforma il segnale $x(t)$ nel segnale $y(t)$}
\end{figure}
Le frecce indicano i segnali di ingresso e di uscita; il rettangolo \`e la
"materializzazione" grafica della trasformazione $\mathcal{T}[\cdot]$; la
funzione $h(t)$ si ritiene per il momento non nota cos\`i come non \`e nota
la struttura interna (racchiusa dal blocco ed inaccessibile), n\`e la maniera
per caratterizzare il comportamento agli effetti esterni. Quest'ultimo argomento
\`e l'oggetto del prossimi paragrafi.
\bigbreak
Un esempio di sistema \`e l'\textit{amplificatore ideale} per il quale la legge
di trasformazione \`e elementare: esso viene infatti descritto dalla semplice 
relazione $y(t) = A \cdot x(t)$, essendo $A$ una costante data
(l'\textit{amplificazione}).

%-------------------------------------------------------------------------------
% Subsubsection: Propriet\`a dei sistemi monodimensionali
%-------------------------------------------------------------------------------

\subsubsection{Propriet\`a dei sistemi monodimensionali}
A prescindere dalla struttura interna del sistema, fortemente dipendente dal
contesto e dall'applicazione, \`e possibile acquisire alcune informazioni
preliminari sul comportamento del sistema stesso e individuare cos\`i alcune
\textit{propriet\`a}, compiendo osservazioni esclusivamente sui segnali di
ingresso/uscita. Esaminiamo dunque queste propriet\`a:
\bigbreak
\noindent
\textbf{Stazionariet\`a}\\
Se le caratteristiche del sistema \textit{non variano nel tempo}, il sistema \`e
\textit{stazionario}\footnote{Un sistema tempo-invariante o sistema stazionario
\`e un sistema dinamico in cui l'uscita non dipende esplicitamente dal tempo.};
questo \`e il caso dei circuiti elettrici con componenti,
per l'appunto, costanti nel tempo. Volendo caratterizzare in modo formare un
sistema siffatto possiamo scrivere che, se
\[
    y(t) = \mathcal{T}[x(t)]
\]
allora
\[
    \mathcal[x(t - t_0)] = y(t - t_0)
\]
Questa relazione dice in pratica che la risposta corrispondente all'eccitazione
traslata nel tempo $x(t - t_0)$ ha lo stesso andamento della risposta al segnale
originario $x(t)$ non traslato, purch\`e la si trasli della \textit{stessa
medesima} quantit\`a $t_0$.
\bigbreak
\noindent
\textbf{Causalit\`a}\\
Un sistema \`e \textit{causale} quando il valore dell'uscita all'istante
arbitrario generico $t$ dipende soltanto dai valori assunti dall'ingresso agli
istanti \textit{precedenti} (o al limite coincidenti con) $t$ stesso:
\[
    y(t) = \mathcal{T}[x(\alpha), \alpha \leq t; t] = \mathcal{T}[x(\alpha)
    \cdot u(t - \alpha); t]
\]
L'aggettivo \textit{causale} deriva dalla considerazione che, se la relazione
precedente non fosse verificata, l'uscita all'istante $t$ sarebbe determinata
anche da valori dell'ingresso $x(\alpha)$ a istanti $\alpha > t$, cio\`e valori
\textit{futuri} relativamente a $t$, in violazione del principio di
causa-effetto.
\bigbreak
La causalit\`a dei sistemi sembrerebbe quindi una propriet\`a scontata. In
realt\`a possiamo introdurre un'ulteriore distinzione: si dice che un sistema
opera in \textit{tempo reale} se produce il segnale di uscita contestualmente
alla presentazione di quello d'ingresso. Quindi un sistema fisicamente
realizzabile che lavora in tempo reale non pu\`o che essere causale. Se invece
l'uscita viene fornita dal sistema solo \textit{successivamente
all'acquisizione completa de segnale di ingresso}, si dice che il sistema opera
in \textit{tempo vrituale}. Registrando il segnale d'ingresso su nastro o disco
magnetico, si pu\`o generale il segnale di uscita elaborando il segnale
\textit{successivamente} all'acquisizione (cio\`e in tempo virtuale) e quindi si
possono anche compiere operazioni di tipo "predittivo" impossibili in tempo
reale e tipiche di un sistema non causale.
\bigbreak
\noindent
\textbf{Memoria}\\
Un caso particolare di sistema causale \`e il cosiddetto sistema
\textit{istantaneo} in cui l'uscita all'istante $t$ dipende \textit{solamente}
dal valore dell'ingresso al medesimo istante:
\[
    y(t) = \mathcal{T}[x(\alpha), \alpha = t; t]
\]
In questo caso si usa anche la dizione di sistema \textit{senza memoria} (per
evidenti motivi). Due diversi segnali di ingresso $x_1(t)$ e $x_2(t)$,
coincidenti a un certo istante $t*$, provocano un medesimo valore in uscita
all'istante $t*$, indipendentemente dai rispettivi andamenti per $t \neq t*$.
L'esempio tipico di un sistema istantaneo \`e l'amplificatore ideale
$y(t) = A \cdot x(t)$. Viceversa, un esempio di sistema \textit{con memoria}
\`e il cosiddetto \textit{integratore a finestra mobile} per il quale
\[
    y(t) = \int\displaylimits_{t - T}^{t} x(\alpha) \ d\alpha
\]
ove $T > 0$ \`e l'ampiezza della "finestra di integrazione". Il calcolo del
valore dell'uscita all'istante $t$ presuppone la conscenza dell'andamento del
segnale d'ingresso in tutto l'intervallo $[t - T, t]$: il sistema mantiene
una certa \textit{memoria} dell'andamento del segnale d'ingresso $x(t)$.
\bigbreak
\noindent
\textbf{Stabilit\`a}\\
Diremo che un sistema \`e \textit{stabile} se, sollecitato da un segnale con
andamento arbitrario ma di \textit{ampiezza limitata}, produce a sua volta in
uscita un segnale di \textit{ampiezza limitata}:
\[
    |x(t)| \leq M \Rightarrow |y(t)| \leq K
\]
con $M$ e $K$ finiti. Questa definizione di stabilit\`a si indica con l'acronimo
BIBO (\textit{Bounded-Input Bounded-Output}) che significa "uscita limitata per
ogni ingresso limitato", ed \`e solo una tra le molte definizioni di stabilit\`a
che si possono dare per i sistemi monodimensionali. Secondo questo criterio, un
"buon pilotaggio" di un sistema stabile, cio\`e un segnale di ingresso
adeguatamente limitato, non causa mai in uscita fenomeni di instabilit\`a,
cio\`e situazioni in cui la risposta $y(t)$ tende a crescere illimitatamente.
\bigbreak
\noindent
\textbf{Invertibilit\`a}\\
In molti casi \`e necessario \textit{ricostruire} il segnale di eccitazione in
ingresso a un sistema nota la risposta al segnale stesso. Questa operazione \`e
possibile solo per sistemi \textit{invertibili}, per i quali cio\`e esiste un
\textit{sistema inverso} $\mathcal{T}^{-1}[\cdot]$ tale  che:
\[
    \mathcal{T}^{-1}[y(t)] = x(t)
\]
qualunque sia il segnale di ingresso $x(t)$. \`E chiaro che l'amplificatore
ideale \`e invertibile
\[
    y(t) = A \cdot x(t) \Rightarrow x(t) = \frac{y(t)}{A}
\]
(e il suo sistema inverso \`e ancora un amplificatore ideale), mentre il sistema
$y(t) = x^2(t)$ non lo \`e.
\bigbreak
\noindent
\textbf{Linearit\`a}\\
Infine, un sistema \`e \textit{lineare} se a esso \`e applicabile il principio
di sovrapposizione degli effetti. Ci\`o significa che al segnale di ingresso
\[
    x(t) = \alpha \cdot x_1(t) + \beta \cdot x_2(t)
\]
costituito da una combinazione lineare con coefficienti costanti $\alpha$ e
$beta$ dekke due eccitazioni $x_1(t)$ e $x_2(t)$ (le \textit{cause}), il sistema
risponde con il segnale di uscita
\[
    y(t) = \mathcal{T}[x(t)] = \alpha \cdot y_1(t) + \beta \cdot y_2(t)
\]
dove
\[
    y_1(t) = \mathcal{T}[x_1(t)]
\]
e
\[
    y_2(t) = \mathcal{T}[x_2(t)].
\]
L'uscita si ottiene dunque mediante la \textit{stessa} combinazione lineare
delle due risposte $y_1(t)$ e $y_2(t)$ (gli \textit{effetti}) alle due
eccitazioni $x_1(t)$ e $x_2(t)$ considerate agenti separatamente.

%-------------------------------------------------------------------------------
% Subsection: Si dimostri che, per un SLS, il segnale di uscita e' scrivibile
%             come la convoluzione del segnale di ingresso per la risposta
%             impulsiva
%-------------------------------------------------------------------------------

\newpage
\subsection{Si dimostri che, per un SLS, il segnale di uscita \`e scrivibile
come la convoluzione del segnale di ingresso per la risposta impulsiva}
Restringiamo adesso la nostra attenzione al caso estremamente importante di
\textit{sistemi lineari e stazionari} (SLS). Questi rivestono una particolare
importanza perch\`e si rivelano estremamente semplici da analizzare e possono
inoltre essere sintetizzati (progettati) con altrettanta facilit\`a.
\bigbreak
Nel paragrafo precedente abbiamo qualificato i sistemi monodimensionali secondo
determinate propriet\`a rilevabili mediante lo studio dei soli segnali di
ingresso e uscita, a prescindere dalla struttura materiale del sistema stesso.
Vogliamo estendere questo modo di procedere, che potremmo chiamare "a scatola
chiusa", per arrivare a una \textit{caratterizzazione} esaustiva del
comportamento dei sistemi \textit{lineari stazionari}.
\bigbreak
Per un SLS dato \`e possibile misurare (o calcolare se si dispone di uno schema
di progetto) la cosiddetta \textit{risposta impulsiva}, cio\`e l'uscita del
sistema in corrispondenza all'eccitazione \textit{impulsiva} $x(t) = \delta(t)$.
Convenzionalmente, tale segnale viene indicato con $h(t)$:
\[
    h(t) \triangleq \mathcal{T}[\delta(t)]
\]
L'importanza della risposta impulsiva di un SLS risiede nel fatto che la sua
conoscenza permette di determinare la risposta del sistema a un segnale di
ingresso di andamento arbitrario.\\
Ricordando che la delta di Dirac \`e l'elemento neutro della convoluzione
\[
    x(t) = x(t) * \delta(t) = \int\displaylimits_{-\infty}^{+\infty} x(\alpha)
    \cdot \delta(t - \alpha) \ d\alpha
\]
possiamo scrivere che
\[
    y(t) = \mathcal{T}[x(t)] = \mathcal{T}[x(t) * \delta(t)] =
\]
\[
    = \mathcal{T}\left[\int\displaylimits_{-\infty}^{+\infty} x(\alpha)
    \cdot \delta(t - \alpha) \ d\alpha\right] =
\]
La trasformazione $\mathcal{T}[\cdot]$ caratteristica del sistema e
l'operazione di integrale sono entrambe operatori \textit{lineari}, e quindi \`e
possibile invertire l'ordine di calcolo. Otteniamo quindi
\[
    y(t) = \int\displaylimits_{-\infty}^{+\infty} \mathcal{T} [x(\alpha) \cdot
    \delta(t - \alpha) \ d\alpha]
\]
in cui \`e importante osservare che l'\textit{operatore} $\mathcal{T}[\cdot]$
agisce su segnali \textit{funzioni del tempo t}. Tenendo conto del fatto che
tale operatore \`e lineare e che, rispetto al tempo $t$, la quantit\`a
$x(\alpha)$ \`e una costante, si ha
\[
    y(t) = \int\displaylimits_{-\infty}^{+\infty} x(\alpha) \cdot
    \mathcal{T}[\delta(t - \alpha)] \ d\alpha.
\]
Infine, per la propriet\`a di \textit{stazionariert\`a}\footnote{$\mathcal{T}
[\delta(t - \alpha)] = h(t - \alpha)$} del sistema e
ricordando la definizione di risposta impulsiva $h(t)$, si ottiene
\[
    y(t) = \int\displaylimits_{-\infty}^{+\infty} x(\alpha) \cdot h(t - \alpha)
    \ d\alpha = x(t) * h(t)
\]
che stabilisce la relazione fondamentale (diremo \textit{costitutiva}) del
sistema lineare stazionario: il segnale di uscita pu\`o essere calcolato
attraverso la convoluzione del segnale di ingresso con la risposta impulsiva.

%-------------------------------------------------------------------------------
% Subsection: Definire quando un SLS e' causale e stabile BIBO e dimostrarlo
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire quando un SLS \`e causale e stabile BIBO e dimostrarlo}
La conoscenza della risposta impulsiva $h(t)$ di un sistema lineare stazionario,
oltre a permettere di ricavare il segnale di uscita dato quello di ingresso,
consente anche di verificare le propriet\`a possedute dal sistema e quindi
\textit{caratterizza completamente} il comportamento del sistema stesso. Nel
paragrafo precedente abbiamo visto che un sistema \`e causale se \`e verificata
la relazione
\[
    y(t) = \mathcal{T}[x(\alpha), \alpha \leq t; t] = \mathcal{T}[x(\alpha)
    u(t - \alpha); t].
\]
Dimostriamo ora che un SLS \`e causale se la sua risposta impulsiva \`e un
\textit{segnale causale}\footnote{Un segnale causale \`e una
funzione dipendente dal tempo che rappresenta uno stimolo esterno su un sistema
fisico. Matematicamente, un segnale causale \`e rappresentato mediante una
funzione nulla per $t < 0$. L'istante $t = 0$ rappresenta il momento in cui
viene applicato dall'esterno il segnale.}, cio\`e
\[
    h(t) \equiv h(t) \cdot u(t) \Longrightarrow h(t) = 0 \quads{2} \forall
    \ t < 0.
\]
Infatti, se calcoliamo il segnale di uscita di un SLS la cui risposta impulsiva
\`e causale, abbiamo:
\[
    y(t) = x(t) * h(t) = \int\displaylimits_{-\infty}^{+\infty} x(\alpha)
    \cdot h(t - \alpha) \ d\alpha = 
\]
\[
    = \int\displaylimits_{-\infty}^{+\infty}
    x(\alpha) \cdot h(t - \alpha) \cdot u(t - \alpha) \ d\alpha =
    = \int\displaylimits_{-\infty}^{t} x(\alpha) \cdot h(t - \alpha) \ d\alpha.
\]
La limitazione dell'estremo superiore di integrazione nella precedente relazione
provocata dalla causalit\`a della risposta impulsiva, porta a concludere che il
valore al generico istante $t$ del segnale di uscita \`e determinato \textit{dai
soli valori assunti da $x(\alpha)$ per $\alpha \leq t$}. In una parola, il
sistema \`e causale.
\bigbreak
Anche la \textit{stabilit\`a} del sistema lineare stazionario \`e univocamente
determinata dall'andamento della risposta impulsiva. Infatti, condizione
necessaria e sufficiente affinch\`e un SLS sia stabile \`e che la sua risposta
impulsiva sia \textit{assolutamente integrabile}:
\[
    \int\displaylimits_{-\infty}^{+\infty} \abs{h(t)} \dt = H < +\infty
    \Longrightarrow \abs{y(t)} < +\infty.
\]
\begin{proof}
Per un segnale di ingresso limitato (cio\`e $\abs{x(t)} \leq M$) possiamo
scrivere
\[
    \abs{y(t)} = \abs{x(t) * h(t)} = \left|
        \int\displaylimits_{-\infty}^{+\infty} x(\alpha) \cdot h(t - \alpha)
        \ d\alpha
    \right| \leq \int\displaylimits_{-\infty}^{+\infty} \abs{
        x(\alpha) \cdot h(t - \alpha)
    } \ d\alpha \leq
\]
\[
    \leq M \cdot \int\displaylimits_{-\infty}^{+\infty}
    \abs{h(t - \alpha)} \ d\alpha = M \cdot
    \int\displaylimits_{+\infty}^{-\infty} \abs{h(t')} \ -dt' =
\]
\[
    = M \cdot \int\displaylimits_{-\infty}^{+\infty}
    \abs{h(t')} \ dt' \leq M \cdot H < +\infty,
\]
quindi il sistema \`e stabile.
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Dare la definizione di replica fedele e specificare quando un
%             filtro non introduce distorsioni lineari
%-------------------------------------------------------------------------------

\newpage
\subsection{Dare la definizione di replica fedele e specificare quando un filtro
non introduce distorsioni lineari}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che l'uscita y(t) da un interpolatore, con funzione
%             interpolatrice p(t) e in ingresso la sequenza x[n], e' scrivibile
%             come y(t) = ATCF{X(f) P(f)}, dove X(f) = TFS{x[n]} e P(f) =
%             TCF{p(t)}
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che l'uscita $y(t)$ da un interpolatore, con funzione
interpolatrice $p(t)$ e in ingresso la sequenza $x[n]$, \`e scrivibile come
$y(t) = \mathcal{F}^{-1}\{\overline{X}(f) \cdot P(f)\}$, dove $\overline{X}(f) =
TFS\{x[n]\}$ e $P(f) = \mathcal{F}\{p(t)\}$}

%-------------------------------------------------------------------------------
% Subsection: Spiegare il concetto di stazionarieta' in senso lato e in senso
%             stretto di un processo aleatorio
%-------------------------------------------------------------------------------

\newpage
\subsection{Spiegare il concetto di stazionariet\`a in senso lato e in senso
stretto di un processo aleatorio}

%-------------------------------------------------------------------------------
% Subsection: Definire l'efficienza spettrale e calcolarla per una PAM binaria
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire l'efficienza spettrale e calcolarla per una PAM binaria}

%-------------------------------------------------------------------------------
% Subsection: Definire l'ISI e la condizione di Nyquist
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire l'ISI e la condizione di Nyquist}
Nel valutare le prestazioni dei sistemi di comunicazione numerici in banda base
considereremo due fenomeni peggiorativi
\begin{itemize}
    \item Interferenza Inter-Simbolo\footnote{In telecomunicazioni con
        interferenza intersimbolica (ISI) si intende un particolare fenomeno
        indesiderato che si manifesta nei ricevitori degli apparati di
        trasmissione digitale sulla base del quale i simboli o forme d'onda
        analogiche trasmesse in sequenza sul canale di comunicazione ad onde
        continue si sovrappongono temporalmente e parzialmente tra di loro
        producendo una distorsione del simbolo in questione con degrado della
        qualit\`a dell'informazione trasportata ossia diminuendo il margine di
        immunit\`a agli errori ed aumentando la probabilit\`a di errore del
        simbolo trasmesso in fase di decodifica/decisione.}
    \item Presenza di rumore
\end{itemize}
Per il momento ignoriamo il rumore e ci concentriamo sul primo problema.
\bigbreak
Il primo fenomeno \`e causato dalla non perfetta risposta in frequenza del
canale di trasmissione, e quindi dalle distorsioni lineari introdotte da
questo.
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{images/isi.jpg}
    \caption{Canale con e senza ISI.}
\end{figure}
Il risultato \`e che il campione estratto al ricevitore dal segnale ricevuto al
$k$-esimo istante non dipende solo dal $k$-esimo simbolo:
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.6\textwidth]{images/isi_scheme.jpg}
    \caption{Schema funzionale ISI.}
\end{figure}
\noindent
\textbf{ASSENZA DI ISI}
\[
    y[k] = f(x[k])
\]
\noindent
\textbf{PRESENZA DI ISI}
\[
    y[k] = f(\dots,\  x[k - 1],\  x[k],\  x[k + 1],\  \dots)
\]
\bigbreak
Per valutare gli effetti dell'ISI si devono considerare
\begin{itemize}
    \item il sagomatore di trasmissione $p(t)$
    \item la risposta impulsiva del canale $c(t)$
    \item il filtro in ricezione
\end{itemize}
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.9\textwidth]{images/full_isi_scheme.jpg}
    \caption{Schema completo ISI.}
\end{figure}
Un canale con Banda $B_c$ in generale introduce ISI. Ci sono due aspetti di cui
ci occuperemo:
\begin{itemize}
    \item Determinazione del $T_s$ minimo che pu\`o essere adattato al fine di
        ottenere una sequenza campionata priva di ISI;
    \item Determinare le condizioni sotto le quali \`e possibile trasmettere un
        segnale $M-PAM$  attraverso un canale non ideale in modo che non vi sia
        ISI nella sequenza campionata.
\end{itemize}
Secondo lo schema precedente, possiamo scrivere che
\[
    Y(f) = R(f) \cdot H_R(f) = S(f) \cdot C(f) \cdot H_R(f) =
\]
\[
    = \overline{X}(f) \cdot P(f) \cdot C(f) \cdot H_R(f) =
\]
\[
    = \overline{X}(f) \cdot H(f)
\]
dove nell'ultimo passaggio si \`e posto
\[
    H(f) = \cdot P(f) \cdot C(f) \cdot H_R(f).
\]
Passando nel dominio del tempo possiamo scrivere che
\[
    y(t) = \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot h(t - nT_s)
\]
Ora, il campione estratto al ricevitore dal segnale ricevuto al $k$-esimo
istante \`e dato da
\[
    y[k] = y(k T_s) = \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot
    h(kT_s - nT_s) = \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot
    h((k - n) T_s) =
\]
\[
    = x[k] \cdot h(0) + \sum\displaylimits_{n \neq k} x[n] \cdot h((k - n) T_s)
\]
dove nell'ultimo passaggio \`e stato estratto dalla sommatoria il campione di
indice $k$.\\
La prima parte del risultato
\[
    x[k] \cdot h(0)
\]
rappresenta la \textbf{componente utile}. Mentre la seconda parte
\[
    \sum\displaylimits_{n = -\infty}^{+\infty} x[n] \cdot h((k - n) T_s)
\]
rappresenta la \textbf{componente ISI}.
Da questo risultato deriva il \textbf{Primo Criterio di Nyquist per la
Trasmissione Priva di ISI} nel dominio del tempo
\[
    h((k - n)T_s) =
        \begin{cases}
            1 \ , \quads{2} k - n = 0\\
            0 \ , \quads{2} k - n \neq 0
        \end{cases}
\]
e nel dominio dell frequenza
\[
    \sum\displaylimits_{k = -\infty}^{+\infty} H\left(f - \frac{k}{T_s}\right) =
    T_s \quads{2} \forall \ f.
\]
\begin{proof}
Il primo criterio di Nyquist nel dominio del tempo garantisce l'assenza di ISI
in quanto
\[
    y[k] = x[k] \cdot h(0) + \sum\displaylimits_{n \neq k} x[n] \cdot
    h((k - n) T_s) = y[k] = x[k] \cdot h(0).
\]
La relazione in frequenza si ottiene come trasformazione
\[
    h[k] = \delta[k] \iff \overline{H}(f) = 1 \quads{2} \forall \ f
\]
\[
    \frac{1}{T_s} \sum\displaylimits_{n = -\infty}^{+\infty}
    H\left(f - \frac{k}{T_s}\right) = 1 \quads{2} \forall \ f
\]
\[
    \Rightarrow \sum\displaylimits_{n = -\infty}^{+\infty}
    H\left(f - \frac{k}{T_s}\right) = T_s \quads{2} \forall \ f
\]
\end{proof}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che, per una modulazione PAM in banda passante, che la
%             condizione di Nyquist nel tempo garantisce l'assenza di ISI
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che, per una modulazione PAM in banda passante, che la
condizione di Nyquist nel tempo garantisce l'assenza di ISI}

%-------------------------------------------------------------------------------
% Subsection: A partire dalla condizione di Nyquist per l'assenza di ISI per una
%             PAM in banda base, dimostrare la condizione di Nyquist in
%             Frequenza
%-------------------------------------------------------------------------------

\newpage
\subsection{A partire dalla condizione di Nyquist per l'assenza di ISI per una
PAM in banda base, dimostrare la condizione di Nyquist in Frequenza}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che, se si rispetta la condizione di Nyquist, non si ha
%             ISI per un sistema PAM binario
%-------------------------------------------------------------------------------

\subsection{Dimostrare che, se si rispetta la condizione di Nyquist, non si ha
ISI per un sistema PAM binario}

%-------------------------------------------------------------------------------
% Subsection: Definizione di Processo Indipendente
%-------------------------------------------------------------------------------

\newpage
\subsection{Definizione di Processo Indipendente}

%-------------------------------------------------------------------------------
% Subsection: Definizione di Processo SSL e SSS
%-------------------------------------------------------------------------------

\newpage
\subsection{Definizione di Processo SSL e SSS}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che se un processo gaussiano e' SSL allora e' anche SSS
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che se un processo gaussiano \`e SSL allora \`e anche
SSS}

%-------------------------------------------------------------------------------
% Subsection: L'uso della codifica Grey invece della condifica naturale in un
%             sistema numerico di comunicazione consente la riduzione della BER
%             (Bit Error Rate) a parita' di rapporto segnale/rumore Eb/N0 in
%             ricezione? Giustificare la risposta.
%-------------------------------------------------------------------------------

\newpage
\subsection{L'uso della codifica Grey invece della codificare naturale in un
sistema numerico di comunicazione consente la riduzione della BER (Bit Error
Rate) a parit\`a di rapporto senglae/rumore $E_b/N_0$ in ricezione? Giustificare
la risposta.}

%-------------------------------------------------------------------------------
% Subsection: Si spieghi il funzionamento del filtro adattato ed il modo in cui 
%             questo sistema minimizza la $P(e)$ in ricezione per un sistema di
%             comunicazione binario in banda base
%-------------------------------------------------------------------------------

\newpage
\subsection{Si spieghi il funzionamento del filtro adattato ed il modo in cui
questo sistema minimizza la $P(e)$ in ricezione per un sistema di comunicazione
binario in banda base}

%-------------------------------------------------------------------------------
% Subsection: Descrivere il sistema PCM standard
%-------------------------------------------------------------------------------

\newpage
\subsection{Descrivere il sistema PCM standard}

%-------------------------------------------------------------------------------
% Subsection: Dimostrare che il valore medio di un processo Y(t) ottenuto
%             filtrando un processo X(t) con un filtro lineare e stazionario
%             con risposta impulsiva h(t) e' scrivibile in termini della h(t) e
%             del valore medio del processo in ingresso nx(t)
%-------------------------------------------------------------------------------

\newpage
\subsection{Dimostrare che il valore medio di un processo $Y(t)$ ottenuto
filtrando un processo $X(t)$ con nu filtro lineare e stazionario con risposta
impulsiva $h(t)$ \`e scrivibile in termini della $h(t)$ e del valore medio del
processo in ingresso $\eta_x(t)$}

%-------------------------------------------------------------------------------
% Subsection: Definire il segnale trasmesso per una QAM generica e calcolare dal
%             punto di vista teorico l'energia media per simbolo trasmesso
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire il segnale trasmesso per una QAM generica e calcolare dal
punto di vista teorico l'energia media per simbolo trasmesso}

%-------------------------------------------------------------------------------
% Subsection: Schema QAM (ricevitore + trasmettitore)
%-------------------------------------------------------------------------------

\newpage
\subsection{Schema QAM (ricevitore + trasmettitore)}

%-------------------------------------------------------------------------------
% Subsection: Schema PAM in banda base e in banda passante
%-------------------------------------------------------------------------------

\newpage
\subsection{Schema PAM in banda base e in banda passante}

%-------------------------------------------------------------------------------
% Subsection: Definire gli indici statistici del primo ordine e quelli del
%             secondo ordine per processi aleatori
%-------------------------------------------------------------------------------

\newpage
\subsection{Definire gli indici statistici del primo ordine e quelli del secondo
ordine per processi aleatori}

%-------------------------------------------------------------------------------
% Subsection: Illustrare le relazioni fra processi
%-------------------------------------------------------------------------------

\newpage
\subsection{Illustrare le relazioni fra processi}

%-------------------------------------------------------------------------------
% Subsubsection: Incorrelati
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Incorrelati}

%-------------------------------------------------------------------------------
% Subsubsection: Ortogonali
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Ortogonali}

%-------------------------------------------------------------------------------
% Subsubsection: Indipendenti
%-------------------------------------------------------------------------------

\newpage
\subsubsection{Indipendenti}

%-------------------------------------------------------------------------------
% Subsection: Dire quando due processi sono congiuntamente stazionari in SSS e
%             SSL
%-------------------------------------------------------------------------------

\newpage
\subsection{Dire quando due processi sono congiuntamente stazionari in SSS e
SSL}

%-------------------------------------------------------------------------------
% Appendices
%-------------------------------------------------------------------------------

\appendix

%-------------------------------------------------------------------------------
% Appendice A: Segnali Canonici
%-------------------------------------------------------------------------------

\chapter{Segnali Canonici}
\epigraph{"
	Life is a sexually transmitted disease and the mortality rate is one
	hundred percent.
"}{--- \textup{R.D. Laing}}
La presente appendice contiene una lista di segnali canonici, per ciascuno dei
quali sono stati analizzati le propriet\`a principali, utilizzati nel testo.

%-------------------------------------------------------------------------------
% A.1: Gradino Unitario
%-------------------------------------------------------------------------------

\section{Gradino Unitario}
Nella teoria dei segnali e dei sistemi \`e utile definire la funzione
\textit{gradino unitario} $u(t)$ (detta anche \textit{funzione di Heaviside})
\begin{equation}
	u(t) =
		\begin{cases}
			1 \quads{4} t > 0\\
			1/2 \quads{3} t = 0\\
			0 \quads{4} t < 0
		\end{cases}
\end{equation}
rappresentata graficamente dalla figura seguente:
\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[width=0.5\textwidth]{images/heaviside_function.png}
	\caption{Segnale Gradino Unitario $u(t)$.}
\end{figure}
Tale funzione, discontinua nell'origine, consente una rappresentazione concisa
dei cosiddetti segnali \textit{causali}\footnote{Un segnale causale \`e una
funzione dipendente dal tempo che rappresenta uno stimolo esterno su un sistema
fisico. Matematicamente, un segnale causale \`e rappresentato mediante una
funzione nulla per $t < 0$. L'istante $t = 0$ rappresenta il momento in cui
viene applicato dall'esterno il segnale.} o \textit{cisoidali}, cio\`e
\textit{nulli} per $t < 0$. Si noti che, mentre per $t > 0$ $u(t)$ assume un
valore unitario, nell'origine assume il valore $1/2$.
\bigbreak
Il segnale gradino serve a modellare matematicamente l'accensione all'istante
$t = 0$ di un generatore ideale di tensione continua. che eroga cos\`i una
tensione costante per ogni valore $t \geq 0$. Il  segnale possiede energia
illimitata, in quanto
\begin{equation}
	\begin{split}
		E_u = \int\displaylimits_{-\infty}^{+\infty} \abs{u(t)}^2 \dt =
		\underbrace{\int\displaylimits_{0}^{+\infty} \abs{u(t)}^2 \dt}_{
			u(t) \ = \ 0 \ \forall \ t \ < \ 0}
		= \underbrace{\int\displaylimits_{0}^{+\infty} 1 \dt}_{u(t) \
		= \ 1 \ \forall \ t \ > \ 0} =
		\\
		= \left[ t \right]_0^{+\infty} = +\infty - 0 = +\infty.
		\quads{6}
	\end{split}
\end{equation}
La potenza media $P_u$, invece, \`e espressa dalla relazione
\begin{equation}
	\begin{split}
		P_u = \lim_{T \rightarrow \infty} \frac{1}{T} 
		\int\displaylimits_{-T/2}^{T/2} \abs{u(t)}^2 \dt = 
		\underbrace{\lim_{T \rightarrow \infty} \frac{1}{T} 
		\int\displaylimits_{0}^{T/2} \abs{u(t)}^2 \dt}_{u(t) \ = \ 0 \ 
		\forall \ t \ < \ 0} =
		\\
		= \underbrace{\lim_{T \rightarrow \infty} \frac{1}{T} 
		\int\displaylimits_{0}^{T/2} 1 \dt}_{u(t) \ = \ 1 \ \forall \ 
		t \ > \ 0} = \lim_{T \rightarrow \infty} \frac{1}{T} \left[ t 
		\right]_0^{T/2} =
		\quads{4}
		\\
		= \lim_{T \rightarrow \infty} \frac{1}{T} \left(\frac{T}{2} - 0
		\right) = \lim_{T \rightarrow \infty} \frac{1}{T} \cdot 
		\frac{T}{2} = \frac{1}{2}.
		\quads{4}
	\end{split}
\end{equation}
Il valore efficace \`e dato da
\begin{equation}
	u_{eff} = \sqrt{P_u} = \sqrt{\frac{1}{2}} = \frac{1}{\sqrt{2}},
\end{equation}
mentre il valore medio \`e ottenibile da
\begin{equation}
	\begin{split}
		u_m = \lim_{T \rightarrow \infty} \frac{1}{T} 
		\int\displaylimits_{-T/2}^{T/2} u(t) \dt = \lim_{T \rightarrow 
		\infty} \frac{1}{T} \int\displaylimits_{0}^{T/2} 1 \dt = 
		\lim_{T \rightarrow \infty} \frac{1}{T} \cdot \frac{T}{2} = 
		\frac{1}{2}.
	\end{split}
\end{equation}
\end{document}

